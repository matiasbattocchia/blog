{
  
    
        "post0": {
            "title": "Random encoders for sentence classification",
            "content": "Charla basada en este paper: . https://arxiv.org/pdf/1901.10444.pdf . A complex pattern-classification problem, cast in a high-dimensional space nonlinearly, is more likely to be linearly separable than in a low-dimensional space, provided that the space is not densely populated. . ‚Äî‚ÄâCover, T. M. . Word embeddings . Palabra $ rightarrow$ t√≥ken. . Embedding: representaci√≥n de densa y de baja dimensionalidad de un t√≥ken. . Aproximaciones no supervisadas basadas en la hip√≥tesis distribucional: palabras que ocurren en el mismo contexto tienden a tener significados similares. . . Word embeddings pre-entrenados: . word2vec | GloVe | fastText | ELMo | . Sentence embeddings . Oraci√≥n $ rightarrow$ documento. . T√©cnica sencilla y aceptable: max o mean de los t√≥kenes del documento. . La intenci√≥n es usar un clasificador sobre los embeddings de documentos (downstream task). . . O simplemente una medida de similaridad. . . Tareas y datasets . https://arxiv.org/pdf/1705.02364.pdf . Clasificaci&#243;n . sentiment analysis (MR, SST), | product reviews (CR), | subjectivity (SUBJ), | opinion polarity (MPQA), | question-type (TREC). | . . Inferencia y similaridad sem&#225;ntica . entailment (SNLI, SICK-E), | semantic relatedness (SICK-R, STS), | paraphrasing (MRPC). | . . Encoders entrenados . $h = f_Œ∏(e_1, ldots, e_n)$ . Interesa obtener una representaci√≥n $h$ de una oraci√≥n, | usando alguna funci√≥n $f$ parametrizada por $Œ∏$, | en funci√≥n de embeddings pre-entrenados $e$ donde $e_i$ es la representaci√≥n de la i-√©sima palabra en una oraci√≥n de largo $n$. | . T√≠picamente los codificadores aprenden $Œ∏$, par√°metros que luego se mantien fijos en las tareas de transferencia. . InferSent . https://arxiv.org/abs/1705.02364 . Supervisado usando el corpus Stanford Natural Language Inference (SNLI). Requiere una gran cantidad de anotaciones. . . . Skip-Thought . https://arxiv.org/abs/1506.06726 . No supervisado. En vez de predecir las palabras que envuelven a una palabra (skip-gram), predice las oraciones alrededor de una oraci√≥n dada. Entrenarlo lleva un tiempo muy largo. . . . Random encoders . Diferentes maneras de parametrizar $f$ para representar el significado de oraciones sin ning√∫n entrenamiento de $Œ∏$. . Bag of random embedding projections . $X = (e_1, ldots, e_n)$ . $X ‚àà mathbb{R}^{n√óD}$. | $n$ es el tama√±o del documento, $D$ es la dimensi√≥n de los word embeddings. | . $h = f_{ text{pool}}(X W)$ . $W ‚àà mathbb{R}^{D√ód}$ se inicializa al azar usando una distribuci√≥n uniforme entre $[ frac{‚àí1}{ sqrt{d}}, frac{1}{ sqrt{d}}]$. | $D$ es la dimensi√≥n de los word embeddings, $d$ es la dimensi√≥n de la proyecci√≥n. | $f_{ text{pool}} = text{max}$ (max pooling) o $f_{ text{pool}} = text{mean}$ (mean pooling). | . Random LSTMs . $h = f_{ text{pool}}( text{BiLSTM}(e_1, ldots, e_n))$ . Los pesos se inicializan al azar usando una distribuci√≥n uniforme entre $[ frac{‚àí1}{ sqrt{d}}, frac{1}{ sqrt{d}}]$. | $d$ es la dimensi√≥n del estado oculto de la LSTM. | . Echo state networks (ESNs) . . $( hat y_1, ldots, hat y_n) = text{ESN}(e_1, ldots, e_n)$ . Descripci√≥n formal de una ESN: . $ tilde h_i = f_{ text{act}} (W^i e_i + W^h h_{i‚àí1} + b^i)$ . $h_i = (1‚àíŒ±) h_{i‚àí1} + Œ± tilde h_i$ . $W^i$, $W^h$, $b^i$ son inicializados al azar y no se actualizan durante el entrenamiento. | $Œ± ‚àà (0,1]$ es el grado de mezcla entre el estado previo y el actual. | . $ hat y_i = W^o [e_i;h_i] + b^o$ . $W^o$, $b^o$ son los √∫nicos parametros que se entrenan. | $ hat y_i$ es la predicci√≥n para $y_i$. | NO SE USA. | . $h = f_{ text{pool}}( text{BiESN}(e_1, ldots, e_n))$ . Se utiliza una ESN bidireccional, los estados del reservorio de ambas direcciones se concatenan $h_i = [ overrightarrow{h_i}; overleftarrow{h_i}]$. | Mediante pooling de estos estados se obtiene la representaci√≥n de la oraci√≥n $h$. | . La echo state property clama que el estado del reservorio debe ser √∫nicamente determinada por la historia de entrada y que los efectos de un estado dado deben disminuir en favor de estados m√°s recientes. En la pr√°ctica esta propiedad se satisface asegurando que el valor absoluto del autovalor m√°s grande de $W^h$ sea menor que 1. . Resultados . Parte 2: C&#243;digo (BOREP) . Vamos a intentar la estrategia de bag of random embeddings projection. . https://github.com/dair-ai/emotion_dataset . sadness üò¢ joy üòÉ love ü•∞ anger üò° fear üò± surprise üòØ . import pandas as pd pd.set_option(&#39;max_colwidth&#39;, 400) df = pd.read_pickle(&#39;datasets/emotions.pkl&#39;) df.emotions.value_counts() . joy 141067 sadness 121187 anger 57317 fear 47712 love 34554 surprise 14972 Name: emotions, dtype: int64 . for emotion in df.emotions.unique(): sample = df.query(f&#39;emotions == @emotion&#39;).sample(5) print(emotion.upper()) for _, text in sample.text.items(): print(&#39;* &#39; + text) print(&#39; n&#39;) . SADNESS * i feel shitty about my looks which makes me feel shitty as a person * i started feeling a few things here and there under me feet or when something messy * i put it aside feeling a little defeated * i feel devastated when i fail * i shut it down reminding myself that i have no time for this feeling burdened by the compulsion to do something about all my thoughts JOY * i feel like all of the colors put together look cool even if they arent realistic * i feel almost too trusting * i feel hopeful somehow and like i am climbing back up from this pit * i feel super happy when i see other people going off for a holiday * i start out feeling very confident positive about my choices and way more together than the stammering person i just painted myself into a second ago i often get this kind of doomsday response LOVE * i walk by animal stores or see people walking their pets i feel a sense of longing for my own animals * i were cool but sometimes i had this gut feeling that she wasn t fond of me * i expect to beaten down to give until i feel as if i can give no more to love without being loved always to continually pray to feel pain for my children and because of my children * i feel like i am expending a lot of effort in supporting them with very little return emotional support * i thought it would be a good time to check in on weasel nation to see how they were feeling about their donut loving coach and their floundering football team ANGER * when some seniors tried to scold and insult some juniors on account of what the juniors were supposed to have said at secondary school * i feel so angry with the person that i have lost and i feel like it is going to consume me at times * i feel a little greedy about these books i got in the mail today * i can really feel those people who insulted the other races * i feels like the type of people who would not bother with such petty crimes but that is what i said about grell beforehand FEAR * i could just embrace feeling weird instead of clinging to what i think is normal * im starting to feel really nervous about all the work that has to be done in the new house john says why * i got so used to the pain that it actually feels weird to be up and functioning instead of being in the usual fetal position * i began to feel very shy and unable to concentrate on my words * im beginning to feel unsure about my current relationship after catching up with my friend jen who was at socc and heard all about her experiences abroad has made wonder what i am doing SURPRISE * i feel fuckin dazed * i feel dazed when im with him * i feel like falling in love with her is part of being amazed at how she makes our family so much better she tells the advocate * i really feel amazed and ashamed at the same time when people say that such a move wont end things the way they are and wont mark a new beginning * i have always had an issue with my weight and stomach fat so this feels weird . Revisando las muestras nos damos cuentas de que es un dataset bastante pol√©mico. . Tokenizaci&#243;n . docs = [doc.split() for doc in df.text] docs[3] . [&#39;i&#39;, &#39;was&#39;, &#39;feeling&#39;, &#39;a&#39;, &#39;little&#39;, &#39;low&#39;, &#39;few&#39;, &#39;days&#39;, &#39;back&#39;] . Indexaci&#243;n . import numpy as np from itertools import chain from collections import Counter import torch from tqdm import tqdm class Vocab(): @property def √≠ndice_relleno(self): return self.mapeo.get(self.t√≥ken_relleno) def __init__(self, t√≥ken_desconocido=&#39;&lt;unk&gt;&#39;, t√≥ken_relleno=&#39;&lt;pad&gt;&#39;, frecuencia_m√≠nima=0.0, frecuencia_m√°xima=1.0, longitud_m√≠nima=1, longitud_m√°xima=np.inf, stop_words=[], l√≠mite_vocabulario=None): self.t√≥ken_desconocido = t√≥ken_desconocido self.t√≥ken_relleno = t√≥ken_relleno self.frecuencia_m√≠nima = frecuencia_m√≠nima self.frecuencia_m√°xima = frecuencia_m√°xima self.longitud_m√≠nima = longitud_m√≠nima self.longitud_m√°xima = longitud_m√°xima self.stop_words = stop_words self.l√≠mite_vocabulario = l√≠mite_vocabulario def reducir_vocabulario(self, lote): contador_absoluto = Counter(chain(*lote)) contador_documentos = Counter() for doc in lote: contador_documentos.update(set(doc)) # frecuencia m√≠nima if isinstance(self.frecuencia_m√≠nima, int): # frecuencia de t√≥ken vocabulario_m√≠n = [t√≥ken for t√≥ken, frecuencia in contador_absoluto.most_common() if frecuencia &gt;= self.frecuencia_m√≠nima] else: # frecuencia de documento vocabulario_m√≠n = [t√≥ken for t√≥ken, frecuencia in contador_documentos.most_common() if frecuencia/len(lote) &gt;= self.frecuencia_m√≠nima] # frecuencia m√°xima if isinstance(self.frecuencia_m√°xima, int): # frecuencia de t√≥ken vocabulario_m√°x = [t√≥ken for t√≥ken, frecuencia in contador_absoluto.most_common() if self.frecuencia_m√°xima &gt;= frecuencia] else: # frecuencia de documento vocabulario_m√°x = [t√≥ken for t√≥ken, frecuencia in contador_documentos.most_common() if self.frecuencia_m√°xima &gt;= frecuencia/len(lote)] # intersecci√≥n de vocabulario_m√≠n y vocabulario_m√°x preservando el √≥rden if len(vocabulario_m√≠n) == len(vocabulario_m√°x): vocabulario = vocabulario_m√≠n else: vocabulario = [t√≥ken for t√≥ken in tqdm(vocabulario_m√≠n, &#39;Procesando documentos&#39;) if t√≥ken in vocabulario_m√°x] # longitud vocabulario = [t√≥ken for t√≥ken in vocabulario if self.longitud_m√°xima &gt;= len(t√≥ken) &gt;= self.longitud_m√≠nima] # stop words vocabulario = [t√≥ken for t√≥ken in vocabulario if t√≥ken not in self.stop_words] # l√≠mite vocabulario = vocabulario[:self.l√≠mite_vocabulario] return vocabulario def fit(self, lote): vocabulario = [] if self.t√≥ken_relleno: vocabulario.append(self.t√≥ken_relleno) if self.t√≥ken_desconocido: vocabulario.append(self.t√≥ken_desconocido) vocabulario += self.reducir_vocabulario(lote) self.mapeo = {t√≥ken: √≠ndice for √≠ndice, t√≥ken in enumerate(vocabulario)} return self def transform(self, lote): if self.t√≥ken_desconocido: # reemplazar return [[t√≥ken if t√≥ken in self.mapeo else self.t√≥ken_desconocido for t√≥ken in doc] for doc in lote] else: # ignorar return [[t√≥ken for t√≥ken in doc if t√≥ken in self.mapeo] for doc in lote] def t√≥kenes_a_√≠ndices(self, lote): lote = self.transform(lote) return [[self.mapeo[t√≥ken] for t√≥ken in doc] for doc in lote] def √≠ndices_a_t√≥kenes(self, lote): mapeo_inverso = list(self.mapeo.keys()) return [[mapeo_inverso[√≠ndice] for √≠ndice in doc] for doc in lote] def __len__(self): return len(self.mapeo) @property def vocabulario(self): return list(v.mapeo.keys()) def obtener_embeddings(self, fastText): embeddings = [ fastText[t√≥ken] for t√≥ken in self.vocabulario if t√≥ken not in (self.t√≥ken_desconocido, self.t√≥ken_relleno) ] embeddings = torch.stack( list( map(torch.tensor, embeddings) ) ) if self.t√≥ken_desconocido: unk = embeddings.mean(dim=0, keepdim=True) embeddings = torch.cat([unk, embeddings]) if self.t√≥ken_relleno: pad = torch.zeros(1, fastText.get_dimension()) embeddings = torch.cat([pad, embeddings]) return embeddings . v = Vocab(t√≥ken_desconocido=None, t√≥ken_relleno=None) v.fit(docs) len(v) . 75302 . v.t√≥kenes_a_√≠ndices([[&#39;i&#39;, &#39;was&#39;, &#39;feeling&#39;, &#39;a&#39;, &#39;little&#39;, &#39;low&#39;, &#39;few&#39;, &#39;days&#39;, &#39;back&#39;]]) . [[0, 23, 5, 6, 53, 409, 187, 162, 98]] . Representaciones pre-entrenadas . import fasttext import fasttext.util fasttext.util.download_model(&#39;en&#39;, if_exists=&#39;ignore&#39;) ft = fasttext.load_model(&#39;cc.en.300.bin&#39;) . e = v.obtener_embeddings(ft) e.shape . torch.Size([75302, 300]) . idxs = v.t√≥kenes_a_√≠ndices(docs) . x = e[ idxs[3] ] x.shape . torch.Size([9, 300]) . Representaciones de oraciones . D = 300 d = 512 w = torch.empty(D, d) w = torch.nn.init.uniform_(w, -1/np.sqrt(d), 1/np.sqrt(d)) w.shape . torch.Size([300, 512]) . xw = torch.mm(x, w) xw.shape . torch.Size([9, 512]) . xw.max(dim=0).values.shape . torch.Size([512]) . s = torch.stack( [ torch.mm(e[doc], w).max(dim=0).values for doc in tqdm(idxs) ] ) . Representaciones de emociones . emo = [ [&#39;sadness&#39;], [&#39;joy&#39;], [&#39;love&#39;], [&#39;anger&#39;], [&#39;fear&#39;], [&#39;surprise&#39;], ] emo_idxs = v.t√≥kenes_a_√≠ndices(emo) . emo_sents = [ torch.mm(e[doc], w).max(dim=0).values for doc in emo_idxs ] . Distancia entre oraciones . d = torch.nn.PairwiseDistance(p=.5) d(emo_sents[0].reshape(1,-1), emo_sents[1].reshape(1,-1)) . tensor([8635.8857]) . dist = torch.stack( [d(s, sent) for sent in tqdm(emo_sents)], dim=1 ) dist.shape . y_pred = dist.min(dim=1).indices . M&#233;tricas . labels = { &#39;sadness&#39;:0, &#39;joy&#39;:1, &#39;love&#39;:2, &#39;anger&#39;:3, &#39;fear&#39;:4, &#39;surprise&#39;:5, } df[&#39;y_true&#39;] = df.emotions.map(labels) . from sklearn.metrics import classification_report print(classification_report(df.y_true, y_pred, target_names=labels)) . precision recall f1-score support sadness 0.43 0.00 0.01 121187 joy 0.51 0.00 0.01 141067 love 0.17 0.03 0.05 34554 anger 0.14 0.23 0.18 57317 fear 0.11 0.19 0.14 47712 surprise 0.04 0.56 0.07 14972 accuracy 0.08 416809 macro avg 0.23 0.17 0.07 416809 weighted avg 0.35 0.08 0.05 416809 . Muy tristes estos resultados üò¢. Quiz√°s random sentence encoders funcione m√°s para entrenar clasificadores m√°s que para medidas de similaridad. . BOE . Dado que la estrategia anterior no funcion√≥, veamos qu√© sucede con la cl√°sica bag of embeddings. . Representaciones de oraciones . x.max(dim=0).values.shape . torch.Size([300]) . s = torch.stack( [ e[doc].max(dim=0).values for doc in tqdm(idxs) ] ) . Representaciones de emociones . emo_sents = [ e[doc].max(dim=0).values for doc in emo_idxs ] . Distancia entre oraciones . dist = torch.stack( [d(s, sent) for sent in tqdm(emo_sents)], dim=1 ) dist.shape . y_pred = dist.min(dim=1).indices . M&#233;tricas . print(classification_report(df.y_true, y_pred, target_names=labels)) . precision recall f1-score support sadness 0.31 0.22 0.25 121187 joy 0.52 0.00 0.01 141067 love 0.17 0.02 0.04 34554 anger 0.14 0.72 0.24 57317 fear 0.32 0.01 0.02 47712 surprise 0.04 0.09 0.06 14972 accuracy 0.17 416809 macro avg 0.25 0.18 0.10 416809 weighted avg 0.34 0.17 0.12 416809 . Embedding a embbeding . Dado que la estrategia anterior no funcion√≥, veamos qu√© sucede con la m√°s cl√°sica todav√≠a comparaci√≥n palabra a palabra usando embeddings de palabras sin proyecci√≥n. Vamos a comparar cada palabra del documento con la palabra de emoci√≥n y nos quedaremos con la distancia m√°s corta para determinar la distancia del documento a la emoci√≥n. . dist = [] for sent in emo_sents: # distancia de cada embedding (t√≥ken) del documento a la emoci√≥n distancias_docs = [ d(e[doc], sent).min() for doc in idxs] dist.append( torch.stack( distancias_docs ) ) dist = torch.stack(dist, dim=1) dist.shape . torch.Size([416809, 6]) . y_pred = dist.min(dim=1).indices print(classification_report(df.y_true, y_pred, target_names=labels)) . precision recall f1-score support sadness 0.52 0.02 0.04 121187 joy 0.53 0.00 0.01 141067 love 0.19 0.06 0.10 34554 anger 0.34 0.01 0.01 57317 fear 0.33 0.01 0.02 47712 surprise 0.04 0.96 0.07 14972 accuracy 0.05 416809 macro avg 0.33 0.18 0.04 416809 weighted avg 0.43 0.05 0.03 416809 .",
            "url": "https://matiasbattocchia.github.io/datitos/Random-sentence-encoders.html",
            "relUrl": "/Random-sentence-encoders.html",
            "date": " ‚Ä¢ Oct 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Salvar un DataFrame en Google Sheets",
            "content": "Este es uno de los mejores trucos que aprend√≠ trabajando en Mutt Data. Del lado del desarrollo, en el d√≠a a d√≠a, trabajar con Pandas es de lo m√°s com√∫n, tanto para an√°lisis exploratorio de datos como manipulaci√≥n de datos en general. Cuando existen otras partes interesadas en un proyecto, sobre todo personas de negocio, la comunicaci√≥n es esencial y en Mutt nos iba bastante bien sacando a la luz DataFrames por medio de Google Sheets ‚Äî ambos est√°n hechos para trabajar con datos tabulares y lo bueno de los documentos de Google es que son f√°ciles de compartir y que las personas de negocio est√°n acostumbradas a interactuar con hojas de c√°lculo. . El paquete que vamos a usar se llama gspread y es una API en Python para Google Sheets. . pip install gspread . Autenticaci&#243;n . Vamos a necesitar una cuenta de servicio ‚Äîque es un archivo con credenciales‚Äî para habilitar a nuestro programa a escribir en Google Sheets. Estas son las instrucciones. . Ir a la consola de Google Cloud Platform (GCP) y crear un proyecto nuevo o seleccionar uno existente (yo cre√© mi-proyecto). Si nunca usaste GCP, vas a tener algunos pasos adicionales. | En la barra de Buscar productos y recursos buscar Google Drive API y habilitarla. | En la barra de Buscar productos y recursos buscar Google Sheets API y habilitarla. | En la barra de Buscar productos y recursos buscar cuentas de servicio, en esa p√°gina: + Crear cuenta de servicio, completar el formulario; con el nombre de la cuenta de servicio es suficiente (yo eleg√≠ google-sheets). | Una vez creada, seleccionarla para entrar en los detalles de la cuenta. | Claves &gt; Agregar clave &gt; Crear clave nueva &gt; JSON &gt; Crear. Aceptar la descarga de la cuenta de servicio. En mi caso, de mi-proyecto-80a030363d28.json. | Mover el archivo a la carpeta de trabajo. Debe estar en lugar seguro. | | La cuenta de servicio servir√° para todas las planillas de c√°lculo que necesitemos dentro de un mismo proyecto de GCP. . Acceso a la hoja de c&#225;lculo . Este paso es mucho muy importante, debe realizarse cada vez que utilicemos una hoja de c√°lculo nueva. . Ir a la hoja de c√°lculo y compartirla con el correo electr√≥nico de la cuenta de servicio (es el que figura en el detalle de la cuenta) de la misma manera que har√≠amos para compart√≠rsela a otra persona mediante su cuenta de correo. . En mi caso, tengo que compartir las hojas con google-sheets@mi-proyecto.iam.gserviceaccount.com. . Pandas . Esta es la funci√≥n que utilizo para escribir un DataFrame en Google Sheets. Hay algunas conversiones de tipos de datos, ya que Pandas y Google Sheets no manejan los mismos tipos. . import gspread GSHEETS_CREDENTIALS = &#39;mi-proyecto-80a030363d28.json&#39; def save_to_gsheets(df, sheet_name, worksheet_name=&#39;Sheet1&#39;): client = gspread.service_account(GSHEETS_CREDENTIALS) sheet = client.open(sheet_name) worksheet = sheet.worksheet(worksheet_name) # convertimos el tipo de las columnas que sean datetime a string for column in df.columns[df.dtypes == &#39;datetime64[ns]&#39;]: df[column] = df[column].astype(str) # reemplazamos valores NaN por strings vac√≠os worksheet.update([df.columns.values.tolist()] + df.fillna(&#39;&#39;).values.tolist()) print(f&#39;DataFrame escrito en la hoja {sheet_name} / {worksheet_name}.&#39;) . Vamos con un ejemplo: . import numpy as np import pandas as pd df = pd.DataFrame(np.random.randint(0, 100, size=(7, 4)), columns=list(&#39;ABCD&#39;)) df . A B C D . 0 80 | 61 | 9 | 52 | . 1 17 | 98 | 55 | 77 | . 2 4 | 52 | 5 | 99 | . 3 3 | 93 | 36 | 99 | . 4 99 | 45 | 51 | 39 | . 5 69 | 62 | 3 | 23 | . 6 93 | 34 | 90 | 88 | . Previamente tuve que crear la hoja Ejemplo Pandas y darle acceso a la cuenta de servicio. . save_to_gsheets(df, &#39;Ejemplo Pandas&#39;, worksheet_name=&#39;Sheet1&#39;) . DataFrame escrito en la hoja Ejemplo Pandas / Sheet1. . Este es el resultado üìù. . .",
            "url": "https://matiasbattocchia.github.io/datitos/Salvar-un-DataFrame-en-Google-Sheets.html",
            "relUrl": "/Salvar-un-DataFrame-en-Google-Sheets.html",
            "date": " ‚Ä¢ Oct 13, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Preprocesamiento de texto para NLP (parte 3)",
            "content": "Este es el art√≠culo final de la serie preprocesamiento de texto para NLP. Los art√≠culos anteriores son parte 1 y parte 2. . En este nos vamos a focalizar en embeddings pre-entrenados. Los embeddings son un tema central en procesamiento del lenguaje y mucho se ha escrito al respecto. Ac√° hay algunos enlaces para introducrise en el tema . The illustrated Word2Vec | CS224n presentaci√≥n word vectors | CS224n trabajo pr√°ctico word vectors | . y ac√° dejamos algunos enlaces sobre c√≥mo algunos frameworks abarcan el tema de este mismo art√≠culo . Keras | Gluon | . Los embeddings son la primera capa en las redes neuronales que procesan texto. Mapean √≠ndices (a cada t√≥ken le corresponde un √≠ndice, los √≠ndices corren de cero hasta len(t√≥kenes)). Estamos mapeando enteros a vectores, a cada √≠ndice le corresponde un vector de palabra que codifica a la palabra. El mapeo se realiza por medio de una matriz que tiene tantas filas como √≠ndices y tantas columnas como la dimensi√≥n de los vectores. Esta dimensi√≥n es un hiperpar√°metro del modelo y b√°sicamente significa la cantidad de atributos con la que representaremos a las palabras. Elegir una fila de la matriz, y a cada √≠ndice/t√≥ken le corresponde una fila) estamos rebanando la matriz de modo de quedarnos con un vector. . Como el resto de las capas de una red neuronal que no ha sido entrenada los pesos de la capa de embeddings se inicializan al azar. O sea que al seleccionar un vector de palabra obtenemos un vector con componentes aleatorios. La idea central de los embeddings es que las palabras adquieren significado a partir de las palabras que la rodean. Una vez que la red neural ha sido entrenada y que los componentes de los vectores de palabras no son azarosos sino que han capturado en mayor o menor medida el significado de las palabras, la distancia entre los vectores (similitud del coseno es una forma de calcular la distancia entre vectores) de palabras similares es m√°s corta, es decir los embeddings est√°n m√°s cerca, que si cuando se consideran palabras con significados dis√≠miles. . Una de las primeras t√©cnicas de transfencia de aprendizaje (transfer learning) fue utilizar embeddings pre-entrenados. La red neuronal con la que son entrenados y la que los utiliza con otros fines pueden tener arquitecturas bien distintas, comparten solamente los vectores de palabras, es decir la primera capa. Vimos que el armado del vocabulario es un asunto central y ser√≠a extra√±o que adoptemos el mismo vocabulario que la red que se utiliz√≥ para entrenar los embeddings; no es esto un problema mientras haya una intersecci√≥n substancial entre el vocabulario que queremos utilizar y el que se utiliz√≥ para los embeddings, ya que nos estamos limitando a este √∫ltimo, posiblemente entrenado con un corpus general (Wikipedia) mientras que el vocabulario que necesitamos posiblemente pertenezca a un corpus particular. Todos los t√≥kenes que no est√°n en el vocabulario se denominan fuera del vocabulario (out-of-vocabulary u OOV) y requieren un tratamiento especial como ser ignorados/eliminados o mapeados a un t√≥ken especial que codifique t√≥kenes desconocidos. . Los √≠ndices del vocabulario que crearemos tampoco ser√° el mismo que los que se usaron para los vectores pre-entrenados. Por lo tanto la estrategia para obtener los pesos de la capa de vectores de palabra es la siguiente. . Descargar los vectores pre-entrenados | Obtener los vectores del vocabulario propio | Ordenar los vectores seg√∫n los √≠ndices propios | Crear un tensor | Inicializar los pesos de la capa de embeddings | Descargar los vectores pre-entrenados . Los proyectos m√°s conocidos son . Word2Vec | GloVe | fastText | . Vamos a usar fastText por tener vectores para idioma espa√±ol y soporte para OOV. Primero instalamos el paquete de Python . pip install fasttext . y luego descargamos e inicializamos el modelo. Pesa unos 3,5 GB as√≠ que la descarga puede demorar. La dimensi√≥n de los vectores de este modelo es 300. . import fasttext import fasttext.util fasttext.util.download_model(&#39;es&#39;, if_exists=&#39;ignore&#39;) ft = fasttext.load_model(&#39;cc.es.300.bin&#39;) . IMPORTANTE. Particularmente la carga de este modelo necesita de unos 12 GB de memoria RAM/swap, lo que me llev√≥ a cerrar aplicaciones para liberar memoria. Para evitar pasar siempre por este paso, una vez que obtuve el tensor con los pesos necesarios lo salv√© en un archivo; levantar este archivo es mucho m√°s liviano. . Obtener los vectores del vocabulario . Redefinimos ligeramente la clase Vocab que fuimos escribiendo en las partes anteriores. Lo nuevo es la propiedad vocabulario, que devuelve la lista de t√≥kenes del vocabulario. . import numpy as np from itertools import chain from collections import Counter class Vocab(): # ning√∫n cambio aqu√≠ @property def √≠ndice_relleno(self): return self.mapeo.get(self.t√≥ken_relleno) # ning√∫n cambio aqu√≠ def __init__(self, t√≥ken_desconocido=&#39;&lt;unk&gt;&#39;, t√≥ken_relleno=&#39;&lt;pad&gt;&#39;, frecuencia_m√≠nima=0.0, frecuencia_m√°xima=1.0, longitud_m√≠nima=1, longitud_m√°xima=np.inf, stop_words=[], l√≠mite_vocabulario=None): self.t√≥ken_desconocido = t√≥ken_desconocido self.t√≥ken_relleno = t√≥ken_relleno self.frecuencia_m√≠nima = frecuencia_m√≠nima self.frecuencia_m√°xima = frecuencia_m√°xima self.longitud_m√≠nima = longitud_m√≠nima self.longitud_m√°xima = longitud_m√°xima self.stop_words = stop_words self.l√≠mite_vocabulario = l√≠mite_vocabulario # ning√∫n cambio aqu√≠ def reducir_vocabulario(self, lote): contador_absoluto = Counter(chain(*lote)) contador_documentos = Counter() for doc in lote: contador_documentos.update(set(doc)) # frecuencia m√≠nima if isinstance(self.frecuencia_m√≠nima, int): # frecuencia de t√≥ken vocabulario_m√≠n = [t√≥ken for t√≥ken, frecuencia in contador_absoluto.most_common() if frecuencia &gt;= self.frecuencia_m√≠nima] else: # frecuencia de documento vocabulario_m√≠n = [t√≥ken for t√≥ken, frecuencia in contador_documentos.most_common() if frecuencia/len(lote) &gt;= self.frecuencia_m√≠nima] # frecuencia m√°xima if isinstance(self.frecuencia_m√°xima, int): # frecuencia de t√≥ken vocabulario_m√°x = [t√≥ken for t√≥ken, frecuencia in contador_absoluto.most_common() if self.frecuencia_m√°xima &gt;= frecuencia] else: # frecuencia de documento vocabulario_m√°x = [t√≥ken for t√≥ken, frecuencia in contador_documentos.most_common() if self.frecuencia_m√°xima &gt;= frecuencia/len(lote)] # intersecci√≥n de vocabulario_m√≠n y vocabulario_m√°x preservando el √≥rden vocabulario = [t√≥ken for t√≥ken in vocabulario_m√≠n if t√≥ken in vocabulario_m√°x] # longitud vocabulario = [t√≥ken for t√≥ken in vocabulario if self.longitud_m√°xima &gt;= len(t√≥ken) &gt;= self.longitud_m√≠nima] # stop words vocabulario = [t√≥ken for t√≥ken in vocabulario if t√≥ken not in self.stop_words] # l√≠mite vocabulario = vocabulario[:self.l√≠mite_vocabulario] return vocabulario def fit(self, lote): vocabulario = [] if self.t√≥ken_relleno: vocabulario.append(self.t√≥ken_relleno) if self.t√≥ken_desconocido: vocabulario.append(self.t√≥ken_desconocido) vocabulario += self.reducir_vocabulario(lote) self.mapeo = {t√≥ken: √≠ndice for √≠ndice, t√≥ken in enumerate(vocabulario)} return self # ning√∫n cambio aqu√≠ def transform(self, lote): if self.t√≥ken_desconocido: # reemplazar return [[t√≥ken if t√≥ken in self.mapeo else self.t√≥ken_desconocido for t√≥ken in doc] for doc in lote] else: # ignorar return [[t√≥ken for t√≥ken in doc if t√≥ken in self.mapeo] for doc in lote] # ning√∫n cambio aqu√≠ def t√≥kenes_a_√≠ndices(self, lote): lote = self.transform(lote) return [[self.mapeo[t√≥ken] for t√≥ken in doc] for doc in lote] # ning√∫n cambio aqu√≠ def √≠ndices_a_t√≥kenes(self, lote): mapeo_inverso = list(self.mapeo.keys()) return [[mapeo_inverso[√≠ndice] for √≠ndice in doc] for doc in lote] # ning√∫n cambio aqu√≠ def __len__(self): return len(self.mapeo) @property def vocabulario(self): return list(v.mapeo.keys()) . Creamos el vocabulario como lo hicimos anteriormente (parte 1). . import pandas as pd df = pd.read_csv(&#39;train.csv&#39;, sep=&#39;|&#39;) # hacemos una tokenizaci√≥n muy simple def tokenizar(texto): return texto.split() train_docs = [tokenizar(doc) for doc in df[&#39;Pregunta&#39;].values] v = Vocab().fit(train_docs) . Desde Python 3.7 est√° garantizado que el orden del diccionario es el orden de inserci√≥n. Por lo tanto el √≥rden de la lista v.vocabulario coincide con el del diccionario v.mapeo (ver implementaci√≥n de Vocab). Tener claro el √≥rden / los √≠ndices de los t√≥kenes es importante porque crearemos un tensor de embeddings al cu√°l accederemos mediante √≠ndices. . v.vocabulario[:10] . [&#39;&lt;pad&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;de&#39;, &#39;el&#39;, &#39;la&#39;, &#39;tarjeta&#39;, &#39;que&#39;, &#39;para&#39;, &#39;un&#39;, &#39;me&#39;] . Por ejemplo, el embedding del t√≥ken tarjeta ser√° embeddings[5] ya que el t√≥ken est√° en el quinto lugar del vocabulario (recordar que empezamos a contar por cero). . La interfaz de fastText para obtener un vector a partir de un t√≥ken es como la de un diccionario. As√≠ luce un embedding de dimensi√≥n 300. . ft[&#39;tarjeta&#39;] . array([ 0.06298134, -0.03280621, 0.03053921, -0.14426479, -0.04330583, 0.02782611, 0.04671088, 0.01322352, -0.00091936, -0.02005653, -0.08679762, -0.00335382, -0.04299804, 0.03553167, -0.07989401, 0.00514562, 0.06741733, -0.01824431, -0.0627635 , -0.03652998, -0.02327815, -0.06624147, 0.00762858, 0.04288524, -0.02111394, -0.02724549, 0.01001478, -0.0437385 , -0.07554701, 0.00330107, -0.00436452, -0.03166814, -0.02237143, -0.00398921, -0.00873911, 0.01801448, 0.06549975, 0.02997639, 0.04104616, 0.08769971, -0.06594162, -0.01973427, 0.03386661, -0.05415446, -0.0547767 , -0.00098864, -0.00864553, 0.05127762, 0.02343957, -0.00937056, -0.03792336, 0.06513872, 0.03453366, 0.00376538, 0.00911847, 0.03639029, -0.04959448, -0.10815199, 0.0189229 , -0.00545404, -0.0441896 , 0.05246361, -0.08793913, 0.01742068, 0.07848521, 0.00829239, 0.00512537, -0.00187416, 0.06793492, -0.0205775 , 0.09385861, 0.06492148, 0.08256735, -0.01685029, 0.04042866, -0.03420147, -0.01297663, -0.03008673, 0.06171214, -0.0073834 , -0.00952853, -0.07957197, 0.05753422, -0.00230803, 0.01646664, 0.00405738, 0.01874345, -0.01656639, 0.03835326, 0.00671893, 0.03538686, -0.05837374, 0.00655341, -0.06613984, -0.00893264, 0.01970789, -0.02059824, -0.01957787, 0.04642227, -0.03362621, -0.03894996, -0.03437151, -0.0639662 , -0.00890221, -0.02950617, 0.030174 , 0.00092385, 0.08426531, -0.00274815, 0.00948968, 0.04102866, 0.01430673, 0.01487885, 0.0998308 , -0.0284079 , -0.00470919, 0.03808989, 0.08536439, 0.03592137, 0.07948075, 0.0172466 , -0.07252405, -0.0107453 , 0.0275656 , 0.02603439, 0.01865727, -0.10967878, 0.04329263, -0.03052348, 0.01704779, -0.05844689, 0.06367239, 0.00445418, 0.1319068 , 0.02953896, 0.02432506, 0.04764185, 0.04224063, -0.05673009, -0.00072847, -0.01646314, 0.0195642 , 0.02678232, -0.02039818, 0.01072512, 0.03165798, 0.02296546, 0.03048908, 0.00605224, 0.03494508, -0.03987421, 0.10772546, 0.05239586, -0.05665122, -0.04541425, -0.03411638, 0.00866744, -0.10566777, -0.06131719, -0.0434983 , 0.07758161, -0.05220485, 0.03249336, -0.12057097, 0.05518946, -0.00267152, -0.0791545 , 0.00928127, -0.03528287, -0.07231892, -0.00943873, 0.02749985, -0.02224496, 0.001105 , -0.04838763, 0.02414943, 0.00739839, 0.03333126, 0.0515946 , -0.0163026 , 0.06550607, -0.01794759, 0.07309239, 0.01166206, -0.01817643, 0.00392749, 0.00703375, 0.03426434, 0.02729288, -0.00475265, 0.01720353, 0.04551698, -0.02496281, -0.06664832, -0.02822311, 0.03184071, -0.02069683, -0.03815257, 0.02659006, 0.18702458, -0.0493904 , 0.02795539, -0.06647408, 0.02131662, 0.01693988, 0.04659843, -0.04887157, -0.09692122, -0.07950865, 0.06913692, -0.0173317 , 0.00877939, -0.06175148, 0.05520935, 0.04833567, 0.00859433, 0.0169889 , -0.02598299, 0.0434835 , -0.03762854, -0.02821014, -0.00132759, -0.06334166, 0.00318673, 0.01190044, -0.02857058, -0.01841859, -0.00682279, 0.00447517, -0.01528993, -0.07283813, 0.00650864, 0.01897584, -0.00431945, -0.02006911, 0.07013839, -0.02700875, 0.04124613, -0.01243533, -0.04903939, -0.01877775, 0.0053995 , -0.00930875, -0.03993747, 0.01549599, -0.01568508, -0.05651587, 0.06928204, 0.01355214, -0.0159476 , 0.04126405, 0.04020314, 0.10078269, 0.02648922, 0.06171743, -0.01357437, 0.0018341 , -0.00616703, 0.04361626, 0.00650506, 0.05089275, -0.00275116, 0.02991083, -0.11814439, -0.01024311, 0.07333191, -0.02508869, 0.01686102, 0.01045217, -0.07310145, -0.01285514, 0.09339073, -0.06714858, -0.09901267, 0.0068216 , 0.03572355, -0.03935919, 0.03302537, 0.02549176, 0.0144202 , -0.02991694, -0.01354563, -0.00787938, 0.03613688, 0.05657197, -0.00474709, -0.02503674, -0.0273344 , 0.05442371, -0.01384753, 0.00932324, -0.04490621, -0.03971119, 0.02634538, -0.02593908, 0.04915658, 0.04001555, -0.1161194 , -0.08524479, -0.04748566], dtype=float32) . Veamos la distancia entre embeddings de t√≥kenes similares, por ejemplo debido a un error ortogr√°fico, y la de t√≥kenes dis√≠miles, de diferente significado. . Para ello utilizar√© la similitud del coseno, una f√≥rmula trigonom√©trica que en la definici√≥n de scipy es igual a cero si ambos vectores apuntan a un mismo lugar; cualquier √°ngulo existente entre los vectores, arrojar√≠a un valor mayor a cero. . Los √≠ndices son cateor√≠as que nada dicen de la relaci√≥n entre las palabras pero los vectores s√≠. . https://en.wikipedia.org/wiki/Cosine_similarity | https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html | . from scipy.spatial import distance distance.cosine(ft[&#39;tarjeta&#39;], ft[&#39;tarjeta&#39;]) . 0.0 . Error ortogr√°fico: . distance.cosine(ft[&#39;tarjeta&#39;], ft[&#39;targeta&#39;]) . 0.18840116262435913 . Otra palabra: . distance.cosine(ft[&#39;tarjeta&#39;], ft[&#39;saldo&#39;]) . 0.6775485575199127 . La siguente funci√≥n servir√° para . obtener los vectores de cada uno de los t√≥kenes del vocabulario, | en el orden de los √≠ndices del vocabulario (es importante mantener este orden), | convertirlos en tensores de PyTorch (map aplica la funci√≥n torch.tensor a cada uno de los vectores), | list convierte el mapeo es una lista, ya que map es lazy, no acciona hasta que se lo piden y convertirlo en lista es una manera de pedirlo, | torch.stack apila los tensores de la lista (cada uno tiene dimensi√≥n 300 y la lista tiene largo $N$, el tama√±o del vocabulario) en un tensor bidimensional de $N times 300$. | import torch # versi√≥n 1 def obtener_embeddings(t√≥kenes, fastText): embeddings = [fastText[t√≥ken] for t√≥ken in t√≥kenes] return torch.stack( list( map(torch.tensor, embeddings) ) ) . embeddings = obtener_embeddings(v.vocabulario, ft) embeddings . tensor([[-0.0503, -0.0404, 0.0759, ..., 0.0252, -0.0356, -0.0142], [ 0.0093, 0.0350, 0.0453, ..., -0.0111, -0.0165, -0.0326], [ 0.0547, 0.0112, 0.1910, ..., 0.0066, -0.0021, -0.0230], ..., [-0.0278, -0.0258, 0.0990, ..., 0.0018, -0.0074, -0.0465], [ 0.0149, -0.0274, 0.0268, ..., 0.0571, 0.0106, -0.0065], [-0.0097, 0.0221, -0.0038, ..., -0.0042, 0.0152, 0.0462]]) . Entonces ahora podemos salvarlos para no tener que volver a generarlos, obviando as√≠ cargar el modelo de fastText. . torch.save(embeddings, &#39;vectores.pkl&#39;) . Si queremos cargarlos m√°s adelante: . embeddings = torch.load(&#39;vectores.pkl&#39;) . Extra: bolsa de palabras . Hay una forma simple y efectiva de obtener la representaci√≥n de un documento, si bien existen otras que son mejores. Los vectores son representaciones de t√≥kenes, los documentos son conjuntos de t√≥kenes, calcular la suma, el promedio o el m√°ximo de los vectores del conjunto nos da un vector que es la representaci√≥n del documento. Como esta agregaci√≥n no tiene en cuenta el orden de los t√≥kenes en el documento se llama bolsa de palabras, o en ingl√©s bag of words. . cambio_cien = obtener_embeddings([&#39;se√±or&#39;, &#39;tiene&#39;, &#39;cambio&#39;, &#39;de&#39;, &#39;cien&#39;], ft) cambio_cien.shape . torch.Size([5, 300]) . Hacemos la agregaci√≥n es en sentido de las columnas, cada columna o dimensi√≥n del embedding es un atributo o feature del t√≥ken, queremos obtener los atributos para el documento. . cambio_cien = torch.mean(cambio_cien, dim=0) cambio_cien.shape . torch.Size([300]) . Representaci√≥n de una variante del documento: . cambio_mil = obtener_embeddings([&#39;se√±or&#39;, &#39;tiene&#39;, &#39;cambio&#39;, &#39;de&#39;, &#39;mil&#39;], ft) cambio_mil = torch.mean(cambio_mil, dim=0) . Representaci√≥n de un documento bien diferente: . extrav√≠o = obtener_embeddings([&#39;extravi√©&#39;, &#39;mi&#39;, &#39;tarjeta&#39;, &#39;de&#39;, &#39;d√©bito&#39;, &#39;anoche&#39;], ft) extrav√≠o = torch.mean(extrav√≠o, dim=0) . Ahora veamos las distancias entre los documentos. . distance.cosine(cambio_cien, cambio_mil) . 0.08124548196792603 . distance.cosine(cambio_cien, extrav√≠o) . 0.39364296197891235 . Vemos que se√±or tiene cambio de cien est√° m√°s cerca de se√±or tiene cambio de mil que de extravi√© mi tarjeta de d√©bito anoche. . En la parte 2 hube mencionado a nn.EmbeddingBag sin contar su finalidad; es un m√≥dulo de PyTorch que hace exactamento esto: recibe un tensor con √≠ndices de t√≥kenes de documentos, reemplaza a los √≠ndices por vectores y los agrega en un vector por documento, usando una funci√≥n que puede ser mean, max, sum. . Inicializar los pesos de la capa de embeddings . El m√©todo copy_ carga el tensor de los pesos en el m√≥dulo de embeddings. Para que la carga funcione las dimensiones del tensor de pesos debe ser exactamente igual a las de la capa. Inicializamosla con cantidad de filas igual al largo del vocabulario y cantidad de columnas igual al tama√±o de los vectores. . capa = nn.EmbeddingBag(len(v), ft.get_dimension(), mode=&#39;mean&#39;) capa . EmbeddingBag(8116, 300, mode=mean) . Chequeamos las dimensiones del tensor de pesos. . embeddings.shape . torch.Size([8116, 300]) . Al inicializar la capa, sus pesos se inicializan con valores al azar. Es con el entrenamiento que adquieren valores significativos para red neuronal. Los embeddings pre-entrenados sirven justamente para comenzar con valores con sentido, lo que acorta los tiempos de aprendizaje de la red en general. . capa.weight.data.copy_(embeddings) . tensor([[-0.0503, -0.0404, 0.0759, ..., 0.0252, -0.0356, -0.0142], [ 0.0093, 0.0350, 0.0453, ..., -0.0111, -0.0165, -0.0326], [ 0.0547, 0.0112, 0.1910, ..., 0.0066, -0.0021, -0.0230], ..., [-0.0278, -0.0258, 0.0990, ..., 0.0018, -0.0074, -0.0465], [ 0.0149, -0.0274, 0.0268, ..., 0.0571, 0.0106, -0.0065], [-0.0097, 0.0221, -0.0038, ..., -0.0042, 0.0152, 0.0462]]) . √≠ndices = v.t√≥kenes_a_√≠ndices([ [&#39;se√±or&#39;, &#39;tiene&#39;, &#39;cambio&#39;, &#39;de&#39;, &#39;cien&#39;], [&#39;se√±or&#39;, &#39;tiene&#39;, &#39;cambio&#39;, &#39;de&#39;, &#39;mil&#39;], [&#39;extravi√©&#39;, &#39;mi&#39;, &#39;tarjeta&#39;, &#39;de&#39;, &#39;d√©bito&#39;, &#39;anoche&#39;], ]) √≠ndices . [[1, 119, 142, 2, 1], [1, 119, 142, 2, 1311], [2268, 11, 5, 2, 149, 1443]] . Por c√≥mo creamos el vocabulario y por c√≥mo est√° definida la clase Vocab, el t√≥ken &lt;unk&gt; de t√≥ken desconocido o fuera del vocabulario tiene asignado el √≠ndice 1; esto ser√° relevante m√°s adelante. . Recordemos que a este m√≥dulo le gusta que los documentos sean contiguos (un √∫nico documento) y que por otro lado le informemos en qu√© posiciones de ese documento contiguo comienza cada uno de los documentos. . Veamos el largo de cada uno de los documentos. . list(map(len, √≠ndices)) . [5, 5, 6] . El primer documento siempre comienza en la posici√≥n 0, el segundo lo hace 5 t√≥kenes/√≠ndices despu√©s, y el tercero en 5 luego del segundo, o sea en la posici√≥n 10. . posiciones = torch.tensor([0, 5, 10]) . Ahora convertimos a los documentos en un documento √∫nico y adem√°s en un tensor. . √≠ndices = torch.tensor([ 1, 119, 142, 2, 1, 1, 119, 142, 2, 1311, 2268, 11, 5, 2, 149, 1443 ]) . Luego de estos procesamientos la capa ejecuta las mismas operaciones que realizamos manualmente. . vectores = capa(√≠ndices, posiciones) vectores.shape . torch.Size([3, 300]) . Lo que podemos verificar calculando la distancia entre se√±or tiene cambio de cien est√° m√°s cerca de se√±or tiene cambio de mil, que manualmente dio $0.081$. . distance.cosine(vectores[0].detach().numpy(), vectores[1].detach().numpy()) . 0.1781657338142395 . Y no se cumpli√≥. üòµ . La explicaci√≥n est√° en las palabras fuera del vocabulario. Los embeddings pre-entrenados no suelen venir con pesos para t√≥kenes especiales como &lt;unk&gt; y al hacer ft[&#39;&lt;unk&gt;&#39;], fastText que est√° preparado para generar vectores para t√≥kenes con los cuales no fue entrenado, devuelve un vector con pesos sin sentido. Es decir, fastText es muy √∫til para obtener vectores aproximados cuando le preguntamos por un t√≥ken que no conoce pero que es parecido a otros que s√≠, sin embargo &lt;unk&gt; no se a parece a ning√∫n otro. Nota: Word2Vec y GloVe no tienen soporte para t√≥kenes fuera del vocabulario (OOV), en el caso de &lt;unk&gt; no hubieran devuelto ning√∫n valor. . ¬øQu√© podr√≠amos haber hecho? . Si contamos con soporte para OOV (fastText), no usar el t√≥ken &lt;unk&gt; ya que no es necesario. Para ello deber√≠amos haber creado el vocabulario inicilizando la clase Vocab con el argumento t√≥ken_desconocido=None. | Si no hay soporte para OOV, salvo que el modelo especifique que cuenta con un vector para el t√≥ken especial desconocido (y que no necesariamente se simbolizar√° con &lt;unk&gt;), no usar el t√≥ken &lt;unk&gt; ya que no es posible. | Entrenar vectores desde cero. Al existir &lt;unk&gt;, este adquire pesos con el sentido propuesto. No era la idea. | Crear un vector a partir de los existentes, seg√∫n est√° expresado en esta respuesta de StackOverflow. | . Creando un vector desconocido . La respuesta de StackOverflow del √∫ltimo punto sugiere que el vector promedio de todos los vectores o, de al menos los que se van a usar, conforman un buen vector desconocido. . unk = embeddings.mean(dim=0) unk.shape . torch.Size([300]) . Creando un vector de relleno . Otro t√≥ken especial que consideramos es el relleno, &lt;pad&gt;, que sirve para completar los espacios en documentos de distinto largo cuando los queremos agrupar en un tensor. Normalmente los pesos para este vector son todos ceros. . pad = torch.zeros(ft.get_dimension()) pad.shape . torch.Size([300]) . pad.shape . torch.Size([300]) . Incluyendo los nuevos cambios . def obtener_embeddings(t√≥kenes, fastText, t√≥ken_desconocido=&#39;&lt;unk&gt;&#39;, t√≥ken_relleno=&#39;&lt;pad&gt;&#39;): embeddings = [fastText[t√≥ken] for t√≥ken in t√≥kenes if t√≥ken not in (t√≥ken_desconocido, t√≥ken_relleno)] embeddings = torch.stack( list( map(torch.tensor, embeddings) ) ) if t√≥ken_desconocido: unk = embeddings.mean(dim=0, keepdim=True) embeddings = torch.cat([unk, embeddings]) if t√≥ken_relleno: pad = torch.zeros(1, fastText.get_dimension()) embeddings = torch.cat([pad, embeddings]) return embeddings . embeddings = obtener_embeddings(v.vocabulario, ft) embeddings . tensor([[ 0.0000, 0.0000, 0.0000, ..., 0.0000, 0.0000, 0.0000], [ 0.0008, -0.0102, 0.0071, ..., -0.0066, 0.0097, -0.0056], [ 0.0547, 0.0112, 0.1910, ..., 0.0066, -0.0021, -0.0230], ..., [-0.0278, -0.0258, 0.0990, ..., 0.0018, -0.0074, -0.0465], [ 0.0149, -0.0274, 0.0268, ..., 0.0571, 0.0106, -0.0065], [-0.0097, 0.0221, -0.0038, ..., -0.0042, 0.0152, 0.0462]]) . Inicializar los pesos en un modelo . Respecto del modelo de la parte 2, la diferencia est√° en el m√©todo init_weights que carga el tensor de los pesos en la capa de embeddings y que es llamado durante la inicializaci√≥n del modelo. Recordemos: para que la carga funcione (copy_) las dimensiones del tensor de pesos debe ser exactamente igual a las de la capa de embedding. . Adem√°s congelamos los pesos (requires_grad = False) para que no cambien durante el entrenamiento. Lo que se aconseja es entrenar el resto de las capas hasta que la funci√≥n de p√©rdida se estabilice; dejar libres a los pesos de la capa de embeddings cuando el resto de la red tiene pesos con valores aleatorios har√° que los embeddings var√≠en significativamente durante el aprendizaje y pierdan sentido. Suele ser √∫til descongelar los pesos una vez que el modelo ha alcanzado cierto nivel de aprendizaje para efectuar un aprendizaje fino, en el que los embeddings se adaptar√°n al problema en cuesti√≥n. . import torch.nn as nn import torch.nn.functional as F DIM_EMBEDDINGS = 8 class ClasificadorBolsa(nn.Module): def __init__(self, vocab_size, embed_dim, num_class): super().__init__() self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False, mode=&#39;max&#39;) self.fc = nn.Linear(embed_dim, num_class) # inicializamos los pesos self.init_weights() def init_weights(self): self.embedding.weight.data.copy_(embeddings) self.embedding.weight.data.requires_grad = False def forward(self, text, offsets): embedded = self.embedding(text, offsets) return self.fc(embedded) . nn.Embedding . Hemos visto con algo de detalle el m√≥dulo de PyTorch nn.EmbeddingBag, una capa de doble acci√≥n: convierte √≠ndices en vectores y calcula un vector agregado, una forma simple de obtener una representaci√≥n de un documento, aunque no la m√°s efectiva de todas. Para lograr mejores representaciones encontramos en uso modelos m√°s complejos. La primera capa de modelos que usan capas LSTM o Transformer es una nn.Embedding, que a diferencia de la mencionada anteriormente es de simple acci√≥n: convierte √≠ndices en vectores y ya. . Quiero ilustrar brevemente c√≥mo son la entrada y la salida de esta capa, ya que son bien diferentes a las de nn.EmbeddingBag. La inicializaci√≥n sin embargo, es similar. El tensor de los pesos tendr√° las dimensiones de tama√±o del vocabulario por la dimensi√≥n (valga la redundancia) de los embeddings. . capa = nn.Embedding(len(v), ft.get_dimension(), padding_idx=v.√≠ndice_relleno) . Diferentemente, como esta capa requiere el uso del t√≥ken de relleno, podemos especificar el √≠ndice del t√≥ken para que la capa inicialice sus pesos al azar excepto los de este vector, que ser√° inicializado en cero. Si lo deseamos, podemos utilizar vectores pre-entrenados. . capa.weight.data.copy_(embeddings) capa.weight.data.requires_grad = False . Ahora armaremos un lote de documentos y lo convertiremos en un tensor. Para poder hacer esto √∫ltimo es fundamental que los documentos tengan el mismo largo (que ser√° igual al del documento m√°s largo), as√≠ que nos valdremos del t√≥ken de relleno para lograrlo. . √≠ndices = v.t√≥kenes_a_√≠ndices([ [&#39;se√±or&#39;, &#39;tiene&#39;, &#39;cambio&#39;, &#39;de&#39;, &#39;cien&#39;, &#39;&lt;pad&gt;&#39;], [&#39;se√±or&#39;, &#39;tiene&#39;, &#39;cambio&#39;, &#39;de&#39;, &#39;mil&#39;, &#39;&lt;pad&gt;&#39;], [&#39;extravi√©&#39;, &#39;mi&#39;, &#39;tarjeta&#39;, &#39;de&#39;, &#39;d√©bito&#39;, &#39;anoche&#39;], ]) √≠ndices = torch.tensor(√≠ndices) √≠ndices.shape . torch.Size([3, 6]) . Tenemos un tensor bidimensional, la dimensi√≥n 0 (filas) es la cantidad de documentos del lote, la dimensi√≥n 1 (columnas) es el tama√±o de los documentos. . As√≠ luce el tensor de √≠ndices. . √≠ndices . tensor([[ 1, 119, 142, 2, 1, 0], [ 1, 119, 142, 2, 1311, 0], [2268, 11, 5, 2, 149, 1443]]) . Ahora lo hacemos pasar por la capa de embeddings. . vectores = capa(√≠ndices) vectores.shape . torch.Size([3, 6, 300]) . Observamos que la capa anadi√≥ una nueva dimensi√≥n, ahora tenemos un tensor tridimensional. Reemplaz√≥ cada √≠ndice (un escalar) por su vector correspondiente de largo 300. La dimensi√≥n 2 (profundidad) siempre corresponder√° al tama√±o del embedding. . Aqu√≠ termina la serie de art√≠culos de pre-procesamiento de texto. Gracias por haber llegado hasta el fin. .",
            "url": "https://matiasbattocchia.github.io/datitos/Preprocesamiento-de-texto-para-NLP-parte-3.html",
            "relUrl": "/Preprocesamiento-de-texto-para-NLP-parte-3.html",
            "date": " ‚Ä¢ Oct 13, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Ejemplo de c√≥mo usar Ignite",
            "content": "Recientemente han salido varias librer√≠as de alto nivel para PyTorch. Una vez planteados los datasets y los modelos todav√≠a queda bastante por programar, sobre todo los bucles de entrenamiento y validaci√≥n, c√≥digo que salvando detalles de implementaci√≥n es siempre el mismo (boilerplate). Librer√≠as como PyTorch Lighting y PyTorch Ignite prometen ahorrarnos el c√≥digo repetitivo y concentrarnos en lo particular. . Se instala con . pip install pytorch-ignite . import torch from sklearn.metrics import balanced_accuracy_score from ignite.engine import Engine, Events from ignite.metrics import Accuracy, Loss . if torch.cuda.is_available(): device = torch.device(&#39;cuda&#39;) else: device = torch.device(&#39;cpu&#39;) . Definimos el tri√°ngulo modelo-optimizador-criterio. Lo de siempre. . modelo = Modelo().to(device) optimizador = torch.optim.Adam(modelo.parameters(), lr=1e-3, weight_decay=1e-5) criterio = torch.nn.CrossEntropyLoss() . Bucle de entrenamiento. . def entrenar(motor, lote): modelo.train() optimizador.zero_grad() predicciones = modelo(lote.documentos.to(device)) p√©rdida = criterio(predicciones, lote.etiquetas.to(device)) p√©rdida.backward() optimizador.step() return p√©rdida.item() entrenador = Engine(entrenar) . Bucle de evaluaci√≥n. Sirve para todo lo que no es entrenamiento, a ser validaci√≥n e inferencia. . def evaluar(motor, lote): modelo.eval() with torch.no_grad(): predicciones = modelo(lote.documentos.to(device)) # validaci√≥n if lote.etiquetas is not None: return predicciones, lote.etiquetas.to(device) # inferencia return predicciones evaluador = Engine(evaluar) inferidor = Engine(evaluar) . Adjuntamos algunas m√©tricas al evaluador. Notar que entrenar devuelve el valor de la funci√≥n de p√©rdida mientras que evaluar devuelve predicciones. . Accuracy().attach(evaluador, &#39;accuracy&#39;) Loss(criterio).attach(evaluador, &#39;loss&#39;) . Evento: al completar una √©poca de entrenamiento. | Acci√≥n: registrar m√©tricas del dataset de entrenamiento. Para ello usamos evaluador.run(train_dl). | . @entrenador.on(Events.EPOCH_COMPLETED) def loguear_resultados_entrenamiento(entrenador): evaluador.run(train_dl) # accedemos a este atributo gracias a haber adjuntado m√©tricas previamente m√©tricas = evaluador.state.metrics print(f&quot;[{entrenador.state.epoch:02}] TRAIN Accuracy: {m√©tricas[&#39;accuracy&#39;]:.2f} Loss: {m√©tricas[&#39;loss&#39;]:.2f}&quot;) . Evento: al completar una √©poca de entrenamiento. Ac√° podr√≠a ser al completar X √©pocas para no validar tan seguido. | Acci√≥n: registrar m√©tricas del dataset de validaci√≥n. Para ello usamos evaluador.run(valid_dl). | . Esta funci√≥n podr√≠a hacer sido m√°s parecida a la de arriba pero no lo es porque queremos calcular una m√©trica que no viene con Ignite. Tenemos dos opciones, definir una m√©trica adjuntable como ignite.metrics.Accuracy ‚Äîque no lo hicimos y quiz√°s hubiese sido lo mejor‚Äî o poner la l√≥gica en la funci√≥n, como vemos aqu√≠. . @entrenador.on(Events.EPOCH_COMPLETED) def loguear_resultados_validaci√≥n(entrenador): # esta artima√±a tendr√° sentido m√°s adelante evaluador.predicciones = [] evaluador.etiquetas = [] evaluador.run(valid_dl) # de todas las categor√≠as nos quedamos con la m√°s probable para cada muestra predicciones = torch.cat(evaluador.predicciones).argmax(dim=1).cpu() etiquetas = torch.cat(evaluador.etiquetas).cpu() score = balanced_accuracy_score(etiquetas, predicciones) m√©tricas = evaluador.state.metrics print(f&quot;[{entrenador.state.epoch:02}] VALID Accuracy: {m√©tricas[&#39;accuracy&#39;]:.2f} Loss: {m√©tricas[&#39;loss&#39;]:.2f} Balanced accuracy: {score:.2f}&quot;) . Evento: al procesar un lote de validaci√≥n. | Acci√≥n: almacenar predicciones y etiquetas. | . Esto le da sentido a la artima√±a que mencionamos. La misma sirve para instanciar listas vac√≠as al inicio de la validaci√≥n, a las que se le agregaran los resultados de cada lote. . @evaluador.on(Events.ITERATION_COMPLETED) def colectar_validaciones_lote(evaluador): evaluador.predicciones.append(evaluador.state.output[0]) evaluador.etiquetas.append(evaluador.state.output[1]) . Evento: al completar el entrenamiento (todas las √©pocas). | Acci√≥n: realizar inferencias. Para ello usamos inferidor.run(infer_dl). | . @entrenador.on(Events.COMPLETED) def colectar_inferencias(entrenador): print(&#39;Realizando inferencias...&#39;) # mismo truco de antes inferidor.y_pred = [] inferidor.run(infer_dl) # de todas las categor√≠as nos quedamos con la m√°s probable para cada muestra y_pred = torch.cat(inferidor.y_pred).argmax(dim=1).reshape(-1,1) # quiz√°s sea un buen momento para recuperar las categor√≠as originales #y_pred = vocabulario_etiquetas.√≠ndices_a_t√≥kenes(y_pred) # ya que estamos, guardamos los resultados en un CSV pd.DataFrame(y_pred).to_csv(&#39;submit.csv&#39;, header=False) . Evento: al procesar un lote de inferencia. | Acci√≥n: almacenar predicciones. | . @inferidor.on(Events.ITERATION_COMPLETED) def colectar_inferencias_lote(inferidor): inferidor.y_pred.append(inferidor.state.output) . Finalmente largamos el entrenamiento con entrenador.run(train_dl). Este es el engranaje principal del mecanismo, que al completar bucles mover√° a los otros engranajes (Engines, que no son literalmente enganajes pero puede que sea una buena met√°fora). . entrenador.run(train_dl, max_epochs=5) . [01] VALID Accuracy: 0.80 Loss: 1.11 Balanced accuracy: 0.75 [01] TRAIN Accuracy: 0.98 Loss: 0.02 [02] VALID Accuracy: 0.80 Loss: 1.11 Balanced accuracy: 0.75 [02] TRAIN Accuracy: 0.98 Loss: 0.02 [03] VALID Accuracy: 0.80 Loss: 1.11 Balanced accuracy: 0.75 [03] TRAIN Accuracy: 0.98 Loss: 0.02 [04] VALID Accuracy: 0.80 Loss: 1.11 Balanced accuracy: 0.75 [04] TRAIN Accuracy: 0.98 Loss: 0.02 [05] VALID Accuracy: 0.80 Loss: 1.11 Balanced accuracy: 0.75 [05] TRAIN Accuracy: 0.98 Loss: 0.02 Realizando inferencias... . Recursos . 8 Creators and Core Contributors Talk About Their Model Training Libraries From PyTorch Ecosystem | .",
            "url": "https://matiasbattocchia.github.io/datitos/PyTorch-Ignite.html",
            "relUrl": "/PyTorch-Ignite.html",
            "date": " ‚Ä¢ Aug 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Preprocesamiento de texto para NLP (parte 2)",
            "content": "En la primera parte llegamos a convertir esto . [ &#39;que se requiere para un prestamo personal&#39;, &#39;me piden mi numero de cuenta es mi cbu&#39;, ] . en esto . [ [4160, 4683, 4484, 3703, 5294, 4011, 3825], [3275, 3854, 3319, 3554, 1532, 1462, 2151, 3319, 950], ] . donde dijimos que las partes fundamentales son la tokenizaci√≥n ‚Äîseparar a los documentos en unidades de informaci√≥n‚Äî y la numericalizaci√≥n ‚Äîel asignarle a cada uno de los t√≥kenes un n√∫mero, m√°s que nada para que la computadora, que gusta mucho de los n√∫meros, sea feliz‚Äî. . Tambi√©n hab√≠amos dicho que un t√≥ken es un atributo pero no dijimos mucho m√°s al respecto. Veamos c√≥mo puede ser esto. La tarea de ejemplo es clasificar documentos. Estamos acustumbrados a tener muestras y etiquetas como X e y en las que la primera es una matriz de muestras (filas) y atributos (columnas), y la segunda suele ser una columna. Cuando el dataset est√° sin pre-procesar tenemos las muestras (filas) pero no los atributos (columnas), por lo general tenemos una √∫nica columna con los documentos en forma de strings, lo que mucha forma de atributos no tiene. . Ahora que hemos pre-procesado el texto estamos a un paso de obtener los atributos. La funci√≥n de los atributos es describir o caracterizar a las muestras. El modelo lee estos atributos para realizar inferencias. Hay distintas maneras de describir a los documentos, algunas m√°s sofisticadas que otras, una intuitiva es aprovechar que los t√≥kenes est√°n numerados desde 0 hasta L (len(vocabulario)) y otorgarle una columna a cada uno en la matriz de atributos de tama√±o N x L (donde N es la cantidad de muestras). . Hecho esto, solo resta contar cu√°ntas veces aparece cada t√≥ken en cada documento y asentarlo en la matriz. . | bien hola si todo - &#39;hola todo bien&#39; | 1 1 0 1 &#39;si bien bien&#39; | 2 0 1 0 . Como comentario, esta forma de describir los documentos ignora enteramente el √≥rden de los t√≥kenes, sabemos que el sentido de una oraci√≥n puede cambir completamente si cambiamos algunas palabras de lugar. Para el problema en cuesti√≥n, no parece ser tan grave ya que para clasificar una pregunta podr√≠a bastar con reconocer algunas palabras claves como cambio y clave o requisito y pr√©stamo. . Ver tf-idf. . PyTorch . El t√≠pico bucle de entrenamiento de PyTorch tiene esta pinta. . for √©poca in range(N_√âPOCAS): for lote in datos_entrenamiento: # reseteamos los gradientes optimizador.zero_grad() predicciones = red_neuronal(lote.X) p√©rdida = criterio(predicciones, lote.y) # calculamos los gradientes p√©rdida.backward() # aplicamos los gradientes optimizador.step() . Recordemos que a diferencia de otros modelos las redes neuronales revisitan varias veces el dataset, en lo que se llaman √©pocas, cada √©poca es un recorrido por todas las muestras de entrenamiento. . En una √©poca el dataset se puede mostrar entero, de a una muestra, o como es com√∫n hoy en d√≠a de a grupos o lotes (batches). La experiencia mostr√≥ que es √∫til variar el orden de las muestras en cada √©poca. . PyTorch provee ciertas facilidades para el manejo de los datos con las clases definidas en torch.utils.data a ser: . Dataset. Organiza los datos. Le pasamos un n√∫mero o √≠ndice de muestra y nos devuelve la muestra usualmente como una tupla (atributos, etiqueta). | Sampler. Salvo que lo queramos de otra manera, se encarga de brindar un orden aleatoreo de los √≠ndices del dataset; uno diferente cada vez que le preguntamos. | BatchSampler. Por defecto, se inicializa con un Sampler y el tama√±o de lote. Se encarga de armar grupos de √≠ndices; diferentes cada vez que le preguntamos. | DataLoader. Vali√©ndose de los grupos de √≠ndices de BatchSampler, obtiene muestras de Dataset. De esta manera para cada √©poca devuelve lotes de muestras al azar. | Por Sampler y BatchSampler no nos detendremos ya el comportamiento por defecto, que es barajar el dataset en cada √©poca y armar lotes del mismo tama√±o es todo lo que necesitamos. . Dataset . from torch.utils.data import Dataset class Textset(Dataset): def __init__(self, documentos, etiquetas=None): self.documentos = documentos self.etiquetas = etiquetas or np.full(len(documentos), np.nan) def __len__(self): return len(self.documentos) def __getitem__(self, item): return self.documentos[item], self.etiquetas[item] . Es una clase que necesita implementar __len__ y __getitem__. Podr√≠a encargarse de levantar y pre-procesar el dataset, que por comodidad lo hemos cargado con Pandas y pre-procesado por fuera: el constructor (__init__) podr√≠a recibir el nombre del archivo, leerlo y aplicarle las funciones pertinentes. No lo hemos hecho internamente porque el vocabulario debe nutrirse del dataset de entrenamiento ya pre-procesado [falta]. . Tambi√©n necesitaremos crear un Textset para el dataset de inferencia, para el cual no contamos con las etiquetas. En el caso de no pasar etiquetas generamos una lista llena de NaNs del mismo largo que la lista de documentos. . train_ds = Textset(train_√≠ndices, etiquetas_train_√≠ndices) len(train_ds) . 18093 . train_ds[10_000] . ([8, 169, 1, 4652, 0, 17, 65], 40) . Es bastante similar a lo que una lista de tuplas podr√≠a lograr, aunque fue una buena oportunidad para juntar los documentos y las etiquetas que luego de cargar el DataFrame y hasta ahora recorrieron caminos separados. Lo realmente importante es el DataLoader, no podemos usar una lista como dataset porque requiere que sea una instancia de Dataset. . DataLoader . DataLoader es un iterable. Los iterables son colecciones de elementos que se pueden recorrer; implementan el m√©todo __iter__, del que se espera que devuelva un objeto iterador (iterador = iter(iterable)). A su vez el iterador implementa el m√©todo __next__ que se encarga devolver secuencialmente los elementos de la colecci√≥n hasta que se agota; una vez que esto sucede el iterador debe ser descartado y en todo caso le pedimos al iterable que nos arme un nuevo iterador. Cuando usamos la construcci√≥n for √≠tem in iterable, el int√©rprete de Python impl√≠citamente obtiene un iterador. . Ver la secci√≥n de interables en el tutorial de Python. . lista = iter([&#39;uno&#39;,&#39;dos&#39;]) next(lista) . &#39;uno&#39; . next(lista) . &#39;dos&#39; . next(lista) . StopIteration Traceback (most recent call last) &lt;ipython-input-203-cfa830c9416d&gt; in &lt;module&gt; -&gt; 1 next(lista) StopIteration: . No hay pr√≥ximo elemento. Cuando se llega al fin del iterador se levanta la excepci√≥n StopIteration. . Suficientes detalles por ahora. Todo esto para decir que DataLoader es un iterable que particularmente devuelve un iterador distinto cada vez, a diferencia de una lista en la que los elementos siempre se recorren en el mismo orden. Es decir, se trata de una colecci√≥n de lotes pero cada iterador agrupa lotes seg√∫n como dicte BatchSampler, que suele ser aleatorio. . En cada √©poca le pedimos un iterador a DataLoader, por lo que recorremos todo el Dataset agrupado en lotes de manera diferente cada vez. . from torch.utils.data import DataLoader train_dl = DataLoader(train_ds, batch_size=32, shuffle=True) . Le estamos diciendo a DataLoader que queremos lotes de 32 muestras (batch_size) y que el armado de los lotes sea aleatorio (shuffle). . un_lote = next(iter(train_dl)) un_lote . [[tensor([ 12, 5168, 26, 9, 16, 8, 24, 10, 8, 15, 49, 49, 46, 16, 15, 8, 1, 62, 26, 12, 8, 7, 12, 157, 44, 2082, 5, 62, 1, 76, 8, 74]), tensor([ 140, 10, 75, 4, 4, 22, 84, 1519, 48, 75, 27, 357, 105, 40, 48, 1911, 213, 585, 14, 48, 19, 203, 102, 164, 57, 0, 2, 22, 1009, 262, 274, 630]), tensor([ 17, 51, 371, 1058, 64, 4, 71, 27, 17, 9, 1, 2, 59, 57, 2, 6, 0, 928, 62, 6, 1973, 3, 17, 713, 10, 36, 56, 4, 18, 21, 1, 5]), tensor([ 56, 67, 83, 0, 22, 724, 18, 24, 3, 765, 64, 207, 1454, 2, 765, 36, 179, 2, 358, 13, 38, 236, 56, 3, 3, 0, 5, 32, 1, 98, 1009, 6])], tensor([144, 153, 247, 3, 55, 0, 223, 15, 18, 6, 26, 160, 89, 199, 149, 49, 260, 285, 13, 3, 198, 18, 23, 0, 1, 103, 35, 112, 128, 20, 128, 3])] . Est√° bueno que ya veamos tensores de PyTorch porque vamos a necesitar los datos en forma de tensor para alimentar a la red neuronal. Sin embargo, algo no parece andar bien con el lote que acabamos de obtener. . len(un_lote) . 2 . Tenemos dos elementos adentro del lote, podr√≠amos pensar que el primero agrupa documentos y el segundo, etiquetas. . type(un_lote[1]), len(un_lote[1]) . (torch.Tensor, 32) . Las etiquetas del lote est√°n perfecto, son un tensor de una dimensi√≥n con largo 32. . type(un_lote[0]), len(un_lote[0]) . (list, 4) . En cambio la agrupaci√≥n de documentos no tiene sentido. Es otra lista de tama√±o 4 con tensores adentro. ¬øQu√© est√° pasando? . Tensores . El problema parece radicar en los tensores. Son estructuras que las podemos imaginar como una columna cuando tienen una dimensi√≥n, una tabla cuando son dos, un cubo cuando tres... . Los tensores son similares a los ndarrays de NumPy, con el aditivo que tambi√©n pueden ser usados en la GPU para acelerar los c√≥mputos. Ver m√°s de tensores en el tutorial de PyTorch. . En el caso de los documentos que a la altura del Dataset son listas de listas de √≠ndices, son dos dimensiones, y al llevarlos a una tabla vemos que tendr√≠amos tantas filas como documentos y tantas columnas como √≠ndices tenga el documento m√°s largo de la colecci√≥n pero que no todos los documentos tienen tantos √≠ndices como columnas la tabla. . √≠ndices = [ [2,2], [4,4,4,4], [7,7,7,7,7,7,7], ] √≠ndices . [[2, 2], [4, 4, 4, 4], [7, 7, 7, 7, 7, 7, 7]] . import torch torch.tensor(√≠ndices) . ValueError Traceback (most recent call last) &lt;ipython-input-232-121200966211&gt; in &lt;module&gt; 1 import torch 2 -&gt; 3 torch.tensor(√≠ndices) ValueError: expected sequence of length 2 at dim 1 (got 4) . Como anticipamos, no le gust√≥ nada. . T&#243;kenes especiales . Lo mencionamos al pasar, a veces se utilizan t√≥kenes especiales como &lt;separador de palabra&gt;, &lt;separador de oraci√≥n&gt;, &lt;inicio del texto&gt;, &lt;fin del texto&gt;. Hay de todo tipo, seg√∫n la tarea a realizar. Uno que est√° presente generalmente en los proyectos es el t√≥ken de relleno &lt;relleno&gt; (en ingl√©s padding). . El t√≥ken de relleno nos va a servir para hacer que todos los documentos tengan el mismo largo y finalmente podamos convertirlos en un tensor. No lo vamos a hacer inmediatamente ya que no nos interesa que tengan el mismo largo en todo el dataset sino en todo el lote. Como los lotes son generados en el DataLoader, este √∫ltimo tendr√° que encargarse de rellenar los documentos. . Vamos a modificar Vocab quien se encarga de la lista de t√≥kenes para que incluya a &lt;relleno&gt;. . import numpy as np from itertools import chain from collections import Counter class Vocab(): @property def √≠ndice_relleno(self): return self.mapeo.get(self.t√≥ken_relleno) def __init__(self, t√≥ken_desconocido=&#39;&lt;unk&gt;&#39;, t√≥ken_relleno=&#39;&lt;pad&gt;&#39;, frecuencia_m√≠nima=0.0, frecuencia_m√°xima=1.0, longitud_m√≠nima=1, longitud_m√°xima=np.inf, stop_words=[], l√≠mite_vocabulario=None): self.t√≥ken_desconocido = t√≥ken_desconocido self.t√≥ken_relleno = t√≥ken_relleno self.frecuencia_m√≠nima = frecuencia_m√≠nima self.frecuencia_m√°xima = frecuencia_m√°xima self.longitud_m√≠nima = longitud_m√≠nima self.longitud_m√°xima = longitud_m√°xima self.stop_words = stop_words self.l√≠mite_vocabulario = l√≠mite_vocabulario # ning√∫n cambio aqu√≠ def reducir_vocabulario(self, lote): contador_absoluto = Counter(chain(*lote)) contador_documentos = Counter() for doc in lote: contador_documentos.update(set(doc)) # frecuencia m√≠nima if isinstance(self.frecuencia_m√≠nima, int): # frecuencia de t√≥ken vocabulario_m√≠n = [t√≥ken for t√≥ken, frecuencia in contador_absoluto.most_common() if frecuencia &gt;= self.frecuencia_m√≠nima] else: # frecuencia de documento vocabulario_m√≠n = [t√≥ken for t√≥ken, frecuencia in contador_documentos.most_common() if frecuencia/len(lote) &gt;= self.frecuencia_m√≠nima] # frecuencia m√°xima if isinstance(self.frecuencia_m√°xima, int): # frecuencia de t√≥ken vocabulario_m√°x = [t√≥ken for t√≥ken, frecuencia in contador_absoluto.most_common() if self.frecuencia_m√°xima &gt;= frecuencia] else: # frecuencia de documento vocabulario_m√°x = [t√≥ken for t√≥ken, frecuencia in contador_documentos.most_common() if self.frecuencia_m√°xima &gt;= frecuencia/len(lote)] # intersecci√≥n de vocabulario_m√≠n y vocabulario_m√°x preservando el √≥rden vocabulario = [t√≥ken for t√≥ken in vocabulario_m√≠n if t√≥ken in vocabulario_m√°x] # longitud vocabulario = [t√≥ken for t√≥ken in vocabulario if self.longitud_m√°xima &gt;= len(t√≥ken) &gt;= self.longitud_m√≠nima] # stop words vocabulario = [t√≥ken for t√≥ken in vocabulario if t√≥ken not in self.stop_words] # l√≠mite vocabulario = vocabulario[:self.l√≠mite_vocabulario] return vocabulario def fit(self, lote): vocabulario = self.reducir_vocabulario(lote) if self.t√≥ken_desconocido: vocabulario.append(self.t√≥ken_desconocido) if self.t√≥ken_relleno: vocabulario.insert(0, self.t√≥ken_relleno) self.mapeo = {t√≥ken: √≠ndice for √≠ndice, t√≥ken in enumerate(vocabulario)} return self # ning√∫n cambio aqu√≠ def transform(self, lote): if self.t√≥ken_desconocido: # reemplazar return [[t√≥ken if t√≥ken in self.mapeo else self.t√≥ken_desconocido for t√≥ken in doc] for doc in lote] else: # ignorar return [[t√≥ken for t√≥ken in doc if t√≥ken in self.mapeo] for doc in lote] # ning√∫n cambio aqu√≠ def t√≥kenes_a_√≠ndices(self, lote): lote = self.transform(lote) return [[self.mapeo[t√≥ken] for t√≥ken in doc] for doc in lote] # ning√∫n cambio aqu√≠ def √≠ndices_a_t√≥kenes(self, lote): mapeo_inverso = list(self.mapeo.keys()) return [[mapeo_inverso[√≠ndice] for √≠ndice in doc] for doc in lote] def __len__(self): return len(self.mapeo) . El √≠ndice del t√≥ken de relleno suele ser 0 y para continuar con esta tradici√≥n en vez de hacerle append al vocabulario le hicimos un prepend para que el t√≥ken encabece el listado. Adem√°s usamos el decorador @property para tener un atributo √≠ndice_relleno (en vez de un m√©todo) que nos devuelva el √≠ndice del t√≥ken. . v = Vocab().fit(train_docs) v.√≠ndice_relleno . 0 . La funci&#243;n que rellena . def rellenar_documentos(lote, largos, √≠ndice_relleno): m√°ximo_largo = max(largos) return [doc + [√≠ndice_relleno] * (m√°ximo_largo - largos[i]) for i, doc in enumerate(lote)] . Le tenemos que pasar el lote, el largo o tama√±o de cada documento del lote y el √≠ndice de relleno. . √≠ndices = [ [2,2], [4,4,4,4], [7,7,7,7,7,7,7], ] largos = [2,4,7] rellenos = rellenar_documentos(√≠ndices, largos, v.√≠ndice_relleno) rellenos . [[2, 2, 0, 0, 0, 0, 0], [4, 4, 4, 4, 0, 0, 0], [7, 7, 7, 7, 7, 7, 7]] . torch.tensor(rellenos) . tensor([[2, 2, 0, 0, 0, 0, 0], [4, 4, 4, 4, 0, 0, 0], [7, 7, 7, 7, 7, 7, 7]]) . ¬°Ahora s√≠ funcion√≥! . Tama&#241;o del documento . Vamos a incluir el tama√±o del documento (en cantidad de t√≥kenes/√≠ndices) junto a cada √≠tem del dataset ya que nos va a hacer falta para la funci√≥n que rellena. . from torch.utils.data import Dataset class Textset(Dataset): def __init__(self, documentos, etiquetas=None): self.documentos = documentos self.etiquetas = etiquetas or np.full(len(documentos), np.nan) def __len__(self): return len(self.documentos) def __getitem__(self, item): return self.documentos[item], len(self.documentos[item]), self.etiquetas[item] . train_ds = Textset(train_√≠ndices, etiquetas_train_√≠ndices) train_ds[10_000] . ([8, 169, 1, 4652, 0, 17, 65], 7, 40) . Bonus: AtributoDiccionario . ¬øAlguna vez quisiste acceder a los elementos de un diccionario como si fuesen atributos de un objeto? Es decir as√≠ . d = {&#39;uno&#39;:1, &#39;dos&#39;:2, &#39;tres&#39;:3} d.uno # =&gt; 1 . en vez de as√≠ . d[&#39;uno&#39;] # =&gt; 1 . Con esta magia ahora es posible: . class AtriDicc(): def __init__(self, *args, **kwargs): self.__dict__ = dict(*args, **kwargs) def __repr__(self): return repr(self.__dict__) . AtriDicc(uno=1, dos=2, tres=3).uno . 1 . Vamos a pimpiar la clase Textset con esto para que en vez de devolver elementos del dataset como tuplas (documento, largo, etiqueta) en el que debemos acordarnos que el orden de los elementos, devolvemos un AtriDicc en el que accedemos las cosas por su nombre y es m√°s c√≥modo que un diccionario. . from torch.utils.data import Dataset class Textset(Dataset): def __init__(self, documentos, etiquetas=None): self.documentos = documentos self.etiquetas = etiquetas or np.full(len(documentos), np.nan) def __len__(self): return len(self.documentos) def __getitem__(self, item): return AtriDicc( documento = self.documentos[item], largo = len(self.documentos[item]), etiqueta = self.etiquetas[item], ) . train_ds = Textset(train_√≠ndices, etiquetas_train_√≠ndices) train_ds[10_000].documento . [8, 169, 1, 4652, 0, 17, 65] . Funci&#243;n collate . Collate significa juntar diferentes piezas de informaci√≥n para ver sus similaridades y diferencias, tambi√©n puede ser colectar y organizar las hojas de un reporte, un libro. En el contexto de DataLoader quiere decir arreglar el lote. Entonces esta funci√≥n recibe una lista de elementos del Dataset, en nuestro caso una lista de de AtriDiccs, y debe devolver el lote en una forma √∫til y en lo posible realizar conversiones a tensores. . DataLoader posee una collate function por defecto que utiliza internamente y que en muchos casos funciona correctamente, pero otros como ahora que tenemos documentos de distinto largo nos toca definir una funci√≥n propia. . def rellenar_lote(lote): &quot;&quot;&quot;Prepara lotes para ingresar a nn.Embedding&quot;&quot;&quot; documentos = [elemento.documento for elemento in lote] largos = [elemento.largo for elemento in lote] etiquetas = [elemento.etiqueta for elemento in lote] rellenos = rellenar_documentos(documentos, largos, v.√≠ndice_relleno) return AtriDicc( documentos = torch.tensor(rellenos), etiquetas = torch.tensor(etiquetas), ) . Cuando instanciamos un DataLoader le pasamos la funci√≥n que acabamos de definir. . train_dl = DataLoader(train_ds, collate_fn=rellenar_lote, batch_size=3, shuffle=True) . un_lote = next(iter(train_dl)) un_lote.documentos . tensor([[781, 31, 17, 104, 111, 9, 383, 93, 18, 11, 489, 0, 0], [ 20, 4, 11, 7, 272, 78, 29, 96, 5, 396, 16, 86, 16], [ 26, 69, 17, 313, 4, 258, 22, 4, 102, 0, 0, 0, 0]]) . un_lote.etiquetas . tensor([ 80, 316, 16]) . Funciona de maravillas. . Una funci&#243;n alternativa . La funci√≥n anterior es compatible con el m√≥dulo de PyTorch nn.Embedding que suele se la puerta de entrada en los modelos de procesamiento de texto. Todav√≠a no hemos hablado nada de los embeddings. Quiz√°s sea un momento para mencionar a nn.EmbeddingBag, que tiene requerimientos completamente diferentes al primer m√≥dulo. . def offsetear_lote(lote): &quot;&quot;&quot;Prepara lotes para ingresar a nn.EmbeddingBag&quot;&quot;&quot; documentos = [torch.tensor(elemento.documento) for elemento in lote] offsets = [0] + [elemento.largo for elemento in lote][:-1] etiquetas = [elemento.etiqueta for elemento in lote] return AtriDicc( documentos = torch.cat(documentos), offsets = torch.tensor(offsets).cumsum(dim=0), etiquetas = torch.tensor(etiquetas), ) . Esta funci√≥n yuxtapone los documentos por un lado, y por otro (offsets) indica cu√°ndo comienza cada documento en ese continuo. . train_dl = DataLoader(train_ds, collate_fn=offsetear_lote, batch_size=3, shuffle=True) . un_lote = next(iter(train_dl)) un_lote.documentos . tensor([ 35, 14, 8, 544, 46, 6, 2493, 30, 384, 2, 1062, 27, 236, 5, 778, 378, 22, 4, 53, 1, 866, 9, 17, 1564, 109, 68, 186, 16, 6, 1419]) . un_lote.offsets . tensor([ 0, 9, 16]) . Avanzado: Memory pinning . https://pytorch.org/docs/stable/data.html#memory-pinning . Esta t√©cnica consiste en pre-disponibilizar los tensores en el GPU. Llevar un lote del disco o de la memoria a la GPU insume tiempo y puede causar un cuello de botella durante el entrenamiento. . Pasar la opci√≥n pin_memory=True al DataLoader pondr√° autom√°ticamente a los tensores en la pinned memory. Por defecto funciona con tensores y colecciones de tensores. Cuando los lotes son de un tipo personalizado (por ejemplo AtriDicc), normal cuando se utiliza una collate function propia, es necesario que el tipo defina el m√©todo pin_memory. . class AttrDict(): def __init__(self, *args, **kwargs): self.__dict__ = dict(*args, **kwargs) def __repr__(self): return repr(self.__dict__) def pin_memory(self): for atributo, valor in self.__dict__.items(): self.__dict__[atributo] = valor.pin_memory() if hasattr(valor, &#39;pin_memory&#39;) else valor return self . El pre-procesamiento hasta ahora . vocabulario_documentos = Vocab().fit(train_docs) train_√≠ndices = vocabulario_documentos.t√≥kenes_a_√≠ndices(train_docs) valid_√≠ndices = vocabulario_documentos.t√≥kenes_a_√≠ndices(valid_docs) infer_√≠ndices = vocabulario_documentos.t√≥kenes_a_√≠ndices(infer_docs) . train_ds = Textset(train_√≠ndices) valid_ds = Textset(valid_√≠ndices) infer_ds = Textset(infer_√≠ndices) . Solo definimos el modelo, no lo entrenamos. Elegimos la funci√≥n offsetear_lote ya que el modelo usa nn.EmbeddingBag. . from torch.utils.data import DataLoader train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=offsetear_lote, pin_memory=True) # validaci√≥n e inferencia no requieren `shuffle` valid_dl = DataLoader(valid_ds, batch_size=32, shuffle=False, collate_fn=offsetear_lote, pin_memory=True) infer_dl = DataLoader(infer_ds, batch_size=32, shuffle=False, collate_fn=offsetear_lote, pin_memory=True) . Hacemos unas definiciones necesarias. No es el punto de lo que queremos mostrar, lo pod√©s pasar por alto. Para mayores detalles recomendamos ver Text Classification with TorchText. . import torch.nn as nn import torch.nn.functional as F DIM_EMBEDDINGS = 8 class ClasificadorBolsa(nn.Module): def __init__(self, vocab_size, embed_dim, num_class): super().__init__() self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False, mode=&#39;max&#39;) self.fc = nn.Linear(embed_dim, num_class) def forward(self, text, offsets): embedded = self.embedding(text, offsets) return self.fc(embedded) modelo = ClasificadorBolsa( len(vocabulario_documentos), DIM_EMBEDDINGS, len(vocabulario_etiquetas) ).to(device) . √çdem. . if torch.cuda.is_available(): device = torch.device(&#39;cuda&#39;) else: device = torch.device(&#39;cpu&#39;) . Nuevamente. . optimizador = torch.optim.Adam(modelo.parameters(), lr=1e-3, weight_decay=1e-5) criterio = nn.CrossEntropyLoss() . Alto aqu√≠. As√≠ es como se usa un DataLoader. . √âPOCAS = 10 for √©poca in range(√âPOCAS): for lote in train_dl: optimizador.zero_grad() predicciones = modelo(lote.documentos.to(device), lote.offsets.to(device)) p√©rdida = criterio(predicciones, lote.etiquetas.to(device)) p√©rdida.backward() optimizador.step() . Con esto concluye la segunda parte. Quedaron los embeddings para la tercera. Ahora deber√≠amos tener m√°s control sobre la carga de datos en PyTorch. Muchos ejemplos de uso y tutoriales dan por sentada esta parte al utilizar datasets de ejemplos, que ya vienen pre-procesados y/o que la carga por defecto de PyTorch maneja sin inconvenientes. .",
            "url": "https://matiasbattocchia.github.io/datitos/Preprocesamiento-de-texto-para-NLP-parte-2.html",
            "relUrl": "/Preprocesamiento-de-texto-para-NLP-parte-2.html",
            "date": " ‚Ä¢ Jul 30, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Preprocesamiento de texto para NLP (parte 1)",
            "content": "Vamos a hacer un recorrido por los pasos b√°sicos del pre-procesamiento de texto. Estos pasos son necesarios para transformar texto del lenguaje humano a un formato legible para m√°quinas para su posterior procesamiento, particularmente motiva esta publicaci√≥n el procesamiento en PyTorch. . Veremos c√≥mo realizar estos pasos con c√≥digo propio, para mayor entendimiento de lo que est√° sucediendo, y con spaCy, una herramienta de nuestro agrado. . En concreto, los pasos son: . Limpieza, la remoci√≥n del contenido no deseado. | Normalizaci√≥n, la conversi√≥n diferentes formas a una sola. | Tokenizaci√≥n, la separaci√≥n del texto en t√≥kenes (unidades m√≠nimas, por ejemplo palabras). | Separaci√≥n en conjuntos de datos: entrenamiento, validaci√≥n, prueba. | Generaci√≥n del vocabulario, la lista de t√≥kenes conocidos. | Numericalizaci√≥n, el mapeo de t√≥kenes a n√∫meros enteros. | Estos pasos son comunes distintas aproximaciones al procesamiento del lenguaje. En la parte 2 mostraremos pasos √∫tiles para abarcarlo usando deep learning. . Loteo, la generaci√≥n de porciones de muestras de entrenamiento. | Relleno, la conversi√≥n del lote en un tensor de PyTorch. | Carga de embeddings, opcionalmente el uso de embeddings precalculados. | Nota: El √≥rden de los primeros tres pasos (limpieza, normalizaci√≥n, tokenizaci√≥n) puede variar seg√∫n conveniencia. El resto de los pasos mantiene el √≥rden. . Dataset de ejemplo . ¬øQu√© ser√≠a de esta publicaci√≥n sin algunos ejemplos? Vamos a usar el dataset de la competencia clasificaci√≥n de preguntas de clientes de Meta:Data. . import pandas as pd df = pd.read_csv(&#39;train.csv&#39;, sep=&#39;|&#39;) . with pd.option_context(&#39;display.max_colwidth&#39;, -1): display(df.sample(10)) . Pregunta Intencion . 9221 hice una compra y no tengo las acciones acreditadas | Cat_153 | . 1298 quiero saber si yo puedo solicitar un pr√©stamo | Cat_248 | . 4488 perdi la credencial universitaria. como solicito una nueva? | Cat_294 | . 28 cambiar moneda tarjeta debitar exterior | Cat_289 | . 19970 quiero adherir al debito de la tarjeta el servicio epec. me pide que ingrese numero cuenta de digitos pero en la factura no aparece un numero | Cat_129 | . 3510 llegar tarjeta recargable solicit√≠¬¨‚â† | Cat_293 | . 14373 buenas tardes | Cat_19 | . 5485 saber de cuanto es el pago m√≠nimo de tarjeta visa | Cat_351 | . 6752 verificar reclamo | Cat_135 | . 10645 que sucede cuando se me vence el plazo fijo? | Cat_180 | . Expresiones regulares . Si las expresiones regulares no te resultan familiares entonces vale la pena estudiarlas brevemente, ya que las usaremos. Pod√©s mirar este tutorial que encontramos en la web. . import re . Limpieza . Muchas t√©cnicas modernas no realizan limpieza alguna. Dependiendo de lo que queramos hacer tal vez convenga deshacernos de algunos elementos. En el dataset de ejemplo los signos de puntuaci√≥n no parecen tener gran relevancia, quiz√°s tampoco la tengan los n√∫meros (que aparentemente han sido removidos de antemano). . def limpiar(texto): puntuaci√≥n = r&#39;[,;.:¬°!¬ø?@#$%&amp;[ ](){}&lt;&gt;~=+ -*/| _^`&quot; &#39;]&#39; # signos de puntuaci√≥n texto = re.sub(puntuaci√≥n, &#39; &#39;, texto) # d√≠gitos [0-9] texto = re.sub(&#39; d&#39;, &#39; &#39;, texto) return texto . En esta funci√≥n substituimos los signos de puntuaci√≥n . , ; . : ¬° ! ¬ø ? @ # $ % &amp; [ ] ( ) { } &lt; &gt; ~ = + - * / | _ ^ ` &quot; &#39; . por espacios (me gusta m√°s; usar string vac√≠o &#39;&#39; para eliminarlos) medieante expresiones regulares (algunos caracteres tuvieron que ser escapados anteponiendo por tener un significado especial para la expresi√≥n regular). Hacemos lo mismo con los d√≠gitos. Veamos un ejemplo de funcionamiento. . limpiar(&#39;hoy 13 trabajan?&#39;) . &#39;hoy trabajan &#39; . Otros elementos que podr√≠amos pensar en remover son caracteres invisibles, espacios redundantes. Veremos que esto en particular tambi√©n puede ser resulto en la tokenizaci√≥n. . Normalizaci&#243;n . Normalizar es la tarea de llevar lo que puede ser expresado de m√∫ltiples maneras como fechas, n√∫meros y abreviaturas a una √∫nica forma. Por ejemplo . 13/03/30 -&gt; trece de marzo de dos mil treinta DC -&gt; departamento de computaci√≥n . Se trata de una pr√°ctica cl√°sica de la √©poca de los modelos de lenguaje probabil√≠sticos, que intentaban reducir lo m√°s posible la cantidad de palabras. En cierta forma 1 palabra = 1 atributo (lo que en los &#39;90s conocimos como convertibilidad). Elegir atributos es ingenier√≠a de atributos, la parte central del machine learning, y lo justamente lo que el deep learning busca automatizar. . Sin embargo hay una normalizaci√≥n muy com√∫n hoy, el convertir todo el texto a min√∫sculas. En el caso del espa√±ol, una normalizaci√≥n com√∫n es la remoci√≥n de tildes. . def normalizar(texto): # todo a min√∫sculas texto = texto.lower() # tildes y diacr√≠ticas texto = re.sub(&#39;√°&#39;, &#39;a&#39;, texto) texto = re.sub(&#39;√©&#39;, &#39;e&#39;, texto) texto = re.sub(&#39;√≠&#39;, &#39;i&#39;, texto) texto = re.sub(&#39;√≥&#39;, &#39;o&#39;, texto) texto = re.sub(&#39;√∫&#39;, &#39;u&#39;, texto) texto = re.sub(&#39;√º&#39;, &#39;u&#39;, texto) texto = re.sub(&#39;√±&#39;, &#39;n&#39;, texto) return texto . normalizar(&#39;Me podr√°n dar informaci√≥n de un pr√©stamo personal&#39;) . &#39;me podran dar informacion de un prestamo personal&#39; . Hay una librer√≠a llamada unidecode que realiza transliteraci√≥n: representa letras o palabras de un alfabeto en otro, √∫til si tenemos caracteres en ruso (cir√≠lico) o chino (caracteres Han), a√∫n √∫til para el alfabeto latino cuando queremos pasar de Unicode a ASCII (lo que substituir√≠a las tildes). . pip install unidecode . from unidecode import unidecode unidecode(&#39;Me podr√°n dar informaci√≥n de un pr√©stamo personal&#39;) . &#39;Me podran dar informacion de un prestamo personal&#39; . Una normalizaci√≥n que vale la pena intentar con este dataset es la correci√≥n ortogr√°fica con un paquete como pyspellchecker. Quiz√°s con art√≠culos de diarios en los que la redacci√≥n est√° m√°s cuidada esto no valga la pena, pero en contextos m√°s informales como este, conversaciones por char, Twitter, las palabras mal escritas en realidad refieren a una sola palabra y no a distintos significados. . Tokenizaci&#243;n . Tokenizar es separar el texto en partes m√°s peque√±as llamadas t√≥kenes. Una unidad muy com√∫n es la palabras pero depende de lo que queramos hacer, si es que no hemos eliminado a los signos de puntuaci√≥n estos tambi√©n ser√≠an t√≥kenes. Las palabras frecuentemente est√°n compuestas por una ra√≠z, prefijo y/o sufijo, por lo que podr√≠amos decidir separarlos tambi√©n. En ingl√©s es com√∫n separar it&#39;s en it y &#39;s, si bien en espa√±ol esta situaci√≥n no es com√∫n. . A diferencia de la limpieza y la normalizaci√≥n, la tokenizaci√≥n es un paso indispesable en la preparaci√≥n de texto para su procesamiento. . Para el dataset en cuesti√≥n la tokenizaci√≥n es simple, vamos a separar se«µun espacios y dem√°s caracteres invisibles como t (tabulaci√≥n) y n (salto de l√≠nea). De haber signos de puntuaci√≥n, pro ejemplo si quisi√©ramos procesar un documento extenso en oraciones, el proceso es m√°s complejo ya que final. tiene un punto en vez de un espacio, y no siempre los puntos demarcan el final de un t√≥ken como en A.M. y P.M.. . Debemos definir si elementos como los signos de puntuaci√≥n son t√≥kenes o si simplemente delimitan palabras o t√≥kenes, en cuyo caso desaparecer√≠an en el proceso. Mismo con los caracteres invisibles, si estuvi√©semos haciendo un modelo que programe en Python, la indentaci√≥n es fundamental y deberiera mantenerse. . def tokenizar(texto): # IMPORTANTE: podr√≠a devolver una lista vac√≠a return [t√≥ken for t√≥ken in texto.split()] . split tambi√©n se encarga de los caracteres invisibles repetidos. . tokenizar(&#39;hola vengo a flotar&#39;) . [&#39;hola&#39;, &#39;vengo&#39;, &#39;a&#39;, &#39;flotar&#39;] . Ac√° estamos cambiando el tipo de datos, ya que de un string hemos pasado a una lista de strings. . Si la expresi√≥n dentro de la funci√≥n no te resulta familiar, es una construcci√≥n llamada list comprehension y es una manera muy efectiva de armar una lista. Es lo mismo que hacer . lista = [] for i in range(10): lista.append(i) lista . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . pero de una manera m√°s expresiva y tambi√©n m√°s eficiente (est√° optimizado por el lenguaje) . [i for i in range(10)] . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . Varios modelos de lenguaje utilizan caracteres en vez de palabras como t√≥kenes, esto es √∫til por varios motivos que listaremos m√°s adelante. Otros utilizan partes de palabras como s√≠labas (las partes se determinan estad√≠sticamente). Ver https://arxiv.org/pdf/1508.07909.pdf. . Tokenizaci&#243;n utilizando alguna librer&#237;a . pip install spacy python -m spacy download es_core_news_sm . import spacy nlp = spacy.load(&#39;es_core_news_sm&#39;) doc = nlp(&#39;Esto es una frase.&#39;) print([t√≥ken.text for t√≥ken in doc]) . [&#39;Esto&#39;, &#39;es&#39;, &#39;una&#39;, &#39;frase&#39;, &#39;.&#39;] . Otros pre-procesos . Cl√°sicamente se aplicaban alguno de estos para reducir a√∫n m√°s la cantidad de palabras: . Stemming . Stem, de ra√≠z, reduce la inflecci√≥n de las palabras, mapeando un grupo de palabras a la misma ra√≠z, sin importar si la ra√≠z es una palabras v√°lida en el lenguaje. . caminando, caminar, camino -&gt; camin . Lemmatization . A diferencia del stemming, la lematizaci√≥n reduce las palabras inflexadas a palabras que pertenecen al lenguaje. La ra√≠z pasa a llamarse lema. . Primera parte del pre-procesamiento . def preprocesar(texto): texto = limpiar(texto) texto = normalizar(texto) texto = tokenizar(texto) return texto . Conjuntos de datos . En la competencias normalmente encontramos dos archivos, el de entrenamiento y el de inferencia ‚Äîque le suelen llamar de prueba y es el que tenemos que predecir para entregar‚Äî. Del que suelen llamar train tambi√©n tenemos que obtener el de validaci√≥n. . infer_df = pd.read_csv(&#39;test.csv&#39;, sep=&#39;,&#39;) . from sklearn.model_selection import train_test_split train_df, valid_df = train_test_split(df, test_size=.1, random_state=42) . . Ahora estamos en condiciones de pre-procesar todo lo que tenemos: . train_docs = [preprocesar(doc) for doc in train_df[&#39;Pregunta&#39;].values] valid_docs = [preprocesar(doc) for doc in valid_df[&#39;Pregunta&#39;].values] infer_docs = [preprocesar(doc) for doc in infer_df[&#39;Pregunta&#39;].values] . Hemos pasado de una Series de Pandas, array de NumPy o una lista de strings . train_df[&#39;Pregunta&#39;].values[:4] . array([&#39;que se requiere para un pr√©stamo personal?&#39;, &#39;me piden mi n√∫mero de cuenta es mi cbu?&#39;, &#39;necesitar adherir aysa tarjeta&#39;, &#39;te financian igual un usado o un 0km?&#39;], dtype=object) . a una lista de listas de strings . train_docs[:4] . [[&#39;que&#39;, &#39;se&#39;, &#39;requiere&#39;, &#39;para&#39;, &#39;un&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;me&#39;, &#39;piden&#39;, &#39;mi&#39;, &#39;numero&#39;, &#39;de&#39;, &#39;cuenta&#39;, &#39;es&#39;, &#39;mi&#39;, &#39;cbu&#39;], [&#39;necesitar&#39;, &#39;adherir&#39;, &#39;aysa&#39;, &#39;tarjeta&#39;], [&#39;te&#39;, &#39;financian&#39;, &#39;igual&#39;, &#39;un&#39;, &#39;usado&#39;, &#39;o&#39;, &#39;un&#39;, &#39;km&#39;]] . Un poco de nomenclatura: estamos llamando corpus a la colecci√≥n de textos. Nos referimos tambi√©n a los textos como documentos. Tambi√©n estamos usando el t√©rmino lote (batch) para referirnos a un (sub)conjunto de documentos. . Vocabulario . Este paso es importante. Aqu√≠ definimos y limitamos la t√≥kenes que vamos a utilizar. El lenguaje es infinito, para convertirlo en un problema tratable muchas veces los que hacemos es reducirlo. Clave para varias pr√°cticas de reducci√≥n es contar las frecuencias de los t√≥kenes, esto es, cu√°ntas veces aparece cada t√≥ken en todo el corpus. Como mencionamos las palabras m√°s frecuentes no aportan mucha informaci√≥n y las m√°s infrecuentes si bien son las que m√°s informaci√≥n tienen no llegar√°n a ser representativas para nuestro modelo. Descartar palabras poco frecuentes tambi√©n afecta a errores ortogr√°ficos. . √ötil para este paso es la clase Counter de la librer√≠a est√°ndar de Python. . from collections import Counter c = Counter([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;a&#39;,&#39;b&#39;,&#39;a&#39;]) # obtener los dos elementos m√°s comunes y sus frecuencias c.most_common(2) . [(&#39;a&#39;, 3), (&#39;b&#39;, 2)] . Una funci√≥n de la librer√≠a est√°ndar llamada chain nos dar√° una mano convirtiendo la lista de listas de t√≥kenes en una lista de t√≥kenes, similar a numpy.flatten, ya que Counter espera una lista con elementos a contar y nuestros t√≥kenes est√°n separados por documentos, hay que juntarlos. . from itertools import chain list(chain([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;], [&#39;c&#39;,&#39;d&#39;])) . [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;, &#39;d&#39;] . chain encadena las listas que le pasamos como argumentos variables. Podemos usar el operador splat * para contentar a la funci√≥n (convertir la lista principal en una serie de argumentos). . list(chain( *[ [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;], [&#39;c&#39;,&#39;d&#39;] ] )) . [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;, &#39;d&#39;] . En vez de una lista podemos pedir un conjunto (set), en el que los elementos no se repiten. Este bien podr√≠a ser el vocabulario. . set(chain( *[ [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;], [&#39;c&#39;,&#39;d&#39;] ] )) . {&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;} . En definitiva, es la lista oficial de t√≥kenes. . class Vocab(): def fit(self, lote): self.vocabulario = set(chain(*lote)) return self def __len__(self): return len(self.vocabulario) . Es importante generar el vocabulario con el dataset de entrenamiento, ya que como mencionamos se trata de la lista de palabras conocidas. Le agregamos un __len__ porque tambi√©n es √∫til conocer el tama√±o del vocabulario. . v = Vocab().fit(train_docs) len(v) . NameError Traceback (most recent call last) &lt;ipython-input-3-04b70b8e4c3d&gt; in &lt;module&gt; -&gt; 1 v = Vocab().fit(train_docs) 2 len(v) NameError: name &#39;train_docs&#39; is not defined . ¬øQu√© pasa con las palabras que no est√°n en la lista? Se las conoce como t√≥kenen fuera del vocabulario (out-of-vocabulary, abreviado OOV). Estas requieren acciones especiales, podr√≠amos . ignorarlas | reemplazarlas por un t√≥ken especial | inferirlas (ver m√°s adelante, embeddings) | . class Vocab(): def __init__(self, t√≥ken_desconocido=&#39;&lt;unk&gt;&#39;): self.t√≥ken_desconocido = t√≥ken_desconocido def fit(self, lote): self.vocabulario = list(set(chain(*lote))) if self.t√≥ken_desconocido: self.vocabulario.append(self.t√≥ken_desconocido) return self def transform(self, lote): if self.t√≥ken_desconocido: # reemplazar return [[t√≥ken if t√≥ken in self.vocabulario else self.t√≥ken_desconocido for t√≥ken in doc] for doc in lote] else: # ignorar return [[t√≥ken for t√≥ken in doc if t√≥ken in self.vocabulario] for doc in lote] def __len__(self): return len(self.vocabulario) . Vocab().fit(train_docs).transform([ [&#39;poder&#39;, &#39;gestionar&#39;, &#39;clave&#39;, &#39;paso&#39;, &#39;pagina&#39;], [&#39;desde&#39;, &#39;cuando&#39;, &#39;arranco&#39;, &#39;con&#39;, &#39;el&#39;, &#39;programa&#39;, &#39;de&#39;, &#39;millas&#39;], ]) . [[&#39;poder&#39;, &#39;gestionar&#39;, &#39;clave&#39;, &#39;paso&#39;, &#39;pagina&#39;], [&#39;desde&#39;, &#39;cuando&#39;, &#39;&lt;unk&gt;&#39;, &#39;con&#39;, &#39;el&#39;, &#39;programa&#39;, &#39;de&#39;, &#39;millas&#39;]] . Numericalizaci&#243;n . Tambi√©n conocido como indexaci√≥n. As√≠ como a las unidades m√≠nimas que consideramos las llamamos t√≥kenes, a los n√∫meros que los representan los llamamos √≠ndices. Ya que el vocabulario tiene la lista de t√≥kenes, le vamos a pedir una responsabilidad adicional: que mantenga una asignaci√≥n entre t√≥kenes y n√∫meros enteros. Posiblemente ya te ha sucedido pasarle valores no n√∫mericos a un estimador y ver c√≥mo falla. . vocabulario = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;&lt;unk&gt;&#39;] {t√≥ken: √≠ndice for √≠ndice, t√≥ken in enumerate(vocabulario)} . {&#39;a&#39;: 0, &#39;b&#39;: 1, &#39;c&#39;: 2, &#39;d&#39;: 3, &#39;&lt;unk&gt;&#39;: 4} . que es lo mismo que . mapeo = {} for √≠ndice, t√≥ken in enumerate(vocabulario): mapeo[t√≥ken] = √≠ndice mapeo . {&#39;a&#39;: 0, &#39;b&#39;: 1, &#39;c&#39;: 2, &#39;d&#39;: 3, &#39;&lt;unk&gt;&#39;: 4} . ¬øQu√© es lo que hace enumerate? Como su nombre lo indica, enumera los elementos de una colecci√≥n. . list(enumerate(vocabulario)) . [(0, &#39;a&#39;), (1, &#39;b&#39;), (2, &#39;c&#39;), (3, &#39;d&#39;), (4, &#39;&lt;unk&gt;&#39;)] . class Vocab(): def __init__(self, t√≥ken_desconocido=&#39;&lt;unk&gt;&#39;): self.t√≥ken_desconocido = t√≥ken_desconocido def fit(self, lote): vocabulario = list(set(chain(*lote))) if self.t√≥ken_desconocido: vocabulario.append(self.t√≥ken_desconocido) self.mapeo = {t√≥ken: √≠ndice for √≠ndice, t√≥ken in enumerate(vocabulario)} return self def transform(self, lote): if self.t√≥ken_desconocido: # reemplazar return [[t√≥ken if t√≥ken in self.mapeo else self.t√≥ken_desconocido for t√≥ken in doc] for doc in lote] else: # ignorar return [[t√≥ken for t√≥ken in doc if t√≥ken in self.mapeo] for doc in lote] def __len__(self): return len(self.mapeo) . Comprobemos que la nueva versi√≥n de Vocab funciona como la anterior. Adem√°s veamos qu√© sucedo cuando no queremos el t√≥ken para palabras fuera de vocabulario. . Vocab(t√≥ken_desconocido=None).fit(train_docs).transform([ [&#39;poder&#39;, &#39;gestionar&#39;, &#39;clave&#39;, &#39;paso&#39;, &#39;pagina&#39;], [&#39;desde&#39;, &#39;cuando&#39;, &#39;arranco&#39;, &#39;con&#39;, &#39;el&#39;, &#39;programa&#39;, &#39;de&#39;, &#39;millas&#39;], ]) . [[&#39;poder&#39;, &#39;gestionar&#39;, &#39;clave&#39;, &#39;paso&#39;, &#39;pagina&#39;], [&#39;desde&#39;, &#39;cuando&#39;, &#39;con&#39;, &#39;el&#39;, &#39;programa&#39;, &#39;de&#39;, &#39;millas&#39;]] . Ahora vamos a agregar m√©todos para convertir t√≥kenes a √≠ndices y viceversa. . class Vocab(): def __init__(self, t√≥ken_desconocido=&#39;&lt;unk&gt;&#39;): self.t√≥ken_desconocido = t√≥ken_desconocido def fit(self, lote): # agregamos `sorted` porque el orden al aplicar `set` no est√° asegurado vocabulario = list(sorted(set(chain(*lote)))) if self.t√≥ken_desconocido: vocabulario.append(self.t√≥ken_desconocido) self.mapeo = {t√≥ken: √≠ndice for √≠ndice, t√≥ken in enumerate(vocabulario)} return self def transform(self, lote): if self.t√≥ken_desconocido: # reemplazar return [[t√≥ken if t√≥ken in self.mapeo else self.t√≥ken_desconocido for t√≥ken in doc] for doc in lote] else: # ignorar return [[t√≥ken for t√≥ken in doc if t√≥ken in self.mapeo] for doc in lote] def t√≥kenes_a_√≠ndices(self, lote): lote = self.transform(lote) return [[self.mapeo[t√≥ken] for t√≥ken in doc] for doc in lote] def √≠ndices_a_t√≥kenes(self, lote): mapeo_inverso = list(self.mapeo.keys()) return [[mapeo_inverso[√≠ndice] for √≠ndice in doc] for doc in lote] def __len__(self): return len(self.mapeo) . v = Vocab(t√≥ken_desconocido=None).fit(train_docs) v.t√≥kenes_a_√≠ndices([ [&#39;que&#39;, &#39;se&#39;, &#39;requiere&#39;, &#39;para&#39;, &#39;un&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;me&#39;, &#39;piden&#39;, &#39;mi&#39;, &#39;numero&#39;, &#39;de&#39;, &#39;cuenta&#39;, &#39;es&#39;, &#39;mi&#39;, &#39;cbu&#39;], ]) . [[4160, 4683, 4484, 3703, 5294, 4011, 3825], [3275, 3854, 3319, 3554, 1532, 1462, 2151, 3319, 950]] . v.√≠ndices_a_t√≥kenes([ [4160, 4683, 4484, 3703, 5294, 4011, 3825], [3275, 3854, 3319, 3554, 1532, 1462, 2151, 3319, 950], ]) . [[&#39;que&#39;, &#39;se&#39;, &#39;requiere&#39;, &#39;para&#39;, &#39;un&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;me&#39;, &#39;piden&#39;, &#39;mi&#39;, &#39;numero&#39;, &#39;de&#39;, &#39;cuenta&#39;, &#39;es&#39;, &#39;mi&#39;, &#39;cbu&#39;]] . Casos especiales . ¬øQu√© sucede con los documentos que al ser tokenizados regresan vac√≠os? ¬øO con documentos compuestos enteramente por palabras fuera del vocabulario? . documentos_problem√°ticos = [ &#39;??? ???&#39;, &#39;Banks charge high fees for foreign ATM&#39; ] [preprocesar(doc) for doc in documentos_problem√°ticos] . [[], [&#39;banks&#39;, &#39;charge&#39;, &#39;high&#39;, &#39;fees&#39;, &#39;for&#39;, &#39;foreign&#39;, &#39;atm&#39;]] . v = Vocab().fit(train_docs) v.transform([[], [&#39;banks&#39;, &#39;charge&#39;, &#39;high&#39;, &#39;fees&#39;, &#39;for&#39;, &#39;foreign&#39;]]) . [[], [&#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;]] . v.t√≥kenes_a_√≠ndices([[], [&#39;banks&#39;, &#39;charge&#39;, &#39;high&#39;, &#39;fees&#39;, &#39;for&#39;, &#39;foreign&#39;]]) . [[], [5583, 5583, 5583, 5583, 5583, 5583]] . v.√≠ndices_a_t√≥kenes([[], [5583, 5583, 5583, 5583, 5583, 5583]]) . [[], [&#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;]] . La conclusi√≥n es que no pasa nada (al menos por ahora). . Bonus: reducci&#243;n del vocabulario . La idea es limitar los t√≥kenes que vamos a utilizar. En cierta forma cada t√≥ken es un atributo (feature) y quisi√©ramos proveer atributos que sean de utilidad para el estimador. . El lenguaje es infinito, para convertirlo en un problema tratable muchas veces los que hacemos es reducirlo. Clave para varias pr√°cticas de reducci√≥n es contar las frecuencias de los t√≥kenes, esto es, cu√°ntas veces aparece cada t√≥ken en todo el corpus. Como mencionamos las palabras m√°s frecuentes no aportan mucha informaci√≥n y las m√°s infrecuentes si bien son las que m√°s informaci√≥n tienen no llegar√°n a ser representativas para nuestro modelo. Descartar palabras poco frecuentes tambi√©n afecta a errores ortogr√°ficos. . √ötil para este paso es la clase Counter de la librer√≠a est√°ndar de Python. . from collections import Counter c = Counter([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;a&#39;,&#39;b&#39;,&#39;a&#39;]) # obtener los elementos ordenados de m√°s comunes a menos c.most_common() . [(&#39;a&#39;, 3), (&#39;b&#39;, 2), (&#39;c&#39;, 1)] . Acerca de contar palabras, no te pierdas la ley de Zipf-Models:-Bag-of-Words). . M&#225;s comunes . Una estrategia simple es ordenar a los t√≥kenes seg√∫n frecuencia y poner un l√≠mite duro al vocabulario, de modo de quedarnos con los l√≠mite m√°s comunes. . l√≠mite = 2 vocabulario = list(c)[:l√≠mite] vocabulario . [&#39;a&#39;, &#39;b&#39;] . Por frecuencia de t&#243;ken . Podr√≠amos descartar los que aparecen . m√°s de m√°ximo veces, | menos de m√≠nimo veces. | . m√°ximo = 3 m√≠nimo = 2 vocabulario = [t√≥ken for t√≥ken, frecuencia in c.most_common() if m√°ximo &gt;= frecuencia &gt;= m√≠nimo] vocabulario . [&#39;a&#39;, &#39;b&#39;] . Por frecuencia de documento . O bien, en vez de contar las apariciones absolutas, contar en cu√°ntos documentos aparece cada t√≥ken. Un t√≥ken que aparezca en todos los documentos no colaborar√≠a en una tarea de clasificaci√≥n, a distinguir documentos pero uno que aparezca en la mitad de los documentos podr√≠a ser √∫til para separarlos en dos grupos. . c = Counter() lote = [ [&#39;hola&#39;, &#39;buen&#39;, &#39;d√≠a&#39;], [&#39;hola&#39;, &#39;buenas&#39;, &#39;tardes&#39;], ] for doc in lote: c.update(set(doc)) c.most_common() . [(&#39;hola&#39;, 2), (&#39;buen&#39;, 1), (&#39;d√≠a&#39;, 1), (&#39;tardes&#39;, 1), (&#39;buenas&#39;, 1)] . Vamos a normalizar la frecuencias por la cantidad total de documentos ($D$) y de manera similar al punto anterior podr√≠amos descartar los elementos que aparecen en: . m√°s del m√°ximo proporci√≥n de los documentos. | menos del m√≠nimo proporci√≥n de los documentos. | . D = len(lote) m√°ximo = .9 m√≠nimo = .1 vocabulario = [t√≥ken for t√≥ken, frecuencia in c.most_common() if m√°ximo &gt;= frecuencia/D &gt;= m√≠nimo] vocabulario . [&#39;buen&#39;, &#39;d√≠a&#39;, &#39;tardes&#39;, &#39;buenas&#39;] . Stop words . Hay listas armadas de palabras muy comunes (stop words). Podemos elaborarla de alguna manera o usar alguna existente. . pip install nltk . import nltk nltk.download(&#39;stopwords&#39;) from nltk.corpus import stopwords stopwords.words(&#39;spanish&#39;)[:10] . [nltk_data] Downloading package stopwords to /home/matias/nltk_data... [nltk_data] Package stopwords is already up-to-date! . [&#39;de&#39;, &#39;la&#39;, &#39;que&#39;, &#39;el&#39;, &#39;en&#39;, &#39;y&#39;, &#39;a&#39;, &#39;los&#39;, &#39;del&#39;, &#39;se&#39;] . Un detalle a cuidar es que la tokenizaci√≥n usada para la lista de stop words tiene que haber sido la misma o similar que la usada para los documentos. . def filtrar_stop_words(lote): return [[t√≥ken for t√≥ken in doc if t√≥ken not in stopwords.words(&#39;spanish&#39;)] for doc in lote] filtrar_stop_words([ [&#39;que&#39;, &#39;se&#39;, &#39;requiere&#39;, &#39;para&#39;, &#39;un&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;me&#39;, &#39;piden&#39;, &#39;mi&#39;, &#39;numero&#39;, &#39;de&#39;, &#39;cuenta&#39;, &#39;es&#39;, &#39;mi&#39;, &#39;cbu&#39;], ]) . [[&#39;requiere&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;piden&#39;, &#39;numero&#39;, &#39;cuenta&#39;, &#39;cbu&#39;]] . Por longitud . Esta t√©cnica no requiere contar la frecuencia de los t√≥kenes, simplemente filtramos t√≥kenes muy cortos o muy largos ya que en general son ruidos. . def filtrar_por_longitud(lote, m√°xima, m√≠nima): return [[t√≥ken for t√≥ken in doc if m√°xima &gt;= len(t√≥ken) &gt;= m√≠nima] for doc in lote] filtrar_por_longitud([ [&#39;que&#39;, &#39;se&#39;, &#39;requiere&#39;, &#39;para&#39;, &#39;un&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;me&#39;, &#39;piden&#39;, &#39;mi&#39;, &#39;numero&#39;, &#39;de&#39;, &#39;cuenta&#39;, &#39;es&#39;, &#39;mi&#39;, &#39;cbu&#39;], ], m√°xima=9, m√≠nima=3) . [[&#39;que&#39;, &#39;requiere&#39;, &#39;para&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;piden&#39;, &#39;numero&#39;, &#39;cuenta&#39;, &#39;cbu&#39;]] . Implementaci&#243;n . Veamos c√≥mo acomodamos lo que hemos visto ahora en la clase Vocab. . import numpy as np from itertools import chain from collections import Counter class Vocab(): def __init__(self, t√≥ken_desconocido=&#39;&lt;unk&gt;&#39;, frecuencia_m√≠nima=0.0, frecuencia_m√°xima=1.0, longitud_m√≠nima=1, longitud_m√°xima=np.inf, stop_words=[], l√≠mite_vocabulario=None): self.t√≥ken_desconocido = t√≥ken_desconocido self.frecuencia_m√≠nima = frecuencia_m√≠nima self.frecuencia_m√°xima = frecuencia_m√°xima self.longitud_m√≠nima = longitud_m√≠nima self.longitud_m√°xima = longitud_m√°xima self.stop_words = stop_words self.l√≠mite_vocabulario = l√≠mite_vocabulario def reducir_vocabulario(self, lote): contador_absoluto = Counter(chain(*lote)) contador_documentos = Counter() for doc in lote: contador_documentos.update(set(doc)) # frecuencia m√≠nima if isinstance(self.frecuencia_m√≠nima, int): # frecuencia de t√≥ken vocabulario_m√≠n = [t√≥ken for t√≥ken, frecuencia in contador_absoluto.most_common() if frecuencia &gt;= self.frecuencia_m√≠nima] else: # frecuencia de documento vocabulario_m√≠n = [t√≥ken for t√≥ken, frecuencia in contador_documentos.most_common() if frecuencia/len(lote) &gt;= self.frecuencia_m√≠nima] # frecuencia m√°xima if isinstance(self.frecuencia_m√°xima, int): # frecuencia de t√≥ken vocabulario_m√°x = [t√≥ken for t√≥ken, frecuencia in contador_absoluto.most_common() if self.frecuencia_m√°xima &gt;= frecuencia] else: # frecuencia de documento vocabulario_m√°x = [t√≥ken for t√≥ken, frecuencia in contador_documentos.most_common() if self.frecuencia_m√°xima &gt;= frecuencia/len(lote)] # intersecci√≥n de vocabulario_m√≠n y vocabulario_m√°x preservando el √≥rden vocabulario = [t√≥ken for t√≥ken in vocabulario_m√≠n if t√≥ken in vocabulario_m√°x] # longitud vocabulario = [t√≥ken for t√≥ken in vocabulario if self.longitud_m√°xima &gt;= len(t√≥ken) &gt;= self.longitud_m√≠nima] # stop words vocabulario = [t√≥ken for t√≥ken in vocabulario if t√≥ken not in self.stop_words] # l√≠mite vocabulario = vocabulario[:self.l√≠mite_vocabulario] return vocabulario def fit(self, lote): vocabulario = self.reducir_vocabulario(lote) if self.t√≥ken_desconocido: vocabulario.append(self.t√≥ken_desconocido) self.mapeo = {t√≥ken: √≠ndice for √≠ndice, t√≥ken in enumerate(vocabulario)} return self def transform(self, lote): if self.t√≥ken_desconocido: # reemplazar return [[t√≥ken if t√≥ken in self.mapeo else self.t√≥ken_desconocido for t√≥ken in doc] for doc in lote] else: # ignorar return [[t√≥ken for t√≥ken in doc if t√≥ken in self.mapeo] for doc in lote] def t√≥kenes_a_√≠ndices(self, lote): lote = self.transform(lote) return [[self.mapeo[t√≥ken] for t√≥ken in doc] for doc in lote] def √≠ndices_a_t√≥kenes(self, lote): mapeo_inverso = list(self.mapeo.keys()) return [[mapeo_inverso[√≠ndice] for √≠ndice in doc] for doc in lote] def __len__(self): return len(self.mapeo) . Vocab(longitud_m√≠nima=3).fit(train_docs).transform([ [&#39;poder&#39;, &#39;gestionar&#39;, &#39;clave&#39;, &#39;paso&#39;, &#39;pagina&#39;], [&#39;desde&#39;, &#39;cuando&#39;, &#39;arranco&#39;, &#39;con&#39;, &#39;el&#39;, &#39;programa&#39;, &#39;de&#39;, &#39;millas&#39;], ]) . [[&#39;poder&#39;, &#39;gestionar&#39;, &#39;clave&#39;, &#39;paso&#39;, &#39;pagina&#39;], [&#39;desde&#39;, &#39;cuando&#39;, &#39;&lt;unk&gt;&#39;, &#39;con&#39;, &#39;&lt;unk&gt;&#39;, &#39;programa&#39;, &#39;&lt;unk&gt;&#39;, &#39;millas&#39;]] . El pre-procesamiento hasta ahora . v = Vocab().fit(train_docs) train_√≠ndices = v.t√≥kenes_a_√≠ndices(train_docs) valid_√≠ndices = v.t√≥kenes_a_√≠ndices(valid_docs) infer_√≠ndices = v.t√≥kenes_a_√≠ndices(infer_docs) . Con esto concluye la primera parte. Hay varias librer√≠as que tienen clases que se encargan de efectuar los pasos que hemos visto. Tienen un comportamiento por defecto, que es configurable (los par√°metros que hemos visto) y a su vez, personalizable, para reemplazar algunos o todos los pasos por c√≥digo propio. En general son librer√≠as desarrolladas por angloparlantes, funcionan out-of-the-box bien para el ingl√©s; cuando queremos procesar texto en espa√±ol vale la pena tener m√°s control sobre estos procesos. . CountVectorizer de scikit-learn. | TextDataBunch de fast.ai. | . Pre-procesando las etiquetas . Las etiquetas del dataset tambi√©n necesitan ser convertidas a n√∫meros enteros consecutivos. No lo pensamos para este fin pero Vocab ser√≠a √∫til en este aspecto. El √∫nico tema es que Vocab.fit y dem√°s m√©todos esperan listas de listas de t√≥kenes y a las etiquetas las encontramos en forma de listas de t√≥kenes simplemente. . train_df[&#39;Intencion&#39;].values . array([&#39;Cat_248&#39;, &#39;Cat_42&#39;, &#39;Cat_132&#39;, ..., &#39;Cat_293&#39;, &#39;Cat_138&#39;, &#39;Cat_219&#39;], dtype=object) . Podemos llevar la columna de las etiquetas a una lista de listas con train_df[&#39;Intencion&#39;].values.reshape(-1,1), de manera de poder interfacearlo con Vocab. Algo como train_df[[&#39;Intencion&#39;]].values para que Pandas devuelva un DataFrame en vez de una Series tambi√©n funcionar√≠a. . train_etiquetas = train_df[[&#39;Intencion&#39;]].values valid_etiquetas = valid_df[[&#39;Intencion&#39;]].values train_etiquetas . array([[&#39;Cat_248&#39;], [&#39;Cat_42&#39;], [&#39;Cat_132&#39;], ..., [&#39;Cat_293&#39;], [&#39;Cat_138&#39;], [&#39;Cat_219&#39;]], dtype=object) . Todo lo que tenga que ver con limitaci√≥n del vocabulario o agregado de t√≥kenes especiales no nos interesa para este caso de uso. . vocabulario_etiquetas = Vocab(t√≥ken_desconocido=None).fit(train_etiquetas) train_etiquetas = vocabulario_etiquetas.t√≥kenes_a_√≠ndices(train_etiquetas) valid_etiquetas = vocabulario_etiquetas.t√≥kenes_a_√≠ndices(valid_etiquetas) train_etiquetas[:10] . [[6], [128], [0], [104], [6], [17], [8], [202], [306], [166]] . Ya casi estamos. Solo debemos reconvertir a las etiquetas en una lista de √≠ndices (su dimensi√≥n original) con un recurso que ya conocemos. . train_etiquetas = list(chain(*train_etiquetas)) valid_etiquetas = list(chain(*valid_etiquetas)) train_etiquetas[:10] . [6, 128, 0, 104, 6, 17, 8, 202, 306, 166] . Ahora est√°s en condiciones de seguir con la segunda parte. . Fuentes consultadas . http://anie.me/On-Torchtext/ | https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908 | .",
            "url": "https://matiasbattocchia.github.io/datitos/Preprocesamiento-de-texto-para-NLP-parte-1.html",
            "relUrl": "/Preprocesamiento-de-texto-para-NLP-parte-1.html",
            "date": " ‚Ä¢ Jul 23, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Graph isomorphism networks",
            "content": "Estas notas corresponden a la secci√≥n de c√≥digo de una charla que di en el seminario MachinLenin en 2019. Esta es la presentaci√≥n que us√© y este el repositorio. . Esta es una implementaci√≥n simple, con fines did√°cticos y para nada eficiente de la publicaci√≥n How powerful are graph neural networks?. Para una implementaci√≥n eficiente ver GINConv de Deep Graph Library (DGL). . La publicaci√≥n mencionada fue la que eleg√≠ para comenzar a aprender sobre redes neuronales de grafos (graph neural networks o GNN). Unas redes interesantes porque no todo es texto, ni todo son im√°genes, ni tablas... una vasta cantidad de informaci√≥n se representa en forma de grafo, y estas redes se especializan en esta estructura de datos. . Publicaciones . Hoy en d√≠a si tuviese que recomendar lecturas introductorias, ser√≠an los reviews de esta lista. . Reviews . Benchmarking Graph Neural Networks | Representation Learning on Graphs: Methods and Applications | http://snap.stanford.edu/proj/embeddings-www/ | . Arquitecturas . GAT | GIN | GCN | GraphSAGE | . import torch import networkx as nx %matplotlib inline . Adjacency matrix . G = nx.binomial_graph(5,0.5) # TODO: numerar los nodos en el gr√°fico nx.draw(G) . A = torch.tensor( nx.adjacency_matrix(G).todense(), dtype=torch.float32 ) A . tensor([[0., 0., 1., 1., 0.], [0., 0., 0., 1., 1.], [1., 0., 0., 1., 1.], [1., 1., 1., 0., 1.], [0., 1., 1., 1., 0.]]) . X = torch.randint(low=0, high=2, size=(5,2), dtype=torch.float32) X . tensor([[0., 1.], [0., 0.], [1., 0.], [0., 0.], [0., 0.]]) . A @ X . tensor([[1., 0.], [0., 0.], [0., 1.], [1., 1.], [1., 0.]]) . Dataset . import torch.utils.data import importlib gnns = importlib.import_module(&#39;powerful-gnns.util&#39;) class GraphDataset(torch.utils.data.Dataset): &quot;&quot;&quot; Levanta los datasets de Powerful-GNNS. &quot;&quot;&quot; def __init__(self, dataset, degree_as_tag=False): self.data, self.classes = gnns.load_data(dataset, degree_as_tag) self.features = self.data[0].node_features.shape[1] def __len__(self): return len(self.data) def __getitem__(self, idx): graph = self.data[idx] adjacency_matrix = nx.adjacency_matrix( graph.g ).todense() item = {} item[&#39;adjacency_matrix&#39;] = torch.tensor(adjacency_matrix, dtype=torch.float32) item[&#39;node_features&#39;] = graph.node_features item[&#39;label&#39;] = graph.label return item . DS = GraphDataset(&#39;PROTEINS&#39;) . loading data # classes: 2 # maximum node tag: 3 # data: 1113 . DL = torch.utils.data.DataLoader(DS) . Net modules . class GINConv(torch.nn.Module): def __init__(self, hidden_dim): super().__init__() self.linear = torch.nn.Linear(hidden_dim, hidden_dim) def forward(self, A, X): &quot;&quot;&quot; Params A [batch x nodes x nodes]: adjacency matrix X [batch x nodes x features]: node features matrix Returns - X&#39; [batch x nodes x features]: updated node features matrix &quot;&quot;&quot; X = self.linear(X + A @ X) X = torch.nn.functional.relu(X) return X . class GNN(torch.nn.Module): def __init__(self, input_dim, hidden_dim, output_dim, n_layers): super().__init__() self.in_proj = torch.nn.Linear(input_dim, hidden_dim) self.convs = torch.nn.ModuleList() for _ in range(n_layers): self.convs.append(GINConv(hidden_dim)) # In order to perform graph classification, each hidden state # [batch x nodes x hidden_dim] is concatenated, resulting in # [batch x nodes x hiddem_dim*(1+n_layers)], then aggregated # along nodes dimension, without keeping that dimension: # [batch x hiddem_dim*(1+n_layers)]. self.out_proj = torch.nn.Linear(hidden_dim*(1+n_layers), output_dim) def forward(self, A, X): X = self.in_proj(X) hidden_states = [X] for layer in self.convs: X = layer(A, X) hidden_states.append(X) X = torch.cat(hidden_states, dim=2).sum(dim=1) X = self.out_proj(X) return X . Train loop . model = GNN(input_dim=DS.features, hidden_dim=3, output_dim=DS.classes, n_layers=3) criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.01) EPOCHS = 5 for epoch in range(EPOCHS): running_loss = 0.0 for i, batch in enumerate(DL): A = batch[&#39;adjacency_matrix&#39;] X = batch[&#39;node_features&#39;] labels = batch[&#39;label&#39;] optimizer.zero_grad() outputs = model(A, X) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f&#39;{epoch} - loss: {running_loss/(i+1)}&#39;) . 0 - loss: 1.2691171061103663 1 - loss: 0.32851389047806046 2 - loss: 0.944981049655904 3 - loss: 0.2894633889519622 4 - loss: 0.3056941165221455 . TODO . Dropout | Mini-batches (collate_fn for padding) | Batch normalization . | Extended neighborhood X = self.linear(X + A @ X + A**2 @ X) . | Node classification | Link prediction | .",
            "url": "https://matiasbattocchia.github.io/datitos/Graph-isomorphism-network.html",
            "relUrl": "/Graph-isomorphism-network.html",
            "date": " ‚Ä¢ Oct 24, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "Grafos",
            "content": "Este es el material de una clase que doy en Digital House en el curso de ciencia de datos. . &#191;Qu&#233; es un grafo? . . Red ‚Äî grafo, graph Nodos ‚Äî v√©rtices, nodes | Conexiones ‚Äî aristas, bordes, edges | . | . &#191;Para qu&#233; sirven? . Redes de transporte . Redes de interacci&#243;n . Redes de comunicaci&#243;n . Es una estructura de datos √∫til para representar: . redes sociales | m√°quinas de estado | mol√©culas | mapas conceptuales | estructura del lenguaje | redes de transporte | y mucho m√°s... | . Ver glosario de teor√≠a de grafos. . NetworkX . https://networkx.github.io . NetworkX es un paquete de Python para la creaci√≥n, manipulaci√≥n, y el estudio de la estructura, din√°mica, y las funciones de las redes complejas. . !pip install --upgrade networkx . %matplotlib inline import networkx as nx G = nx.Graph() . G.add_node(&#39;Buenos Aires&#39;) G.add_node(&#39;C√≥rdoba&#39;) G.add_node(&#39;Mendoza&#39;) nx.draw(G) . G.add_edge(&#39;Buenos Aires&#39;, &#39;C√≥rdoba&#39;, distancia=647) G.add_edge(&#39;Buenos Aires&#39;, &#39;Mendoza&#39;, distancia=948) G.add_edge(&#39;C√≥rdoba&#39;, &#39;Mendoza&#39;, distancia=682) nx.draw(G) . G.add_edge(&#39;Tucum√°n&#39;,&#39;Salta&#39;, distancia=227) nx.draw(G) . Subgrafo: subconjunto de nodos y conexiones. | Componente conectada: grupo de nodos conectados. | . G.nodes . NodeView((&#39;Buenos Aires&#39;, &#39;C√≥rdoba&#39;, &#39;Mendoza&#39;, &#39;Tucum√°n&#39;, &#39;Salta&#39;)) . G.edges . EdgeView([(&#39;Buenos Aires&#39;, &#39;C√≥rdoba&#39;), (&#39;Buenos Aires&#39;, &#39;Mendoza&#39;), (&#39;C√≥rdoba&#39;, &#39;Mendoza&#39;), (&#39;Tucum√°n&#39;, &#39;Salta&#39;)]) . G.degree[&#39;Mendoza&#39;] . 2 . Graficando . pos=nx.planar_layout(G) # dibujar etiquetas nx.draw_networkx_labels(G, pos=pos) # dibujar nodos nx.draw_networkx_nodes(G, pos=pos) # dibujar conexiones nx.draw_networkx_edges(G, pos=pos); . Graficar grafos no es una tarea sencilla y NetworkX lo deja en mano de otras aplicaciones, como Gephi. . Tipos de grafos . Dirigido . nx.DiGraph . . Multigrafo . nx.MultiGraph, nx.MultiDiGraph . . Algoritmos comunes . Grafo de ejemplo: Familias florentinas . Familias que se disputaron el control pol√≠tico de la ciudad de Florencia alrededor de 1430. Dos facciones fueron dominantes en la disputa: Medicis y Strozzis. . Dataset de uniones maritales y de negocio entre familias. . Fuente: Padgett, J. F., &amp; Ansell, C. K. (1993). Robust Action and the Rise of the Medici, 1400-1434. American Journal of Sociology, 98(6), 1259-1319. . G = nx.florentine_families_graph() nx.draw_networkx(G) . G.nodes . NodeView((&#39;Acciaiuoli&#39;, &#39;Medici&#39;, &#39;Castellani&#39;, &#39;Peruzzi&#39;, &#39;Strozzi&#39;, &#39;Barbadori&#39;, &#39;Ridolfi&#39;, &#39;Tornabuoni&#39;, &#39;Albizzi&#39;, &#39;Salviati&#39;, &#39;Pazzi&#39;, &#39;Bischeri&#39;, &#39;Guadagni&#39;, &#39;Ginori&#39;, &#39;Lamberteschi&#39;)) . Componentes conectadas . https://en.wikipedia.org/wiki/Component_(graph_theory) . . for sub_graph in nx.connected_components(G): print(sub_graph) . {&#39;Strozzi&#39;, &#39;Bischeri&#39;, &#39;Acciaiuoli&#39;, &#39;Salviati&#39;, &#39;Pazzi&#39;, &#39;Castellani&#39;, &#39;Ridolfi&#39;, &#39;Albizzi&#39;, &#39;Peruzzi&#39;, &#39;Medici&#39;, &#39;Ginori&#39;, &#39;Lamberteschi&#39;, &#39;Barbadori&#39;, &#39;Tornabuoni&#39;, &#39;Guadagni&#39;} . El camino m&#225;s corto . https://en.wikipedia.org/wiki/Shortest_path_problem . . Camino: secuencia de nodos conectados. | . Aplicaciones . Google Maps | LinkedIn | . nx.shortest_path(G, source=&#39;Medici&#39;, target=&#39;Strozzi&#39;, weight=None) . [&#39;Medici&#39;, &#39;Ridolfi&#39;, &#39;Strozzi&#39;] . &#193;rbol recubridor m&#237;nimo . https://en.wikipedia.org/wiki/Minimum_spanning_tree . . Aplicaciones . Tendido de redes | . mst = nx.minimum_spanning_tree(G, weight=None) nx.draw_networkx(mst) . Pagerank . https://en.wikipedia.org/wiki/PageRank . . rank = nx.pagerank(G, weight=None) sorted(rank.items(), key=lambda item: item[1], reverse=True)[:5] . [(&#39;Medici&#39;, 0.14581844065218275), (&#39;Guadagni&#39;, 0.09839859717156552), (&#39;Strozzi&#39;, 0.08809849082725613), (&#39;Albizzi&#39;, 0.079121502380729), (&#39;Tornabuoni&#39;, 0.07127928374676082)] . Ejemplo de uso en aprendizaje autom√°tico: Red de influencia de inversores. . Intermediaci&#243;n . https://en.wikipedia.org/wiki/Betweenness_centrality . . rank = nx.betweenness_centrality(G, weight=None) sorted(rank.items(), key=lambda item: item[1], reverse=True)[:5] . [(&#39;Medici&#39;, 0.521978021978022), (&#39;Guadagni&#39;, 0.2545787545787546), (&#39;Albizzi&#39;, 0.21245421245421245), (&#39;Salviati&#39;, 0.14285714285714288), (&#39;Ridolfi&#39;, 0.11355311355311355)] . Modularidad . https://en.wikipedia.org/wiki/Modularity_(networks) . . nx.algorithms.community.modularity_max.greedy_modularity_communities(G) . [frozenset({&#39;Acciaiuoli&#39;, &#39;Medici&#39;, &#39;Pazzi&#39;, &#39;Ridolfi&#39;, &#39;Salviati&#39;, &#39;Tornabuoni&#39;}), frozenset({&#39;Barbadori&#39;, &#39;Bischeri&#39;, &#39;Castellani&#39;, &#39;Peruzzi&#39;, &#39;Strozzi&#39;}), frozenset({&#39;Albizzi&#39;, &#39;Ginori&#39;, &#39;Guadagni&#39;, &#39;Lamberteschi&#39;})] . &#205;ndice Adamic/Adar . https://en.wikipedia.org/wiki/Adamic/Adar_index . . Aplicaciones . Recomendaci√≥n de contactos | Recomendaci√≥n de productos | . predicciones = nx.adamic_adar_index(G, ebunch=[(&#39;Medici&#39;, &#39;Strozzi&#39;), (&#39;Medici&#39;, &#39;Pazzi&#39;)]) for predicci√≥n in predicciones: print(predicci√≥n) . (&#39;Medici&#39;, &#39;Strozzi&#39;, 0.9102392266268373) (&#39;Medici&#39;, &#39;Pazzi&#39;, 1.4426950408889634) . Pandas . import pandas as pd URL = &#39;http://cdn.buenosaires.gob.ar/datosabiertos/datasets/bicicletas-publicas/recorridos-realizados-2019.csv&#39; ARCHIVO = &#39;muestreo_recorridos.csv.gz&#39; df = pd.read_csv(ARCHIVO, low_memory=False) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2850451 entries, 0 to 2850450 Data columns (total 19 columns): id_usuario object edad_usuario float64 genero_usuario object fecha_origen_recorrido object id_estacion_origen object nombre_estacion_origen object direccion_estacion_origen object capacidad_estacion_origen float64 lat_estacion_origen float64 long_estacion_origen float64 duracion_recorrido object fecha_destino_recorrido object id_estacion_destino object nombre_estacion_destino object direccion_estacion_destino object capacidad_estacion_destino float64 lat_estacion_destino float64 long_estacion_destino float64 servicio object dtypes: float64(7), object(12) memory usage: 413.2+ MB . df.head() . id_usuario edad_usuario genero_usuario fecha_origen_recorrido id_estacion_origen nombre_estacion_origen direccion_estacion_origen capacidad_estacion_origen lat_estacion_origen long_estacion_origen duracion_recorrido fecha_destino_recorrido id_estacion_destino nombre_estacion_destino direccion_estacion_destino capacidad_estacion_destino lat_estacion_destino long_estacion_destino servicio . 0 137063_0 | 26.0 | FEMENINO | 2019-01-07 17:06:49 | 156.0 | Plaza Alemania | Plaza Alemania: Del Libertador Av y Cavia | NaN | -34.577849 | -58.407966 | 0 days 00:24:48.000000000 | 2019-01-07 17:31:37 | 170.0 | San Luis y Ecuador | San Luis 2862 entre Ecuador y Boulogne Sur Mer | NaN | -34.599733 | -58.405966 | antiguo | . 1 79103 | NaN | NaN | 2019-06-10 11:51:12 | 29.0 | Parque Centenario | Patricias Argentinas Av. y Estivao | 30.0 | -34.607941 | -58.433557 | 0 days 00:24:57.000000000 | 2019-06-10 12:16:09 | 87.0 | Guayaquil | Guayaquil y Doblas | 16.0 | -34.619845 | -58.431494 | nuevo | . 2 387321 | NaN | NaN | 2019-07-30 15:20:38 | 289.0 | BARRANCAS DE BELGRANO | Sucre, Antonio Jose De, Mcal. y Vertiz Virrey Av. | 24.0 | -34.559793 | -58.448432 | 0 days 00:51:39.000000000 | 2019-07-30 16:12:17 | 175.0 | Constituci√≥n | Garay, Juan De Av. 1050 | 36.0 | -34.626741 | -58.380935 | nuevo | . 3 113196 | NaN | NaN | 2019-04-28 00:59:32 | 56.0 | Plaza Palermo Viejo | Costa Rica y Armenia | 16.0 | -34.588567 | -58.425999 | 0 days 00:21:24.000000000 | 2019-04-28 01:20:56 | 69.0 | Ecuador | Ecuador 1226 | 16.0 | -34.596101 | -58.404609 | nuevo | . 4 248012 | NaN | NaN | 2019-06-12 15:04:26 | 153.0 | JUAN MANUEL DE BLANES | Blanes, Juan Manuel 383 | 16.0 | -34.630777 | -58.362070 | 0 days 00:13:16.000000000 | 2019-06-12 15:17:42 | 126.0 | MINISTERIO DE JUSTICIA Y SEGURIDAD | Regimiento De Patricios Av. y Araoz De Lamadri... | 16.0 | -34.640267 | -58.369224 | nuevo | . El DataFrame debe contener al menos dos columnas con nombres de nodos (origen y destino) y cero o m√°s columnas con atributos de las conexiones. . Cada fila se procesa como una conexi√≥n. . df[&#39;minutos_viaje&#39;] = pd.to_timedelta(df.duracion_recorrido, unit=&#39;minute&#39;, errors=&#39;coerce&#39;) . pre_grafo = df.groupby([&#39;nombre_estacion_origen&#39;,&#39;nombre_estacion_destino&#39;]) .minutos_viaje .mean(numeric_only=False) .to_frame() .reset_index() pre_grafo.head() . nombre_estacion_origen nombre_estacion_destino minutos_viaje . 0 11 de septiembre | 11 de septiembre | 00:59:58.833333 | . 1 11 de septiembre | Arribe√±os | 00:08:51 | . 2 11 de septiembre | BARRANCAS DE BELGRANO | 00:14:15 | . 3 11 de septiembre | Balb√≠n | 00:10:22 | . 4 11 de septiembre | Catedral | 00:54:11 | . estaciones = nx.convert_matrix.from_pandas_edgelist( pre_grafo, source=&#39;nombre_estacion_origen&#39;, target=&#39;nombre_estacion_destino&#39;, edge_attr=&#39;minutos_viaje&#39;, create_using=nx.MultiDiGraph ) . nx.draw(estaciones) . üò± üò± üò± . Pr&#225;ctica . Dataset . https://data.buenosaires.gob.ar/dataset/bicicletas-publicas . M&#225;s datasets . https://snap.stanford.edu/data/ . Ideas . https://medium.com/@fcatalano/bicisendas-en-buenos-aires-a29f62bc9e7c ‚Äî &quot;La red de bicicletas fue diagramada como un grafo dirigido. Esto, porque nos interesaba representar los recorridos realizados respetando el sentido de los viajes.&quot; . An√°lisis exploratorio del dataset | An√°lisis de in-degree y out-degree | . | . https://towardsdatascience.com/buenos-aires-bicycle-lanes-ii-1a40b13ccc25 ‚Äî &quot;[...] we built a graph where two users shared a link if and only if at least one of them had taken a bicycle at approximately the same time from the same station, and returned them together (also to the same station).&quot; . Distribuci√≥n grados | An√°lisis exploratorio del grafo | . | . https://medium.com/@martinpalazzo/buenos-aires-bicycle-lanes-iii-d0ca4539e767 ‚Äî &quot;The first step to understand the communities of stations is to build a network where each node is a station and each edge between station is the quantity of bicycle journeys.&quot; . Modularidad | . | .",
            "url": "https://matiasbattocchia.github.io/datitos/Grafos.html",
            "relUrl": "/Grafos.html",
            "date": " ‚Ä¢ Sep 27, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Pandas (parte 1)",
            "content": "import pandas as pd . Estructuras de datos . Dos tipos de datos muy usados en Python son las listas (list) y los diccionarios (dict). Ambos son √∫tiles para coleccionar valores de todo tipo. . lista = [&#39;hidr√≥geno&#39;, &#39;helio&#39;, &#39;litio&#39;] lista . [&#39;hidr√≥geno&#39;, &#39;helio&#39;, &#39;litio&#39;] . Podemos referenciar un valor coleccionado utilizando el operador √≠ndice [ ]. Las listas indexan por posici√≥n. En Python se empieza contar desde cero, por lo tanto las posiciones tambi√©n. . lista[0] . &#39;hidr√≥geno&#39; . lista[0] = &#39;?&#39; lista . [&#39;?&#39;, &#39;helio&#39;, &#39;litio&#39;] . Los diccionarios son colecciones de pares etiqueta:valor e indexan por etiqueta. No es posible referenciar a un elemento por posici√≥n. . diccionario = { &#39;H&#39; :&#39;hidr√≥geno&#39;, &#39;He&#39;:&#39;helio&#39;, &#39;Li&#39;:&#39;litio&#39;, } diccionario . {&#39;H&#39;: &#39;hidr√≥geno&#39;, &#39;He&#39;: &#39;helio&#39;, &#39;Li&#39;: &#39;litio&#39;} . diccionario[&#39;H&#39;] . &#39;hidr√≥geno&#39; . diccionario[&#39;Be&#39;] = &#39;berilio&#39; diccionario . {&#39;H&#39;: &#39;hidr√≥geno&#39;, &#39;He&#39;: &#39;helio&#39;, &#39;Li&#39;: &#39;litio&#39;, &#39;Be&#39;: &#39;berilio&#39;} . Pandas . Una Series es un objeto unidimensional, similar a una columna en una tabla. Se comporta como una lista y tambi√©n le asigna una etiqueta a cada elemento en la colecci√≥n, comport√°ndose como un diccionario. Por defecto, cada elemento recibir√° una etiqueta que va de 0 a N-1, donde N es la longitud/tama√±o de la colecci√≥n. . pd.Series([&#39;hidr√≥geno&#39;, &#39;helio&#39;, &#39;litio&#39;]) . 0 hidr√≥geno 1 helio 2 litio dtype: object . Nota: dtype es el tipo de los elementos de la colecci√≥n. En este caso object hace referencia a objetos en general, es decir, cualquier tipo de objeto: entero (int), flotante (float), texto (str), etc√©tera. . pd.Series([&#39;hidr√≥geno&#39;, &#39;helio&#39;, &#39;litio&#39;], index=[&#39;H&#39;, &#39;He&#39;, &#39;Li&#39;]) . H hidr√≥geno He helio Li litio dtype: object . s = pd.Series({ &#39;H&#39; :&#39;hidr√≥geno&#39;, &#39;He&#39;:&#39;helio&#39;, &#39;Li&#39;:&#39;litio&#39;, }) s . H hidr√≥geno He helio Li litio dtype: object . s.index . Index([&#39;H&#39;, &#39;He&#39;, &#39;Li&#39;], dtype=&#39;object&#39;) . El √≠ndice es un objeto del tipo Index, aportado por Pandas. . s.values . array([&#39;hidr√≥geno&#39;, &#39;helio&#39;, &#39;litio&#39;], dtype=object) . Los valores se coleccionan en arreglos de NumPy (objetos del tipo array). Los arreglos son similares a las listas, tienen m√°s funcionalidades. . s[&#39;H&#39;] . &#39;hidr√≥geno&#39; . s[&#39;Be&#39;] = &#39;berilio&#39; s . H hidr√≥geno He helio Li litio Be berilio dtype: object . . Un DataFrame es una estructura tabular (bidimensional) compuesta por filas y columnas, similar a una hoja de c√°lculo, una tabla de una base de datos, o a un data.frame del lenguaje R. . Se puede pensar al DataFrame como una Series de Series: una Series cuyo √≠ndice son los nombres de las columnas y cuyos elementos son Series que se comportan como columnas. El √≠ndice de las Series-columna son los nombres de las filas. . pd.DataFrame([ [1, 1.008], [2, 4.003], [3, 6.941], [4, 9.012] ]) . 0 1 . 0 1 | 1.008 | . 1 2 | 4.003 | . 2 3 | 6.941 | . 3 4 | 9.012 | . df = pd.DataFrame( [[1, 1.008], [2, 4.003], [3, 6.941], [4, 9.012]], index=[&#39;H&#39;, &#39;He&#39;, &#39;Li&#39;, &#39;Be&#39;], columns=[&#39;n√∫mero_at√≥mico&#39;, &#39;masa_at√≥mica&#39;] ) df . n√∫mero_at√≥mico masa_at√≥mica . H 1 | 1.008 | . He 2 | 4.003 | . Li 3 | 6.941 | . Be 4 | 9.012 | . Otras formas de inicializar un DataFrame, usando: . diccionario de listas | diccionario de diccionarios | lista de diccionarios | . df.index . Index([&#39;H&#39;, &#39;He&#39;, &#39;Li&#39;, &#39;Be&#39;], dtype=&#39;object&#39;) . df.columns . Index([&#39;n√∫mero_at√≥mico&#39;, &#39;masa_at√≥mica&#39;], dtype=&#39;object&#39;) . df.values . array([[1. , 1.008], [2. , 4.003], [3. , 6.941], [4. , 9.012]]) . Nota: Los nombres de columnas tambi√©n son un √≠ndice (Index). Recapitulando, las Series tiene un √≠ndice y los DataFrames tienen dos. Como Pandas piensa la tabla como una colecci√≥n de columnas, para obtener un valor de una celda primero hay que acceder a la columna y luego a la fila. . df[&#39;masa_at√≥mica&#39;] . H 1.008 He 4.003 Li 6.941 Be 9.012 Name: masa_at√≥mica, dtype: float64 . df[&#39;nombre&#39;] = s df . n√∫mero_at√≥mico masa_at√≥mica nombre . H 1 | 1.008 | hidr√≥geno | . He 2 | 4.003 | helio | . Li 3 | 6.941 | litio | . Be 4 | 9.012 | berilio | . df[&#39;masa_at√≥mica&#39;][&#39;He&#39;] . 4.003 . Nota: Los √≠ndices se corresponden con las dimensiones del objeto, un valor de una celda no tiene √≠ndices, por lo que es un objeto cero-dimensional, tambi√©n llamado escalar. . Carga de datos . Ver m√°s: http://pandas.pydata.org/pandas-docs/stable/io.html . Pandas importa y exporta datos de y hacia gran cantidad de formatos. . CSV . df = pd.read_csv(&#39;datos/properati_b√°sico.csv.gz&#39;) . Para exportar un DataFrame a un archivo CSV podemos usar . df.to_csv(&#39;archivo.csv&#39;) . Excel . Requiere un paquete adicional. . !pip install xlrd . Leer una hoja de c√°lculo. . pd.read_excel(&#39;datos/archivo.xlsx&#39;) . SQL . El siguiente c√≥digo muestra c√≥mo consultar una base de datos. . import sqlite3 conexi√≥n = sqlite3.connect(&#39;datos/db.sqlite&#39;) consulta = &quot;SELECT * FROM tabla&quot; pd.read_sql(consulta, conexi√≥n) . BigQuery . Requiere un paquete adicional. . !pip install pandas-gbq . El resto es parecido a importar desde una base de datos relacional. . consulta = &quot;SELECT * FROM tabla&quot; pd.read_gbq(consulta, project_id=&#39;properati-data-public&#39;) . Inspecci&#243;n . df.head() . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 0 2017-11-01 | departamento | -34.629118 | -58.480835 | 275000.0 | NaN | NaN | 5.0 | FLORESTA | . 1 2017-11-03 | departamento | -34.627120 | -58.475595 | 268000.0 | 134.0 | 122.0 | 4.0 | FLORESTA | . 2 2017-11-03 | departamento | -34.630429 | -58.486193 | 88000.0 | 37.0 | 34.0 | 2.0 | FLORESTA | . 3 2017-11-09 | departamento | -34.628319 | -58.479828 | 134900.0 | 70.0 | 70.0 | 3.0 | FLORESTA | . 4 2017-11-14 | departamento | -34.620887 | -58.491288 | 69000.0 | NaN | 50.0 | 2.0 | FLORESTA | . El m√©todo info nos informa de ambos √≠ndices (filas y columnas), incluyendo cantidades. Nombres de columnas, tipos de datos, cantidades de valores no nulos, uso de memoria RAM. . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 24258 entries, 0 to 24257 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 fecha 24258 non-null object 1 tipo 24258 non-null object 2 lat 24258 non-null float64 3 lon 24258 non-null float64 4 precio 23141 non-null float64 5 superficie_total 22261 non-null float64 6 superficie_cubierta 22664 non-null float64 7 ambientes 20533 non-null float64 8 barrio 24258 non-null object dtypes: float64(6), object(3) memory usage: 1.7+ MB . El m√©todo describe muestra estad√≠sticas b√°sicas acerca de las columnas n√∫mericas. Aplica sobre todas las columnas n√∫mericas, incluso sobre aquellas en las que no tiene sentido hacer estad√≠sticas (lat y lon por ejemplo). . df.describe() . lat lon precio superficie_total superficie_cubierta ambientes . count 24258.000000 | 24258.000000 | 2.314100e+04 | 22261.000000 | 22664.000000 | 20533.000000 | . mean -34.599238 | -58.436230 | 2.779905e+05 | 133.717578 | 137.000176 | 2.874056 | . std 0.026307 | 0.039340 | 3.611446e+05 | 1633.426319 | 2418.268251 | 1.690704 | . min -34.695717 | -58.529982 | 4.870050e+03 | 0.000000 | 1.000000 | 1.000000 | . 25% -34.618590 | -58.464357 | 1.200000e+05 | 46.000000 | 41.000000 | 2.000000 | . 50% -34.598897 | -58.436198 | 1.800000e+05 | 70.000000 | 62.000000 | 3.000000 | . 75% -34.580350 | -58.403884 | 2.990000e+05 | 120.000000 | 104.000000 | 4.000000 | . max -34.536066 | -58.353946 | 1.350000e+07 | 184000.000000 | 263960.000000 | 38.000000 | . Indexaci&#243;n . Por indexaci√≥n nos referimos a la selecci√≥n de un subconjunto de un DataFrame o de una Series. Por ahora hemos visto un uso b√°sico de los corchetes [ ], el operador √≠ndice, para acceder a columnas de un DataFrame y a valores de una Series. En el caso de las Series los casos de uso del operador no difieren pr√°cticamente de lo que se puede hacer con un arreglo de NumPy, sin embargo en el caso de los DataFrames se complejiza ya que permite seleccionar por filas (no lo vimos) o por columnas (s√≠ lo vimos). . Los corchetes existen en Pandas por conveniencia para hacer algunas operaciones m√°s simples, lo cual tambi√©n traer√° aparejado limitaciones. Pandas va a determinar si estamos queriendo acceder a la tabla usando el √≠ndice que se llama index (nombres filas) o el que se llama columns. . A su vez hay dos maneras de utilizar los √≠ndices: . por etiqueta (como en los diccionarios), | por posici√≥n (como en las listas). | . Esta modalidad dual no est√° presente en los diccionarios, que no pueden ser indexados por posici√≥n, ni en las listas, que no pueden ser indexadas por etiqueta. Est√° presente en Pandas y nos permite acceder a los datos de la forma conveniente seg√∫n el caso. . df[&#39;barrio&#39;].head() . 0 FLORESTA 1 FLORESTA 2 FLORESTA 3 FLORESTA 4 FLORESTA Name: barrio, dtype: object . No es posible obtener una sola fila con [ ], pero s√≠ un subconjunto de filas y tambi√©n un subconjunto de columnas. Antes de mostrar c√≥mo hacerlo, introducimos tres nuevos conceptos: slice, lista de √≠ndices y m√°scara booleana. . Cortes . Existe en Python un tipo de objeto llamado slice, traducido como corte o rebanada, cuyo fin es ayudar a producir sublistas a partir de listas, &quot;cortando&quot; una lista desde una posici√≥n inicial hasta una posici√≥n final. En realidad la lista original no es modificada sino que se devuelve una nueva, usando los elementos que ya existen en la original (los objetos no son copiados). . frutas = [&#39;naranja&#39;, &#39;banana&#39;, &#39;anan√°&#39;, &#39;durazno&#39;, &#39;uva&#39;] frutas . [&#39;naranja&#39;, &#39;banana&#39;, &#39;anan√°&#39;, &#39;durazno&#39;, &#39;uva&#39;] . Sabemos c√≥mo obtener valores individuales. . frutas[2] . &#39;anan√°&#39; . Supongamos que nos interesa conseguir una sublista con las tres primeras frutas, ser√≠an los elementos en las posiciones 0, 1, 2, por lo que deber√≠amos cortar desde 0 hasta 3 (no inclusive). . corte = slice(0, 3) corte . slice(0, 3, None) . frutas[corte] . [&#39;naranja&#39;, &#39;banana&#39;, &#39;anan√°&#39;] . Podemos obtener el mismo resultado con un atajo. Los atajos reciben el nombre de az√∫car sint√°ctica, son expresiones c√≥modas para idiomas frecuentes, que existen en paralelo a las formas regulares del lenguaje. . frutas[0:3] . [&#39;naranja&#39;, &#39;banana&#39;, &#39;anan√°&#39;] . frutas[:3] . [&#39;naranja&#39;, &#39;banana&#39;, &#39;anan√°&#39;] . frutas[3:] . [&#39;durazno&#39;, &#39;uva&#39;] . frutas[:] . [&#39;naranja&#39;, &#39;banana&#39;, &#39;anan√°&#39;, &#39;durazno&#39;, &#39;uva&#39;] . corte = slice(None) corte . slice(None, None, None) . frutas[corte] . [&#39;naranja&#39;, &#39;banana&#39;, &#39;anan√°&#39;, &#39;durazno&#39;, &#39;uva&#39;] . Listas de &#237;ndices y m&#225;scaras booleanas . Los arreglos de NumPy son como las listas de Python pero con funcionalidades agregadas. Admiten indexado por posici√≥n, por corte y a√±aden nuevas maneras de formar sub-arreglos, mediante . listas o rangos de posiciones, | listas de valores booleanos, llamadas m√°scaras. | . import numpy as np frutas = np.array([&#39;naranja&#39;, &#39;banana&#39;, &#39;anan√°&#39;, &#39;durazno&#39;, &#39;uva&#39;]) frutas . array([&#39;naranja&#39;, &#39;banana&#39;, &#39;anan√°&#39;, &#39;durazno&#39;, &#39;uva&#39;], dtype=&#39;&lt;U7&#39;) . Listas de posiciones . Supongamos que queremos obtener los elementos naranja, anan√°, uva. A diferencia de las listas que solo permiten enteros o cortes, NumPy nos permite pasar una lista con las posiciones de inter√©s. . frutas[[0,2,4]] . array([&#39;naranja&#39;, &#39;anan√°&#39;, &#39;uva&#39;], dtype=&#39;&lt;U7&#39;) . En Python existe un tipo de objeto llamado rango (range), que se puede crear ‚Äîcomo casi todos los tipos de objetos en el lenguaje‚Äî utilizando una funci√≥n hom√≥nima, que representa una colecci√≥n de n√∫meros desde un valor inicial hasta un valor final (no inclusive) utilizando un intervalo determinado. A diferencia de slice, los rangos pueden ser convertidos en listas. . Si se trata de un patr√≥n regular, es mejor definir un rango antes que una lista. . rango = range(0, 5, 2) rango . range(0, 5, 2) . list(rango) . [0, 2, 4] . frutas[rango] . array([&#39;naranja&#39;, &#39;anan√°&#39;, &#39;uva&#39;], dtype=&#39;&lt;U7&#39;) . M&#225;scaras . La m√°scara debe tener el mismo largo que el arreglo y estar compuesta por valores booleanos (bool). En este caso, cinco elementos. . m√°scara = [True, False, True, False, True] frutas[m√°scara] . array([&#39;naranja&#39;, &#39;anan√°&#39;, &#39;uva&#39;], dtype=&#39;&lt;U7&#39;) . Se filtran aquellos elementos cuya posici√≥n se corresponde con las posiciones de los valores False de la m√°scara, se seleccionan aquellos donde True. La m√°scara sirve para ocultar algunas partes y mostrar otras. . . Podemos utilizar una lista de etiquetas para obtener un subconjunto de columnas. . df[[&#39;barrio&#39;, &#39;ambientes&#39;, &#39;precio&#39;]].head() . barrio ambientes precio . 0 FLORESTA | 5.0 | 275000.0 | . 1 FLORESTA | 4.0 | 268000.0 | . 2 FLORESTA | 2.0 | 88000.0 | . 3 FLORESTA | 3.0 | 134900.0 | . 4 FLORESTA | 2.0 | 69000.0 | . Podemos utilizar un corte para obtener un subconjunto de filas por posici√≥n. . df[:5] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 0 2017-11-01 | departamento | -34.629118 | -58.480835 | 275000.0 | NaN | NaN | 5.0 | FLORESTA | . 1 2017-11-03 | departamento | -34.627120 | -58.475595 | 268000.0 | 134.0 | 122.0 | 4.0 | FLORESTA | . 2 2017-11-03 | departamento | -34.630429 | -58.486193 | 88000.0 | 37.0 | 34.0 | 2.0 | FLORESTA | . 3 2017-11-09 | departamento | -34.628319 | -58.479828 | 134900.0 | 70.0 | 70.0 | 3.0 | FLORESTA | . 4 2017-11-14 | departamento | -34.620887 | -58.491288 | 69000.0 | NaN | 50.0 | 2.0 | FLORESTA | . Nota: En muchos casos los nombres de filas van a coincidir con las posiciones de filas, ya que si no especificamos un √≠ndice, el √≠ndice por defecto que asigna Pandas a las filas va de 0 a N-1, siendo N la cantidad de filas. Estas etiquetas coinciden con la posici√≥n. . Tambi√©n podemos utilizar una m√°scara. Hay formas muy pr√°cticas de generar m√°scaras, la que se presenta a continuaci√≥n solo tiene fines demostrativos y jam√°s volver√° a ser utilizada. . cantidad_filas = len(df) # usando la funci√≥n np.null armamos un arreglo tan largo como la tabla, lleno de un valor especificado (False) m√°scara = np.full(cantidad_filas, False) # retocamos la m√°scara para recuperar solo las primeras dos filas m√°scara[0] = True m√°scara[1] = True df[m√°scara] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 0 2017-11-01 | departamento | -34.629118 | -58.480835 | 275000.0 | NaN | NaN | 5.0 | FLORESTA | . 1 2017-11-03 | departamento | -34.627120 | -58.475595 | 268000.0 | 134.0 | 122.0 | 4.0 | FLORESTA | . Resumimos las capacidades de [ ]: . Una etiqueta de columna, devuelve una columna. | Una lista de etiquetas de columnas, devuelve una subtabla. | Un corte de posiciones de filas, devuelve una subtabla. | Una m√°scara de posiciones de filas, devuelve una subtabla. | . Se nota el comportamiento de filas por sus posiciones y columnas por sus etiquetas. . Alternativamente se puede acceder a una columna como si fuera un atributo del DataFrame. . df.barrio.head() . 0 FLORESTA 1 FLORESTA 2 FLORESTA 3 FLORESTA 4 FLORESTA Name: barrio, dtype: object . Filas y columnas . ¬øQu√© pasa si queremos filtrar utilizando ambos √≠ndices simult√°neamente? En principio podr√≠amos hacer . df[columnas][filas] . sin embargo esta manera efect√∫a dos selecciones, una seguida de la otra, no es simult√°nea, lo que traer√° m√°s adelante implicancias para la asignaci√≥n. Para esta situaci√≥n Pandas provee las siguientes formas de indexar. . loc[filas, columnas] para acceder por etiquetas, | iloc[filas, columnas] para acceder por posici√≥n. | . Es un poco confuso al principio, la i de iloc hace pensar en la fila (o en la columna) n√∫mero i, o sea en la posici√≥n i. . La limitaci√≥n de estas formas es que no es posible mezclar etiquetas con posiciones; por ejemplo posici√≥n de filas y etiquetas de columnas, una combinaci√≥n bastante deseable. . # el orden que las filas tienen en la tabla (no hace falta entender esta l√≠nea en este momento); # el m√©todo sample sirve para obtener una muestra al azar de filas df = df.sample(frac=1, random_state=42) df.head() . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 19411 2018-02-23 | ph | -34.640644 | -58.500677 | 128000.0 | 65.0 | 60.0 | NaN | VILLA LURO | . 17819 2018-02-28 | departamento | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | BARRACAS | . 14611 2018-02-07 | departamento | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | COLEGIALES | . 22362 2018-02-20 | casa | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | VILLA DEVOTO | . 16962 2018-04-11 | departamento | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | VILLA CRESPO | . Por etiquetas . df.loc[100, &#39;fecha&#39;] . &#39;2018-02-21&#39; . La selecci√≥n de todas las columnas en R se logra con df[filas,] pero en Python esta sintaxis no es v√°lida, opciones son df.loc[filas] o df.loc[filas, :]. . df.loc[100] . fecha 2018-02-21 tipo departamento lat -34.629 lon -58.4797 precio 92000 superficie_total 48 superficie_cubierta 48 ambientes 3 barrio FLORESTA Name: 100, dtype: object . df.loc[[1,10,100], &#39;fecha&#39;] . 1 2017-11-03 10 2017-11-29 100 2018-02-21 Name: fecha, dtype: object . df.loc[[1,10,100], [&#39;superficie_cubierta&#39;, &#39;superficie_total&#39;]] . superficie_cubierta superficie_total . 1 122.0 | 134.0 | . 10 341.0 | 401.0 | . 100 48.0 | 48.0 | . La selecci√≥n de todas las filas en R se logra con df[,columnas] pero en Python esta sint√°xis no es v√°lida, se requiere de un corte total slice(None) o m√°s com√∫n, su az√∫car sint√°ctica df.loc[:, columnas]. . df.loc[:, [&#39;superficie_cubierta&#39;, &#39;superficie_total&#39;]].head() . superficie_cubierta superficie_total . 19411 60.0 | 65.0 | . 17819 32.0 | 34.0 | . 14611 48.0 | 58.0 | . 22362 112.0 | 112.0 | . 16962 40.0 | 47.0 | . Si bien las m√°scaras trabajan por posici√≥n y la indexaci√≥n de loc es por etiquetas, funcionan de todas formas ‚Äî es confuso pero ser√° uno de los idiomas m√°s √∫tiles e incluso hasta cobrar√° sentido m√°s adelante. . df.loc[m√°scara, [&#39;superficie_cubierta&#39;, &#39;superficie_total&#39;]] . superficie_cubierta superficie_total . 19411 60.0 | 65.0 | . 17819 32.0 | 34.0 | . IMPORTANTE: Pandas hace su propia interpretaci√≥n de los cortes en la indexaci√≥n por etiquetas. . Ambos extremos son inclusivos. | Van de un valor del √≠ndice a otro seg√∫n su orden en el √≠ndice. | list(df.columns) . [&#39;fecha&#39;, &#39;tipo&#39;, &#39;lat&#39;, &#39;lon&#39;, &#39;precio&#39;, &#39;superficie_total&#39;, &#39;superficie_cubierta&#39;, &#39;ambientes&#39;, &#39;barrio&#39;] . Por ejemplo, slice(&#39;tipo&#39;, &#39;precio&#39;) tiene el mismo efecto que [&#39;tipo&#39;, &#39;lat&#39;, &#39;lon&#39;, &#39;precio&#39;]. . df.loc[100, &#39;tipo&#39;:&#39;precio&#39;] . tipo departamento lat -34.629 lon -58.4797 precio 92000 Name: 100, dtype: object . Veamos el caso de las etiquetas de las filas. . list(df.index)[:10] . [19411, 17819, 14611, 22362, 16962, 19442, 15399, 12244, 6674, 4715] . Un corte desde la tercera etiqueta (14611) hasta la quinta (16962), slice(14611, 16962), es como pasar la lista [14611, 22362, 16962]. Si el orden del √≠ndice fuese otro, la selecci√≥n generada por el mismo corte, ser√≠a otra. . df.loc[14611:16962] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 14611 2018-02-07 | departamento | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | COLEGIALES | . 22362 2018-02-20 | casa | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | VILLA DEVOTO | . 16962 2018-04-11 | departamento | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | VILLA CRESPO | . Por posici&#243;n . df.iloc[:5] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 19411 2018-02-23 | ph | -34.640644 | -58.500677 | 128000.0 | 65.0 | 60.0 | NaN | VILLA LURO | . 17819 2018-02-28 | departamento | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | BARRACAS | . 14611 2018-02-07 | departamento | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | COLEGIALES | . 22362 2018-02-20 | casa | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | VILLA DEVOTO | . 16962 2018-04-11 | departamento | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | VILLA CRESPO | . IMPORTANTE: con iloc las columnas se seleccionan por su posici√≥n, no por sus etiquetas. . list(df.columns) . [&#39;fecha&#39;, &#39;tipo&#39;, &#39;lat&#39;, &#39;lon&#39;, &#39;precio&#39;, &#39;superficie_total&#39;, &#39;superficie_cubierta&#39;, &#39;ambientes&#39;, &#39;barrio&#39;] . df.iloc[:, [0,1]].head() . fecha tipo . 19411 2018-02-23 | ph | . 17819 2018-02-28 | departamento | . 14611 2018-02-07 | departamento | . 22362 2018-02-20 | casa | . 16962 2018-04-11 | departamento | . IMPORTANTE: iloc no acepta m√°scaras. . Resumen . Como con listas, es posible indexar por posici√≥n. | Como con diccionarios, es posible indexar por etiqueta. | Como con arreglos de NumPy, es posible indexar por m√°scaras booleanas. | Cualquiera de estos indexadores pueden ser escalares (int, str, ...), listas, o cortes. | Funcionan en filas y columnas. | Funcionan en √≠ndices jer√°rquicos (pr√≥ximo tema). | . Asignaci&#243;n . Las distintas formas de indexar tambi√©n sirven para guardar valores. . Supongamos que nos informan que las propiedades con etiquetas 333 y 666 est√°n mal anotadas como departamentos, que en realidad son casas. . df.loc[[333, 666]] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 333 2017-12-06 | departamento | -34.552416 | -58.456552 | 120000.0 | 37.0 | 33.0 | 1.0 | NU√ëEZ | . 666 2018-03-25 | departamento | -34.556262 | -58.464462 | 377944.0 | 110.0 | 108.0 | 4.0 | NU√ëEZ | . df.loc[[333, 666], &#39;tipo&#39;] = &#39;casa&#39; df.loc[[333, 666]] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 333 2017-12-06 | casa | -34.552416 | -58.456552 | 120000.0 | 37.0 | 33.0 | 1.0 | NU√ëEZ | . 666 2018-03-25 | casa | -34.556262 | -58.464462 | 377944.0 | 110.0 | 108.0 | 4.0 | NU√ëEZ | . Adicionalmente nos reportan que el precio de la propiedad con etiqueta 666 ha sido actualizado. . df.loc[666, [&#39;fecha&#39;, &#39;precio&#39;]] = [&#39;2018-10-28&#39;, 510000] # pasamos una lista de etiquetas para que devuelva un DataFrame, # esto es v√°lido m√°s all√° de que sea una lista de un solo elemento df.loc[[666]] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 666 2018-10-28 | casa | -34.556262 | -58.464462 | 510000.0 | 110.0 | 108.0 | 4.0 | NU√ëEZ | . &#205;ndices jer&#225;rquicos . Hasta ahora vimos c√≥mo trabajar con √≠ndices creados por defecto (cuando las etiquetas van de 0 a N-1) y con √≠ndices impuestos al cargar un archivo (las etiquetas de las columnas), sin embargo el √≠ndice puede ser modificado seg√∫n necesidad. Hay varias formas de definir un nuevo √≠ndice, una muy com√∫n es elegir a una columna para que se convierta en el nuevo √≠ndice. . df.set_index(&#39;barrio&#39;, inplace=True) df.head() . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes . barrio . VILLA LURO 2018-02-23 | ph | -34.640644 | -58.500677 | 128000.0 | 65.0 | 60.0 | NaN | . BARRACAS 2018-02-28 | departamento | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | . COLEGIALES 2018-02-07 | departamento | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | . VILLA DEVOTO 2018-02-20 | casa | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | . VILLA CRESPO 2018-04-11 | departamento | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | . Nota: Muchos m√©todos que transforman DataFrames nos devuelven un nuevo DataFrame, mientras que el original permanece intacto. El argumento inplace=True se usa para que el m√©todo transforme el DataFrame original; en este caso el m√©todo no devuelve nada (None). . Al convertir a la columna barrio en √≠ndice, esta columna dej√≥ de existir. Esto quiere decir que no podemos invocar a la columna como antes, haciendo df[&#39;barrio&#39;] o df.barrio, en todo caso podemos pedir df.index. Es importante recordar que aunque las etiquetas de las filas parezcan una columna, no lo son, as√≠ como las etiquetas de las columnas aunque parecen ser una fila, tampoco lo son. . De aqu√≠ en m√°s, podemos hacer indexaci√≥n con las nuevas etiquetas. . df.loc[&#39;PALERMO&#39;].head() . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes . barrio . PALERMO 2018-03-30 | casa | -34.578329 | -58.397749 | 3100000.0 | 450.0 | 350.0 | 3.0 | . PALERMO 2018-04-10 | departamento | -34.586787 | -58.416735 | 145000.0 | NaN | NaN | 3.0 | . PALERMO 2018-04-18 | departamento | -34.582436 | -58.414845 | 850000.0 | 270.0 | 240.0 | 6.0 | . PALERMO 2018-01-26 | departamento | -34.578773 | -58.415132 | 330000.0 | 74.0 | 64.0 | 3.0 | . PALERMO 2018-04-06 | departamento | -34.569005 | -58.436241 | 300000.0 | 110.0 | 98.0 | NaN | . resultado = df.loc[[&#39;PALERMO&#39;,&#39;CHACARITA&#39;], :] resultado.head(2) . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes . barrio . PALERMO 2018-03-30 | casa | -34.578329 | -58.397749 | 3100000.0 | 450.0 | 350.0 | 3.0 | . PALERMO 2018-04-10 | departamento | -34.586787 | -58.416735 | 145000.0 | NaN | NaN | 3.0 | . resultado.tail(2) . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes . barrio . CHACARITA 2017-12-09 | ph | -34.582115 | -58.455020 | 329000.0 | 265.0 | 215.0 | 5.0 | . CHACARITA 2018-02-11 | departamento | -34.590656 | -58.448731 | 92000.0 | 33.0 | 30.0 | 1.0 | . Cuando necesitemos recuperar al √≠ndice como columna, podemos traerla de vuelta. . df.reset_index(inplace=True) df.head() . barrio fecha tipo lat lon precio superficie_total superficie_cubierta ambientes . 0 VILLA LURO | 2018-02-23 | ph | -34.640644 | -58.500677 | 128000.0 | 65.0 | 60.0 | NaN | . 1 BARRACAS | 2018-02-28 | departamento | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | . 2 COLEGIALES | 2018-02-07 | departamento | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | . 3 VILLA DEVOTO | 2018-02-20 | casa | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | . 4 VILLA CRESPO | 2018-04-11 | departamento | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | . . Pandas tiene la capacidad de convertir a m√°s de una columna en √≠ndice, lo que recibe el nombre de multi-√≠ndice, en la que cada ex-columna aporta un nivel de indexaci√≥n, estableci√©ndose tambi√©n una jerarqu√≠a de indexaci√≥n. Los valores existentes en las columnas se convierten en etiquetas. . df.set_index([&#39;barrio&#39;,&#39;tipo&#39;], inplace=True) df.head() . fecha lat lon precio superficie_total superficie_cubierta ambientes . barrio tipo . VILLA LURO ph 2018-02-23 | -34.640644 | -58.500677 | 128000.0 | 65.0 | 60.0 | NaN | . BARRACAS departamento 2018-02-28 | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | . COLEGIALES departamento 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | . VILLA DEVOTO casa 2018-02-20 | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | . VILLA CRESPO departamento 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | . En este caso barrio es el nivel 0 de indexaci√≥n, tiene precedencia por sobre tipo, que viene a ser el nivel 1. Vamos a poder referenciar a los niveles por nombre y por posici√≥n. . df.index.names . FrozenList([&#39;barrio&#39;, &#39;tipo&#39;]) . Seleccionar filas utilizando solo el nivel superior es como si trabaj√°semos con un √≠ndice simple, lo que ven√≠amos haciendo. . df.loc[&#39;PALERMO&#39;].head() . fecha lat lon precio superficie_total superficie_cubierta ambientes . tipo . casa 2018-03-30 | -34.578329 | -58.397749 | 3100000.0 | 450.0 | 350.0 | 3.0 | . departamento 2018-04-10 | -34.586787 | -58.416735 | 145000.0 | NaN | NaN | 3.0 | . departamento 2018-04-18 | -34.582436 | -58.414845 | 850000.0 | 270.0 | 240.0 | 6.0 | . departamento 2018-01-26 | -34.578773 | -58.415132 | 330000.0 | 74.0 | 64.0 | 3.0 | . departamento 2018-04-06 | -34.569005 | -58.436241 | 300000.0 | 110.0 | 98.0 | NaN | . Antes de continuar es necesario introducir a las tuplas (tuple), otro tipo de objeto nativo de Python. Son muy parecidas a las listas, se diferencian en algo que no nos importa mucho ahora: las listas son objetos mutables y las tuplas son inmutables. La forma literal de crearlas es utilizando par√©ntesis en vez de corchetes. . tupla = (1, 2, 3) tupla . (1, 2, 3) . Para hacer una selecci√≥n utilizando m√°s niveles del multi-√≠ndice se requiere escribir las etiquetas deseadas dentro de tuplas respetando la jerarqu√≠a de niveles, as√≠ (nivel 0, nivel 1, ...). . filas = (&#39;PALERMO&#39;, &#39;departamento&#39;) df.loc[filas].head() . /home/matias/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:4: PerformanceWarning: indexing past lexsort depth may impact performance. after removing the cwd from sys.path. . fecha lat lon precio superficie_total superficie_cubierta ambientes . barrio tipo . PALERMO departamento 2018-04-10 | -34.586787 | -58.416735 | 145000.0 | NaN | NaN | 3.0 | . departamento 2018-04-18 | -34.582436 | -58.414845 | 850000.0 | 270.0 | 240.0 | 6.0 | . departamento 2018-01-26 | -34.578773 | -58.415132 | 330000.0 | 74.0 | 64.0 | 3.0 | . departamento 2018-04-06 | -34.569005 | -58.436241 | 300000.0 | 110.0 | 98.0 | NaN | . departamento 2018-04-06 | -34.582669 | -58.414019 | 360000.0 | 110.0 | 110.0 | 4.0 | . filas = ([&#39;SAN TELMO&#39;,&#39;CHACARITA&#39;], &#39;departamento&#39;) # al usar multi-√≠ndice es IMPORTANTE especificar tambi√©n las columnas, aunque no queramos filtrarlas df.loc[filas, :].head() . fecha lat lon precio superficie_total superficie_cubierta ambientes . barrio tipo . CHACARITA departamento 2018-02-07 | -34.585998 | -58.454436 | 190000.0 | 73.0 | 70.0 | 4.0 | . departamento 2018-04-08 | -34.583265 | -58.452488 | 118000.0 | 33.0 | 30.0 | 1.0 | . departamento 2018-04-18 | -34.582635 | -58.451826 | 325000.0 | 86.0 | NaN | 3.0 | . SAN TELMO departamento 2018-03-27 | -34.619606 | -58.374275 | 365000.0 | 140.0 | 140.0 | 3.0 | . CHACARITA departamento 2018-03-02 | -34.587261 | -58.452466 | 158000.0 | 62.0 | 60.0 | 2.0 | . Para seleccionar las propiedades en todos los barrios que son departamentos tenemos que pedir todas las etiquetas del nivel 0 (todos los barrios) y la etiqueta del nivel 1 (departamento). . En principio podr√≠amos pensar lo siguiente, ya que : representar√≠a todas las etiquetas de los barrios. . filas = (:, &#39;departamento&#39;) . Sin embargo esta sintaxis no es correcta: el az√∫car sint√°ctica de : solo sirve cuando est√° encerrado entre corchetes [:] y en el caso anterior lo est√° en par√©ntesis. . Recordemos que : es una forma abreviada de tipear slice(None) y que estas dos formas significan lo mismo. . filas = (slice(None), &#39;departamento&#39;) df.loc[filas, :].head() . fecha lat lon precio superficie_total superficie_cubierta ambientes . barrio tipo . BARRACAS departamento 2018-02-28 | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | . COLEGIALES departamento 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | . VILLA CRESPO departamento 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | . VILLA LURO departamento 2018-03-18 | -34.638511 | -58.504406 | 73000.0 | 29.0 | 26.0 | 1.0 | . VILLA URQUIZA departamento 2018-03-27 | -34.577253 | -58.497460 | 150392.0 | 53.0 | 46.0 | 2.0 | . Pandas provee una manera de recuperar el az√∫car sint√°ctica. . filas = pd.IndexSlice[:, &#39;departamento&#39;] df.loc[filas, :].head() . fecha lat lon precio superficie_total superficie_cubierta ambientes . barrio tipo . BARRACAS departamento 2018-02-28 | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | . COLEGIALES departamento 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | . VILLA CRESPO departamento 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | . VILLA LURO departamento 2018-03-18 | -34.638511 | -58.504406 | 73000.0 | 29.0 | 26.0 | 1.0 | . VILLA URQUIZA departamento 2018-03-27 | -34.577253 | -58.497460 | 150392.0 | 53.0 | 46.0 | 2.0 | . La gente suele escribir menos recurriendo a esta artima√±a. . idx = pd.IndexSlice idx[:, &#39;departamento&#39;] . (slice(None, None, None), &#39;departamento&#39;) . Otra opci√≥n que tenemos a mano para seleccionar todos los departamentos, m√°s simple aunque solo sirve para selecci√≥n y no para asignaci√≥n, es el m√©todo cross-section (xs) que selecciona datos seg√∫n valor de un √≠ndice particular. . df.xs(&#39;departamento&#39;, level=&#39;tipo&#39;, drop_level=False).head() . fecha lat lon precio superficie_total superficie_cubierta ambientes . barrio tipo . BARRACAS departamento 2018-02-28 | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | . COLEGIALES departamento 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | . VILLA CRESPO departamento 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | . VILLA LURO departamento 2018-03-18 | -34.638511 | -58.504406 | 73000.0 | 29.0 | 26.0 | 1.0 | . VILLA URQUIZA departamento 2018-03-27 | -34.577253 | -58.497460 | 150392.0 | 53.0 | 46.0 | 2.0 | . No lo hemos mostrado pero el √≠ndice de las columnas tambi√©n puede ser un multi-√≠ndice. . df.reset_index(inplace=True) . Operaciones vectorizadas . Vector y arreglo son dos t√©rminos intercambiables. El t√≠tulo se refiere a operaciones sobre arreglos de NumPy que se realizan elemento a elemento (element-wise) que tambi√©n est√°n Pandas por estar construido sobre NumPy. Adem√°s estas operaciones est√°n preparadas para tratar con valores ausentes. Ninguna de estas dos funcionalidades est√° presente en Python puro. . a = np.array([2, 2, 2]) b = np.array([1, 2, np.nan]) a * b . array([ 2., 4., nan]) . Nota: Los valores ausentes NA no existen nativamente en Python, como s√≠ existen en R. NumPy lo mitiga usando NaN (not a number) para representarlos. NaN es un valor del tipo float. Como los arreglos de NumPy pueden tener un solo dtype, un arreglo de enteros es promovido a un arreglo de flotantes y un arreglo de booleanos es llevado a uno de objetos (object) en presencia de valores ausentes, ya que por ejemplo, no podr√≠amos tener un arreglo con enteros y NaN (float) simult√°neamente. . Los operadores (+, -, ...) terminan invocando a funciones; existen dentro de la sint√°xis del lenguaje por practicidad. Cuando alguno de los operandos es un arreglo de NumPy, se invocan funciones de NumPy correspondientes (np.add, np.substract, ...) en vez de funciones de Python puro. Tambi√©n las podemos llamar directamente. . np.multiply(a, b) . array([ 2., 4., nan]) . Como caracter√≠stica, son funciones universales (ufunc), ya que hacen su trabajo elemento a elemento independientemente de las dimensiones de los argumentos (escalar, columna, tabla). . Pandas a su vez implementa las funciones de NumPy como m√©todos (aunque en la mayor parte del tiempo usamos operadores). . df.precio.multiply(35).head() . 0 4480000.0 1 3010000.0 2 6795250.0 3 8575000.0 4 5001500.0 Name: precio, dtype: float64 . Las operaciones vectorizadas en cualquiera de sus sabores (operadores, funciones, m√©todos) son ideales para trabajar sobre las columnas existentes y crear nuevas. . df[&#39;precio_m2&#39;] = df.precio / df.superficie_total df.head() . barrio tipo fecha lat lon precio superficie_total superficie_cubierta ambientes precio_m2 . 0 VILLA LURO | ph | 2018-02-23 | -34.640644 | -58.500677 | 128000.0 | 65.0 | 60.0 | NaN | 1969.230769 | . 1 BARRACAS | departamento | 2018-02-28 | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | 2529.411765 | . 2 COLEGIALES | departamento | 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | 3347.413793 | . 3 VILLA DEVOTO | casa | 2018-02-20 | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | 2187.500000 | . 4 VILLA CRESPO | departamento | 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | 3040.425532 | . Operadores y funciones comunes . Existen operadores como + que requieren dos operandos, llamados operadores binarios y operadores como - (negaci√≥n) que trabajan con un solo operando, los operadores unarios. . Comparaci√≥n . == np.equal != np.not_equal &lt; np.less &lt;= np.less_equal &gt; np.greater &gt;= np.greater_equal . Matem√°ticas . + np.add - np.subtract - np.negate (unario) * np.multiply / np.divide ** np.pow np.sqrt np.exp np.log . Booleanas . &amp; np.bitwise_and | np.bitwise_or ~ np.bitwise_not (unario) . Flotantes . np.round np.isna np.notna . IMPORTANTE: and y or realizan una sola evaluaci√≥n booleana en el objeto entero, mientras que &amp; y | realizan m√∫ltiples evaluaciones sobre el contenido (elementos de la colecci√≥n) de los objetos. Cuando trabajamos con arreglos, esto √∫ltimo es lo que normalmente queremos. . a = np.array([True, True, False]) b = np.array([True, False, False]) # bitwise and a &amp; b . array([ True, False, False]) . a | b . array([ True, True, False]) . Operaciones con texto . Los strings (str) de Python poseen una gran variedad de m√©todos muy √∫tiles. Pandas implementa estos m√©todos y atributos (junto con otros m√°s) como operaciones vectorizadas que se acceden desde el atributo Series.str y DataFrame.str. . frutas = pd.Series([&#39;manzana&#39;, &#39;banana&#39;]) frutas.str.upper() . 0 MANZANA 1 BANANA dtype: object . Operaciones con objetos temporales . Python viene con objetos del tipo fecha (date), hora (time), fecha y hora (datetime) e intervalos temporales (timedelta). Los m√©todos y atributos de estos objetos aparecen vectorizados bajo el atributo Series.dt y DataFrame.dt. . fechas = pd.Series([&#39;1990-01-01&#39;, &#39;2018-10-31&#39;]) fechas . 0 1990-01-01 1 2018-10-31 dtype: object . fechas = pd.to_datetime(fechas) fechas . 0 1990-01-01 1 2018-10-31 dtype: datetime64[ns] . fechas.dt.year . 0 1990 1 2018 dtype: int64 . Nota: NumPy no implementa operaciones con texto y tiempo pero Pandas s√≠. . Selecci&#243;n usando columnas . En vez de usar el √≠ndice de las filas podemos seleccionar filas usando las columnas. Para ello generamos m√°scaras booleanas usando columnas (que de por s√≠ tienen un largo igual al de la tabla) y operaciones que devuelvan valores booleanos. Las operaciones de comparaci√≥n son bastante √∫tiles en este contexto ya que nos permiten seleccionar filas seg√∫n valores en las columnas. . df[df.precio &lt; 80000].head() . barrio tipo fecha lat lon precio superficie_total superficie_cubierta ambientes precio_m2 . 5 VILLA LURO | departamento | 2018-03-18 | -34.638511 | -58.504406 | 73000.0 | 29.0 | 26.0 | 1.0 | 2517.241379 | . 7 BELGRANO | local | 2018-01-25 | -34.560463 | -58.458433 | 32000.0 | 30.0 | 30.0 | 1.0 | 1066.666667 | . 16 VILLA CRESPO | local | 2018-04-21 | -34.597679 | -58.443019 | 11111.0 | 200.0 | 200.0 | NaN | 55.555000 | . 26 LINIERS | local | 2018-04-19 | -34.638658 | -58.528026 | 19900.0 | 16.0 | 16.0 | NaN | 1243.750000 | . 31 BALVANERA | departamento | 2018-03-25 | -34.612309 | -58.412878 | 54000.0 | 25.0 | 25.0 | 1.0 | 2160.000000 | . IMPORTANTE: Por una cuesti√≥n de precedencia de operadores, de querer combinar m√°scaras usando operaciones bitwise, si las m√°scaras son creadas usando operadores de comparaci√≥n, deben ser encerradas entre par√©ntesis. Esto es porque Python resuelve antes los operadores como &amp; y | que los operadores como &lt; e ==; usamos los par√©ntesis para indicarle a Python el orden de evaluaci√≥n pretendido: primero las comparaciones, luego las operaciones bitwise. Como en matem√°ticas, lo que est√° dentro de par√©ntesis debe resolverse primero (tiene precedencia) sobre lo que est√° afuera. . df[(df.precio &lt; 80000) &amp; (df.ambientes == 3)].head() . barrio tipo fecha lat lon precio superficie_total superficie_cubierta ambientes precio_m2 . 1404 PARQUE PATRICIOS | departamento | 2018-04-28 | -34.629577 | -58.398071 | 78000.0 | 57.0 | 50.0 | 3.0 | 1368.421053 | . 2277 FLORES | departamento | 2018-04-20 | -34.652157 | -58.459236 | 70000.0 | 60.0 | NaN | 3.0 | 1166.666667 | . 2378 VILLA LUGANO | departamento | 2018-03-09 | -34.686170 | -58.466922 | 77000.0 | 70.0 | 62.0 | 3.0 | 1100.000000 | . 3054 BALVANERA | departamento | 2017-12-06 | -34.612801 | -58.406688 | 78000.0 | 44.0 | 44.0 | 3.0 | 1772.727273 | . 3055 VILLA SOLDATI | departamento | 2018-01-08 | -34.668741 | -58.443146 | 52000.0 | 55.0 | 55.0 | 3.0 | 945.454545 | . df[df.precio.lt(80000) &amp; df.ambientes.eq(3)].head() . barrio tipo fecha lat lon precio superficie_total superficie_cubierta ambientes precio_m2 . 1404 PARQUE PATRICIOS | departamento | 2018-04-28 | -34.629577 | -58.398071 | 78000.0 | 57.0 | 50.0 | 3.0 | 1368.421053 | . 2277 FLORES | departamento | 2018-04-20 | -34.652157 | -58.459236 | 70000.0 | 60.0 | NaN | 3.0 | 1166.666667 | . 2378 VILLA LUGANO | departamento | 2018-03-09 | -34.686170 | -58.466922 | 77000.0 | 70.0 | 62.0 | 3.0 | 1100.000000 | . 3054 BALVANERA | departamento | 2017-12-06 | -34.612801 | -58.406688 | 78000.0 | 44.0 | 44.0 | 3.0 | 1772.727273 | . 3055 VILLA SOLDATI | departamento | 2018-01-08 | -34.668741 | -58.443146 | 52000.0 | 55.0 | 55.0 | 3.0 | 945.454545 | . Alineamiento autom&#225;tico . En casos en los que intervengan m√°s de una tabla o columna, como cuando queremos a√±adir una columna nueva a una tabla, podr√≠amos pensar en cuidar aspectos como que la columna tenga el mismo largo que la tabla y que el orden de los elementos en la nueva columna se condiga con el orden de las filas de la tabla. Esto normalmente es as√≠ al trabajar con arreglos de NumPy. . Afortunadamente nada de esto debe preocuparnos al usar Pandas. Los √≠ndices cumplen una funci√≥n de gran utilidad en las operaciones y en las asignaciones, donde les elementos son alianeados seg√∫n sus √≠ndices para que todo salga como lo esperamos. . Operaciones . algunas_frutas = pd.Series([&#39;ü•ù&#39;, &#39;ü••&#39;, &#39;üçá&#39;, &#39;üçå&#39;, &#39;üçé&#39;]) algunas_frutas.index = [&#39;kiwi&#39;, &#39;coco&#39;, &#39;uva&#39;, &#39;banana&#39;, &#39;manzana&#39;] algunas_frutas . kiwi ü•ù coco ü•• uva üçá banana üçå manzana üçé dtype: object . otras_frutas = pd.Series([&#39;üçá&#39;, &#39;üçê&#39;, &#39;üçé&#39;]) otras_frutas.index = [&#39;uva&#39;, &#39;pera&#39;, &#39;manzana&#39;] otras_frutas . uva üçá pera üçê manzana üçé dtype: object . algunas_frutas + otras_frutas . banana NaN coco NaN kiwi NaN manzana üçéüçé pera NaN uva üçáüçá dtype: object . El √≠ndice de la Series resultante tiene todas las etiquetas de ambos operandos, con los valores resultantes de la operaci√≥n. Notar que los elementos no estaban alineados. . Nota: Cuando alguno de los operandos es NaN el resultado de la operaci√≥n es NaN. . Asignaci&#243;n . dg = algunas_frutas.to_frame(name=&#39;algunas&#39;) dg . algunas . kiwi ü•ù | . coco ü•• | . uva üçá | . banana üçå | . manzana üçé | . dg[&#39;otras&#39;] = otras_frutas dg . algunas otras . kiwi ü•ù | NaN | . coco ü•• | NaN | . uva üçá | üçá | . banana üçå | NaN | . manzana üçé | üçé | . Se respeta el √≠ndice de la tabla, es decir no se le agregan las etiquetas que otras_frutas tiene de m√°s. Los valores se asignan alineados y los valores con etiquetas no presentes en otras_frutas quedan ausentes en la nueva columna. . M&#225;scaras . La alineaci√≥n autom√°tica funciona a√∫n en el indexado, siempre y cuando la m√°scara sea una Series y su √≠ndice coincida con el del objeto sobre el cual se indexa, lo que suele ser el caso cuando trabajamos con las columnas del DataFrame. Lo que veremos a continuaci√≥n no funcionar√≠a con una m√°scara que sea un arreglo de NumPy ya que carece de √≠ndice. . m√°scara = df.ambientes.eq(2) m√°scara.head() . 0 False 1 False 2 True 3 True 4 True Name: ambientes, dtype: bool . df[m√°scara].head() . barrio tipo fecha lat lon precio superficie_total superficie_cubierta ambientes precio_m2 . 2 COLEGIALES | departamento | 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | 3347.413793 | . 3 VILLA DEVOTO | casa | 2018-02-20 | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | 2187.500000 | . 4 VILLA CRESPO | departamento | 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | 3040.425532 | . 6 VILLA URQUIZA | departamento | 2018-03-27 | -34.577253 | -58.497460 | 150392.0 | 53.0 | 46.0 | 2.0 | 2837.584906 | . 15 NU√ëEZ | departamento | 2017-11-21 | -34.540443 | -58.473773 | 213500.0 | 48.0 | 44.0 | 2.0 | 4447.916667 | . # (no hace falta entender esta l√≠nea en este momento) al_rev√©s = m√°scara[::-1] # el orden de esta m√°scara es diferente al de la anterior al_rev√©s.head() . 24257 True 24256 False 24255 False 24254 False 24253 False Name: ambientes, dtype: bool . df[al_rev√©s].head() . /home/matias/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index. . barrio tipo fecha lat lon precio superficie_total superficie_cubierta ambientes precio_m2 . 2 COLEGIALES | departamento | 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | 3347.413793 | . 3 VILLA DEVOTO | casa | 2018-02-20 | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | 2187.500000 | . 4 VILLA CRESPO | departamento | 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | 3040.425532 | . 6 VILLA URQUIZA | departamento | 2018-03-27 | -34.577253 | -58.497460 | 150392.0 | 53.0 | 46.0 | 2.0 | 2837.584906 | . 15 NU√ëEZ | departamento | 2017-11-21 | -34.540443 | -58.473773 | 213500.0 | 48.0 | 44.0 | 2.0 | 4447.916667 | . Nota: Que el indexado haga uso de las etiquetas de la m√°scara es la raz√≥n por la cu√°l las m√°scaras funcionan en el indexado por etiquetas (loc) y no en el indexado por posici√≥n (iloc). . Ejemplo integrador . El siguiente ejemplo asigna a aquellos avisos que carecen de superficie cubierta, su superficie total. . df.loc[df.superficie_cubierta.isnull(), &#39;superficie_cubierta&#39;] = df.superficie_total . loc para indexar por filas y columnas; necesitamos indexar por etiquetas para hacer referencia a la columna sobre la cual hacer la asignaci√≥n por su nombre. | Creamos una m√°scara usando la columna superficie_cubierta y la operaci√≥n vectorizada isnull, que servir√° para seleccionar aquellas filas en las que superficie_cubierta es NaN. | Asignamos los valores presentes en la columna superficie_total y no nos preocupamos por hacer una selecci√≥n (como la del punto anterior) sobre estos valores ya que la alineaci√≥n autom√°tica se encargar√° de alinear las filas de ambos lados de la asignaci√≥n por su etiqueta. | Visualizaci&#243;n b&#225;sica . Ver m√°s: http://pandas.pydata.org/pandas-docs/stable/visualization.html . %matplotlib inline . Tanto Series como DataFrames tienen un atributo llamado plot que habilita los siguientes m√©todos para distintos gr√°ficos t√≠picos. . line y area para l√≠neas y l√≠neas s√≥lidas. | bar y barh para barras verticales y horizontales. | pie para tortas. | scatter para puntos. | hist y hexbin para histogramas e histogramas 2D. | density o kde para densidad de probabilidad. | box para boxplot. | . IMPORTANTE: Cuando graficamos en base a un DataFrame, cada columna se interpreta como una variable. . df.precio.plot.hist(bins=1000, xlim=(0,500000)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f825e369fd0&gt; . Actividad de investigaci&#243;n . Consultar la ayuda o la web para saber m√°s de los m√©todos que se encuentran a continuaci√≥n. Por ejemplo: . help(pd.DataFrame.sort_values) . Help on function sort_values in module pandas.core.frame: sort_values(self, by, axis=0, ascending=True, inplace=False, kind=&#39;quicksort&#39;, na_position=&#39;last&#39;, ignore_index=False) Sort by the values along either axis. Parameters - by : str or list of str Name or list of names to sort by. - if `axis` is 0 or `&#39;index&#39;` then `by` may contain index levels and/or column labels. - if `axis` is 1 or `&#39;columns&#39;` then `by` may contain column levels and/or index labels. .. versionchanged:: 0.23.0 Allow specifying index or column level names. axis : {0 or &#39;index&#39;, 1 or &#39;columns&#39;}, default 0 Axis to be sorted. ascending : bool or list of bool, default True Sort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by. inplace : bool, default False If True, perform operation in-place. kind : {&#39;quicksort&#39;, &#39;mergesort&#39;, &#39;heapsort&#39;}, default &#39;quicksort&#39; Choice of sorting algorithm. See also ndarray.np.sort for more information. `mergesort` is the only stable algorithm. For DataFrames, this option is only applied when sorting on a single column or label. na_position : {&#39;first&#39;, &#39;last&#39;}, default &#39;last&#39; Puts NaNs at the beginning if `first`; `last` puts NaNs at the end. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, ‚Ä¶, n - 1. .. versionadded:: 1.0.0 Returns - sorted_obj : DataFrame or None DataFrame with sorted values if inplace=False, None otherwise. Examples -- &gt;&gt;&gt; df = pd.DataFrame({ ... &#39;col1&#39;: [&#39;A&#39;, &#39;A&#39;, &#39;B&#39;, np.nan, &#39;D&#39;, &#39;C&#39;], ... &#39;col2&#39;: [2, 1, 9, 8, 7, 4], ... &#39;col3&#39;: [0, 1, 9, 4, 2, 3], ... }) &gt;&gt;&gt; df col1 col2 col3 0 A 2 0 1 A 1 1 2 B 9 9 3 NaN 8 4 4 D 7 2 5 C 4 3 Sort by col1 &gt;&gt;&gt; df.sort_values(by=[&#39;col1&#39;]) col1 col2 col3 0 A 2 0 1 A 1 1 2 B 9 9 5 C 4 3 4 D 7 2 3 NaN 8 4 Sort by multiple columns &gt;&gt;&gt; df.sort_values(by=[&#39;col1&#39;, &#39;col2&#39;]) col1 col2 col3 1 A 1 1 0 A 2 0 2 B 9 9 5 C 4 3 4 D 7 2 3 NaN 8 4 Sort Descending &gt;&gt;&gt; df.sort_values(by=&#39;col1&#39;, ascending=False) col1 col2 col3 4 D 7 2 5 C 4 3 2 B 9 9 0 A 2 0 1 A 1 1 3 NaN 8 4 Putting NAs first &gt;&gt;&gt; df.sort_values(by=&#39;col1&#39;, ascending=False, na_position=&#39;first&#39;) col1 col2 col3 3 NaN 8 4 4 D 7 2 5 C 4 3 2 B 9 9 0 A 2 0 1 A 1 1 . Ordenar valores . sort_values | sort_index | . Tirar filas/columnas . drop | drop_duplicates | . Renombrar filas/columnas . rename | . Datos ausentes . dropna | fillna | . Recursos adicionales . Documentaci&#243;n . API | . Art&#237;culos . Intro to Pandas | Modern Pandas | . Machetes . DataQuest cheat sheet | DataCamp cheat sheet | Pandas cheat sheet | . Libros . Python for data science | Python for data analysis | .",
            "url": "https://matiasbattocchia.github.io/datitos/Pandas-parte-1.html",
            "relUrl": "/Pandas-parte-1.html",
            "date": " ‚Ä¢ Nov 11, 2018"
        }
        
    
  
    
        ,"post9": {
            "title": "Introducci√≥n a la programaci√≥n",
            "content": "Es un trabajo en progreso y qui√©n sabe cu√°ndo ser√° el d√≠a que lo retome. . Una receta para elaborar cerveza . Ingredientes . Agua purificada. Se usan distintos tipos de agua para diferentes tipos de cerveza. | Cebada malteada, molida. La cebada es un cereal, el cual pasa por un proceso de malteado que extrae su almid√≥n y sus enzimas. | L√∫pulo. Las flores de l√∫pulo son las encargadas de dar el amargor y el aroma caracter√≠sticos de la cerveza. | Levadura para cerveza: Son microorganismos encargados de convertir az√∫cares en alcohol. | . Elaboraci√≥n . Maceraci√≥n: Se mezcla el agua con la cebada durante cierto periodo de tiempo a cierta temperatura, las enzimas atacan al almid√≥n convirti√©ndolo en az√∫cares. Se filtra la mezcla. Producto: mosto. . | Hervido: Se hierve el mosto durante cierto tiempo, esto permite su esterilizaci√≥n. En el hervido tambi√©n se agrega el l√∫pulo, adem√°s de aportar sabor y aroma, frena los procesos enzim√°ticos. Se lo deja enfriar. Producto: mosto amargo. . | Fermentaci√≥n: Se le agrega la levadura al mosto. Este proceso dura cierta cantidad de d√≠as a cierta temperatura. Luego el mosto fermentado es nuevamente filtrado. Producto: cerveza. . | Fuente: http://www.blogdecerveza.com/infografia-cerveza-sajonia-brewing . La receta vista como un programa . En la receta se hace distinci√≥n entre los ingredientes y las instrucciones de elaboracci√≥n. En un programa sucede algo similar, es posible distinguir a los datos (los ingredientes) del c√≥digo (las instrucciones); el c√≥digo es aplicado a los datos para transformarlos y as√≠ obtener nuevos datos (los productos), sobre los cuales se puede seguir aplicando c√≥digo hasta obtener un resultado (o producto) final. . Datos . # cantidades para 15 litros de cerveza artesanal agua = 30 litros de agua mineral malta = 4,5 kilos de harina de cebada malteada (malta) l√∫pulo = 200 gramos de l√∫pulo levadura = 10 gramos de levadura de cerveza . C√≥digo . mosto = macerar(agua, malta) amargo = hervir(mosto, l√∫pulo) cerveza = fermentar(amargo, levadura) . Resalta en este programa la estructura renglonada. Cada rengl√≥n constituye una instrucci√≥n que expresa alguna acci√≥n a realizar. Vemos que un programa est√° conformado por una secuencia de una o m√°s instrucciones. . Todo texto precedido por un numeral (#) es un comentario. Estos son ignorados por el programa, son √∫tiles para dejar anotaciones que faciliten la lectura del mismo. . Tambi√©n es notorio el uso del signo igual (=). Su funci√≥n es asignar el valor de la derecha a la variable de la izquierda en lo que se conoce como instrucci√≥n de asignaci√≥n. Un valor es un dato, como mencionamos, una entidad capaz de ser manipulada por el c√≥digo. A su vez, una variable es un nombre simb√≥lico (tambi√©n conocido como identificador) que est√° asociado a una entidad. . La variable es la manera breve de referirse al valor, como el nombre o el DNI de una persona, que sirve para evitar hacer una descripci√≥n que la identifique cada vez que la queramos mencionar. A diferencia del nombre o el documento de las personas, que est√°n asociados de por vida a una persona particular, una variable puede cambiar de valor mediante posteriores re-asignaciones. . Tipos de datos . Analizaremos con mayor profundidad la secci√≥n ingredientes/datos. Es com√∫n que los programas comiencen con algunas deficiones de variables ya que constituyen la materia prima sobre la cual realizar la ejecuci√≥n del c√≥digo. . Queremos experimentar un poco, de la receta respetaremos el proceso de elaboraci√≥n y variaremos las cantidades y las cualidades de las materias primas para obtener diversas cervezas. Asociaremos cantidades con valores y cualidades con tipos. A continuaci√≥n, una tabla que resume los ingredientes. . VALOR | TIPO -- 30 litros | agua mineral 4,5 kilos | harina de cebada malteada (malta) 200 gramos | l√∫pulo 10 gramos | levadura de cerveza . Podr√≠amos usar menos l√∫pulo para crear una cerveza menos amarga. Quiz√°s queramos usar m√°s malta para una cerveza m√°s alcoh√≥lica. Mientras mantengamos los valores dentro de ciertos rangos y proporciones la receta funcionar√°. Esto es interesante y esencialmente se debe a que los tipos de ingredientes contin√∫an siendo los mismos. Diferente ser√≠a el caso de no contar con levadura de cerveza y querer reemplazarla por levadura de pan; sucede que no sirve para hacer cerveza. . Es as√≠ que los datos se diferencian por sus cualidades y que los programas se escriben teniendo en cuenta ciertos tipos de datos y no otros. Algunos lenguajes de programaci√≥n son escrictos respecto a los tipos ‚Äîsi la receta dice agua mineral entonces s√≠ o s√≠ tiene que ser agua mineral‚Äî y otros, como Python, no lo son: mientras los datos se comporten de manera similar a efectos del programa entonces pueden usarse de manera indistinta. Por ejemplo, en vez de agua mineral podr√≠amos utilizar agua filtrada ya que tambi√©n se puede beber y a efectos de la receta servir√° para la obtenci√≥n de mosto por medio del proceso de maceraci√≥n. . Funciones . Ahora profundizaremos sobre la secci√≥n elaboraci√≥n/c√≥digo. En abstracto se trata de una secuencia de operaciones que resuelve un problema (el de hacer cerveza), lo que se conoce como algoritmo. Concretamente, un algoritmo puede ser expresado de diversas maneras, utilizando un lengaje natural como en la receta, un lenguaje formal como en el programa e incluso gr√°ficamente mediante un diagrama. . Las operaciones toman datos de entrada, internamente realizan tareas, y devuelven un dato de salida. Asociaremos el concepto de operaci√≥n con el de funci√≥n. A continuaci√≥n, una tabla que lista las operaciones del proceso de elaboraci√≥n. . FUNCI√ìN | ENTRADAS | SALIDA -- macerar | agua, malta | mosto hervir | mosto, l√∫pulo | mosto amargo fermentar | mosto amargo, levadura | cerveza . Las funciones son, como los datos, tambi√©n entidades. Como sucede con los datos, tambi√©n se las asocia con un nombre o identificador para poder invocarlas cuando sea necesario. En principio las funciones son operaciones en potencia, son entidades que si bien son capaces de realizar tareas, se encuentran en letargo. Para convertirlas en acto, en la realizaci√≥n de las tareas, las funciones deben ser accionadas, lo que se conoce como llamar a la funci√≥n. Una manera de lograrlo es mediante el operador llamar a funci√≥n (), que al acompa√±ar a una funci√≥n, la activa. Dentro de los par√©ntesis se especifican los valores (datos) de entrada, los llamados argumentos de la funci√≥n. Como resultado la funci√≥n ser√° evaluada‚Äîconvertida en un valor (dato) de salida. . funci√≥n(argumentos...) -&gt; valor macerar(agua, malta) -&gt; mosto . Caracterizan a una funci√≥n los argumentos que requiere, en n√∫mero y en tipo. Por ejemplo macerar requiere dos argumentos, el primero del tipo agua y el segundo del tipo malta. Fallar al proporcionar los valores requeridos por alguna funci√≥n durante la ejecuci√≥n del c√≥digo har√° que se detenga. . Dijimos que las funciones internamente realizan tareas. No siempre estas tareas producir√°n meramente el dato a ser retornado sino que tendr√°n otros efectos como almacenar datos o imprimirlos en pantalla y a veces importan m√°s estos efectos que el valor de salida. . En la receta asignamos los valores retornados a variables as√≠ luego los podemos referenciar. . Ejercicio 1 . Elaborar un glosario con las palabras resaltadas en negrita. . Constantes . Las constantes son como las variables solo que mantienen su valor. Es decir que una vez asignado un valor a un nombre (identificador), a este no se le podr√° reasignar otro valor en el transcurso del programa. . Operadores . Los operadores son como las funciones solo que se escriben diferente para otorgarle al lenguaje naturalidad y expresividad (facilidad para leerlo y escribirlo). Por ejemplo, las operaciones artim√©ticas nos resultar√°n m√°s familiares as√≠ . 1 + 1 -&gt; 2 8 - 1 -&gt; 7 . que en una forma funcional. . sumar(1, 1) -&gt; 2 restar(8, 1) -&gt; 7 . Si bien el orden de escritura es otro, en ambos casos el tipo de dato de los argumentos/operandos tiene que ser compatible con la operaci√≥n y la operaci√≥n retornar√° un valor. . funci√≥n(argumento_1, argumento_2) -&gt; valor operando_1 operador operando_2 -&gt; valor . Literales . Los valores surgen de las operaciones, as√≠ como la flor de l√∫pulo sale de una planta‚Äîviendo a la planta como un proceso que engendra flores. Por otro lado las operaciones requieren valores de entrada, as√≠ como se necesita una semilla para dar origen a una planta. Estamos ante una situaci√≥n del tipo el huevo y la gallina, en la que se requieren operaciones para obtener valores y valores para obtener operaciones. Para salir de este enriedo los lenguajes de programaci√≥n proveen literales para algunos tipos de datos, una manera de escribir valores de manera espont√°nea, sin recurrir a operaciones que les den origen. . Entidades primitivas . Los lenguajes de programaci√≥n proveen elementos b√°sicos con los cuales crear programas. Llamamos primitivas a las entidades preexistentes en el lenguaje. Las llamamos as√≠ para diferenciarlas de las entidades que es posible construir a partir de estas. Por ejemplo, podemos definir nuestras propias funciones, afines al problema a resolver (m√°s adelante veremos c√≥mo). Mientras tanto el lenguaje cuenta con entidades gen√©ricas con las cuales empezar. . Python no es un lenguaje especialmente pensado para elaborar cerveza. Los ingredientes con los que cuenta son otros, esta es una lista no exhaustiva de tipos de datos primitivos, los m√°s frecuentes. Entre par√©ntesis se encuentra el nombre que les da Python. . Nada (NoneType) | Booleanos (bool) | Num√©ricos Enteros (int) | Flotantes (float) | . | Secuencias Texto (str) | Listas (list) | . | Mapeos Diccionarios (dict) | . | . Nada. Este tipo tiene un solo valor (hay una √∫nica entidad con este valor). Nos podemos refereir a esta entidad usando la constante primitiva None. Se usa en varias situaciones para significar la ausencia de un valor. . Booleanos. Este tipo tiene dos valores. Estas entidades representan los valores verdadero y falso. Nos podemos referir a estas entidades mediante las constantes primitivas True y False respectivamente. . Enteros. Este tipo tiene infinitos valores, son los n√∫meros positivos y negativos. Estos valores se pueden referenciar de una manera literal utilizando numerales (0‚Äì9). Python interpretar√° al s√≠mbolo 2 como un entero de valor dos. . Flotantes. Son los n√∫meros reales. Se pueden referenciar literalmente usando numerales y el punto (.) como separador decimal. Python interpretar√° 3.14 como un valor del tipo flotante. Son representaciones aproximadas de los n√∫meros reales, es una consesi√≥n de precisi√≥n por rango, que permite computar n√∫meros reales muy peque√±os y muy grandes eficientemente. . Nota: Las constantes None, True y False son necesarias ya que no existen en nuestro lenguaje natural occidental s√≠mbolos para referirnos a la nada, a lo verdadero y a lo falso de manera literal, en la manera en la que s√≠ existen caracteres num√©ricos para referirnos a los n√∫meros. . Secuencias . Las secuencias son colecciones ordenadas de datos indexadas por n√∫meros no negativos. A diferencia de los tipos de datos vistos hasta el momento, el valor de una colecci√≥n es los datos que contiene. Un caj√≥n de cervezas viene a ser una colecci√≥n de botellas de cerveza; el caj√≥n es muy √∫til para manipular todas esas botellas en conjunto. Por ordenado e indexado nos referimos a que cada lugar del caj√≥n est√° numerado, lo que nos permite colocar o retirar botellas de lugares espec√≠ficos. . La funci√≥n primitiva len devuelve la cantidad de √≠tems de la secuencia‚Äîaplicada a un caj√≥n lleno deber√≠a devolver el n√∫mero 12. . len(secuencia) -&gt; tama√±o de la secuencia len(caj√≥n) -&gt; 12 . El operador √≠ndice [] selecciona una posici√≥n en la secuencia y devuelve su contenido; dentro de los corchetes se especifica la posici√≥n. En Python, el √≠ndice comienza en cero. . secuencia[entero] -&gt; dato caj√≥n[2] -&gt; botella de cerveza del lugar n√∫mero 3 . Texto. Normalmente reciben el nombre de strings (cuerdas), son colecciones de caracteres. Los valores del tipo string se pueden generar de manera literal al escribirlos entre comillas simples (&#39;) o dobles (&quot;). Por ejemplo el literal &quot;cerveza&quot; se diferencia del identificador cerveza, el int√©rprete de Python [revisar: no se ha explicado qu√© es el int√©rprete] al leer el literal lo transformar√° en un dato del tipo texto cuyo valor es cerveza, en cambio cuando si lee el identificador ‚Äîla palabra sin comillas‚Äî que podr√≠a tratarse del nombre de una variable o una funci√≥n, invocar√° a la entidad asociada. . Listas. Son colecciones arbitrarias de datos. Una lista puede contener todo tipo de datos, incluso otras listas. Se pueden crear de manera literal utilizando corchetes ([ ]) como delimitadores y coma (,) para separar los elementos de la lista. . n√∫meros = [1, 2, 3] n√∫meros[0] -&gt; 1 . Nota: Puede resultar ambig√ºa la utilizaci√≥n de corchetes, ya que sirven tanto para la creaci√≥n de listas como para el operador √≠ndice. Sin embargo se utilizan de manera diferente, lo que rompe la ambig√ºedad. . Mapeos . Los mapeos son colecciones de datos indexadas por valores arbitrarios. Un ejemplo es una agenda de contactos telef√≥nicos; se trata de una colecci√≥n de n√∫meros de tel√©fono, en la que cada n√∫mero est√° relacionado ‚Äîmapeado‚Äî a un nombre de contacto. Relaciona claves (nombres de contacto) con valores (n√∫meros de tel√©fono). Las secuencias y los mapeos son parecidos, ambos son colecciones, se diferencian en la forma de indexar los datos, es decir, en la forma de recuperarlos: mediante la posici√≥n en el caso de las secuencias, por medio de una clave en el de los mapeos. En la agenda, para encontrar un n√∫mero de tel√©fono (el valor), lo buscamos por el nombre del contacto (la clave). . Como sucede con las secuencias, la funci√≥n len devuelve la cantidad de elementos del mapeo y el operador √≠ndice [] selecciona un elemento del mapeo y devuelve su valor; dentro de los corchetes se especifica la clave. . Diccionarios. Es el mapeo paradigm√°tico. Se pueden crear de manera literal utilizando llaves ({ }) como delimitadores, coma (,) para separar los elementos del diccionario, lo que nos recuerda un poco a las listas, solo que los elementos del diccionario son pares clave-valor, utiliz√°ndose dos puntos (:) para separar la clave del valor. . proveedores = {&#39;a&#39;:1, &#39;b&#39;:2, &#39;c&#39;:3} proveedores[&#39;b&#39;] -&gt; 2 . En este ejemplo se usaron strings para las claves y enteros para los valores. Los tipos de datos que pueden cumplir estas funciones no est√°n limitados a estos, los valores pueden ser de cualquier tipo, incluyendo listas y otros diccionarios; las claves est√°n m√°s acotadas, normalmente son strings, podr√≠an ser de otros pero no listas ni diccionarios. .",
            "url": "https://matiasbattocchia.github.io/datitos/Introducci%C3%B3n-a-la-programaci%C3%B3n.html",
            "relUrl": "/Introducci%C3%B3n-a-la-programaci%C3%B3n.html",
            "date": " ‚Ä¢ Mar 8, 2018"
        }
        
    
  
    
        ,"post10": {
            "title": "Un recorrido por scikit-learn",
            "content": "Prepar√© unas clases introductorias a la librer√≠a de aprendizaje autom√°tico scikit-learn. Las notas de las clases, que aqu√≠ comparto, si bien no llegan a conformar un tutorial, perfilan como una gu√≠a para recorrer la documentaci√≥n de la librer√≠a en gran parte de su extensi√≥n. . En esencia reorganic√© las gu√≠as de usuario de la librer√≠a en torno al flujo de trabajo del desarrollo de modelos. Adicionalmente, como en la vida real los datos de los que uno suele disponer est√°n lejos de ser perfectos, a√±ad√≠ algunas t√©cnicas para trabajar con conjuntos de datos desbalanceados. . Recorrido por las posibilidades de la librer√≠a (gu√≠as de usuario) | Familizarizaci√≥n con la documentaci√≥n (API) | Tarea: implementaci√≥n de un flujo de trabajo sencillo para regresi√≥n (parecido al tutorial b√°sico) | . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sn . Requisitos . Algo de familiaridad con NumPy. Ver este tutorial. . Utilidad . Aprendizaje supervisado Clasificaci√≥n | Regresi√≥n | . | Aprendizaje no supervisado | Aprendizaje por refuerzos | . Redes neuronales: solo perceptr√≥n multicapa y restricted Boltzmann machine. . Datos . scikit-learn consume datos con forma de matriz o arreglo bidimensional, de dimensi√≥n (n_muestras, n_atributos) ‚Äî es como imaginamos normalmente a los datos, dispuestos en una tabla donde las columnas son los atributos y hay tantas muestras como filas. . Convencionalmente en la documentaci√≥n la varible X se utiliza para los atributos propiamente dichos, y la variable y para los objetivos. Cuando el objetivo es uno solo, y suele tomar la forma de arreglo unidimensional de dimensi√≥n (n_muestras,). . Objetos . En scikit-learn hay dos tipos fundamentales de objetos: . Los transformadores, que implementan los m√©todos . fit(X, y) y | transform(X), | . | y los estimadores, que implementan . fit(X, y), | predict(X). | . | . Flujo de trabajo . Datos . Vamos a usar el conjunto de datos de plantas de iris para los ejemplos. Este dataset es una especie de hola mundo del aprendizaje de m√°quinas. . . Cantidad de instancias: 150 . Atributos (4) . 1. Largo del s√©palo [cm] 2. Ancho del s√©palo [cm] 3. Largo del p√©talo [cm] 4. Ancho del p√©talo [cm] . Objetivos (1) . 5. Clase - Setosa - Versicolour - Virginica . Valores ausentes: No . from sklearn.datasets import load_iris iris = load_iris() X, y = iris.data, iris.target print(&#39;X: datos &#39;, X.shape) print(&#39;y: objetivo&#39;, y.shape) . X: datos (150, 4) y: objetivo (150,) . Limpieza de datos . https://en.wikipedia.org/wiki/Data_cleansing . Cardinalidad | Rango | Desviaci√≥n | Tipo Booleano | Num√©ro (separadores) | Texto espacios (trimming) | tildes | casos (may√∫sculas, min√∫sculas) | . | . | Codificaci√≥n (UTF-8, etc√©tera) | . Partici&#243;n del conjunto de datos . Entrenamiento (50%) | Validaci√≥n (25%) ‚Äî salvo cross-validation o ausencia de hiperpar√°metros. | Prueba (25%) | . Ejemplo: train_test_split . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) . Transformadores: preprocesamiento de atributos . Extracci&#243;n . http://scikit-learn.org/stable/modules/feature_extraction.html . Im√°genes | Texto | . Transformaci&#243;n . http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling . Estandarizaci√≥n (StandardScaler) ‚Äî A cada atributo le remueve su valor medio y lo escala dividi√©ndolo por su desviaci√≥n est√°ndar. Centrar los datos es pr√°cticamente obligatorio (hay excepciones). | Normalizar los datos solo si los atributos difieren en unidades y/u √≥rdenes de magnitud. | . | Reajuste Rango (MinMaxScaler) | Valor absoluto (MaxAbsScaler) | . | . Ejemplo: StandardScaler . from sklearn.preprocessing import StandardScaler std = StandardScaler(with_mean=True, with_std=True) std.fit(X_train) X_train_std = std.transform(X_train) . . http://scikit-learn.org/stable/modules/preprocessing.html#normalization . Normalizaci√≥n (Normalizer) ‚Äî Divide vectores por su norma (afecta filas en vez de columnas). | . Imputaci&#243;n de valores ausentes . http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values . Descarte (tirar la muestra) | Valor m√°s com√∫n | Valor medio | Valor mediano | Estimaci√≥n (clasificaci√≥n/regresi√≥n) | Hot-deck (el valor de la muestra m√°s parecida) | Valor ausente (NA) como otro valor | . Creaci&#243;n . http://scikit-learn.org/stable/modules/preprocessing.html#generating-polynomial-features . De $(X_1, X_2)$ a $(1, X_1, X_2, X_1^2, X_1X_2, X_2^2)$. . Reducci&#243;n de dimensionalidad . http://scikit-learn.org/stable/modules/unsupervised_reduction.html . PCA ‚Äî an√°lisis de componentes principales. Estandarizar los datos antes de usar PCA. | Casi todos los estimadores no supervisados implementan el m√©todo transform(X). | Algunos estimadores supervisados tambi√©n. | . Ejemplo: PCA . from sklearn.decomposition import PCA pca = PCA(n_components=2, whiten=False) pca.fit(X_train_std) X_train_pca = pca.transform(X_train_std) . . Selecci&#243;n &#8212; solo aprendizaje supervisado . http://scikit-learn.org/stable/modules/feature_selection.html . Umbral de varianza | An√°lisis univariado | Usando un estimador | Eliminaci√≥n recursiva (tambi√©n existe la agregaci√≥n recursiva) | . Ejemplo: SelectKBest . from sklearn.feature_selection import SelectKBest kbest = SelectKBest(k=1) kbest.fit(X_train_std, y_train) X_train_kbest = kbest.transform(X_train_std) . Estimadores: selecci&#243;n de modelos . . ¬øQu√© estimador usar para el conjunto de datos de plantas de iris? . ¬øM√°s de 50 muestras? S√≠, el conjunto de datos tiene 150 (en realidad un poco menos porque hemos separado un conjunto de prueba). | ¬øHay que predecir una categor√≠a? S√≠, queremos predecir a qu√© especie pertenece cada planta. | ¬øLos datos est√°s anotados? S√≠, est√°n anotados en tres categor√≠as. | ¬øM√°s de 100,000 muestras? No... | Nos recomiendan usar una m√°quina de vectores de soporte (support vector machine) con un kernel lineal. . Ejemplo: SVC . from sklearn.svm import SVC estimador = SVC(kernel=&#39;linear&#39;, C=1, probability=True) estimador.fit(X_train_std, y_train) X_test_std = std.transform(X_test) y_pred = estimador.predict(X_test_std) . predict_proba(X) . http://scikit-learn.org/stable/modules/calibration.html . Al realizar la clasificaci√≥n, a menudo se desea no solo predecir la etiqueta de la clase, sino tambi√©n obtener una probabilidad de la etiqueta. Esta probabilidad da alg√∫n tipo de confianza en la predicci√≥n. Algunos modelos pueden darle estimaciones pobres de las probabilidades de la clase y algunos incluso no admiten la predicci√≥n de probabilidad. El m√≥dulo de calibraci√≥n le permite calibrar mejor las probabilidades de un modelo determinado o agregar soporte para la predicci√≥n de probabilidad. . estimador.predict_proba(X_test[:3]).round() . array([[ 0., 0., 1.], [ 0., 0., 1.], [ 0., 1., 0.]]) . Cantidad de objetivos &#8212; solo aprendizaje supervisado . http://scikit-learn.org/stable/modules/multiclass.html . Clasificador Binario | Multi Clase | Etiqueta | Clase-etiqueta | . | . | Regresor Univariado | Multivariado | . | . En scikit-learn todos los clasificadores aceptan varias clases. Algunos estimadores trabajan inherentemente con m√∫ltiples objetivos y le sacan provecho a la correlaci√≥n entre los mismos. Cuando no es el caso del estimador, existen diferentes estrategias para que admita m√∫ltiples objetivos: . OVO (uno-contra-uno), | OVA (uno-contra-todos). | . Ensambles &#8212; solo aprendizaje supervisado . http://scikit-learn.org/stable/modules/ensemble.html . Diferentes tipos de ensambles: . Promediadores: estimadores en paralelo, reducen la varianza. | Propulsores (boosting): estimadores en serie, reducen el sesgo. | . Evaluaci&#243;n del modelo . http://scikit-learn.org/stable/modules/model_evaluation.html . Cada estimador implementa un m√©todo llamado score(X, y) que devuelve un puntaje del desempe√±o del estimador. El puntaje es calculado usando una m√©trica acorde a la naturaleza del estimador, por ejemplo los regresores suelen reportar R¬≤ mientas que los clasificadores, efectividad. . estimador.score(X_test_std, y_test) . 0.97368421052631582 . Efectividad . from sklearn.metrics import accuracy_score accuracy_score(y_test, y_pred) . 0.97368421052631582 . Matriz de confusi&#243;n . https://en.wikipedia.org/wiki/Confusion_matrix . from sklearn.metrics import confusion_matrix confusion_matrix(y_test, y_pred) . array([[13, 0, 0], [ 0, 15, 1], [ 0, 0, 9]]) . Reporte . from sklearn.metrics import classification_report clases = [&#39;Setosa&#39;, &#39;Versicolour&#39;, &#39;Virginica&#39;] print(classification_report(y_test, y_pred, target_names=clases)) . precision recall f1-score support Setosa 1.00 1.00 1.00 13 Versicolour 1.00 0.94 0.97 16 Virginica 0.90 1.00 0.95 9 avg / total 0.98 0.97 0.97 38 . F1 . √ötil para conjuntos de datos desbalanceados. . $F_1 = 2 cdot frac{ mathrm{precision} cdot mathrm{recall}}{ mathrm{precision} + mathrm{recall}}$ . from sklearn.metrics import f1_score f1_score(y_test, y_pred, average=&#39;weighted&#39;) . 0.97395228308462156 . Kappa de Cohen . https://en.wikipedia.org/wiki/Cohen&#39;s_kappa . √ötil para conjuntos de datos desbalanceados. . $ kappa = frac{p_o - p_e}{1 - p_e}$ . from sklearn.metrics import cohen_kappa_score cohen_kappa_score(y_test, y_pred) . 0.95978835978835975 . Flujo de datos . Pipeline . http://scikit-learn.org/stable/modules/pipeline.html . Todos los objetos del flujo, excepto el √∫ltimo, deben ser muestreadores/transformadores (deben implementar el m√©todo sample/transform). El √∫ltimo objeto puede ser de cualquier tipo, suele ser un estimador. . Prestar atenci√≥n a ColumnTransformer que permite aplicar transformadores a DataFrames de Pandas. . from sklearn.pipeline import make_pipeline flujo = make_pipeline(StandardScaler(), SVC()) flujo.fit(X_train, y_train) flujo.score(X_test, y_test) . 0.97368421052631582 . Optimizaci&#243;n: validaci&#243;n cruzada . http://scikit-learn.org/stable/modules/cross_validation.html . El conjunto de datos de validaci√≥n sirve para ajustar a los hiperpar√°metros de los objetos que componen el flujo de trabajo, tanto como para la composici√≥n del flujo en s√≠ mismo. La validaci√≥n cruzada es √∫til cuando el conjunto de validaci√≥n es necesario y las muestras son escasas. . Se necesitan dos cosas: . Una estrategia de particionamiento de los datos. | Una m√©trica de evaluaci√≥n. | Estrategias . K-fold, stratified k-fold ‚Äî estrategias por defecto para regresores y clasificadores respectivamente. | Leave one out (LOO) | Leave P out (LPO) | Shuffle &amp; split, stratified shuffle &amp; split | . . M&#233;tricas . De no especificarse ninguna, se usa el m√©todo score(X, y) del estimador. | Las m√©tricas m√°s comunes se pueden pasar como argumento (string), ver tabla. | Se pueden armar puntuadores a partir de cualquier m√©trica, tanto de la API como definidas por el usuario, y pasar como argumento (funci√≥n). | . http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html . from sklearn.model_selection import cross_val_score resultados = cross_val_score(flujo, X_train, y_train, cv=5, scoring=&#39;f1_weighted&#39;) print(&#39;F1 promedio: %0.2f (+/- %0.2f)&#39; % (resultados.mean(), resultados.std() * 2)) . F1 promedio: 0.96 (+/- 0.08) . Optimizaci&#243;n: b&#250;squeda . http://scikit-learn.org/stable/modules/grid_search.html . from sklearn.model_selection import GridSearchCV hiperpar√°metros = { &#39;svc__kernel&#39;:(&#39;linear&#39;, &#39;rbf&#39;), &#39;svc__C&#39;:[1, 10] } grilla = GridSearchCV(flujo, hiperpar√°metros) grilla.fit(X_train, y_train) estimador = grilla.best_estimator_ . Evaluaci&#243;n final . Una vez elegido el modelo y ajustados sus hiperpar√°metros, si se desea los conjuntos de datos de entrenamiento y de validaci√≥n pueden fusionarse en un nuevo conjunto de entrenamiento para reentrenar el modelo final usando m√°s datos ‚Äî de hecho es lo que hace GridSearchCV para el mejor estimador. . En cambio por m√°s seguridad que se tenga del desempe√±o del modelo, no es recomendable usar los datos del conjunto de prueba, es mejor usarlos para medir su desempe√±o y asegurarnos de que est√© libre de errores. . y_pred = estimador.predict(X_test) f1_score(y_test, y_pred, average=&#39;weighted&#39;) . Persistencia del modelo . http://scikit-learn.org/stable/modules/model_persistence.html . import pickle # persistencia with open(&#39;modelo.pickle&#39;, &#39;wb&#39;) as archivo: pickle.dump(estimador, archivo) # carga with open(&#39;modelo.pickle&#39;, &#39;rb&#39;) as archivo: estimador = pickle.load(archivo) . Extra: Muestreo &#8212; conjunto desbalanceado . El entrenamiento y la validaci√≥n de estimadores suele requerir conjuntos de datos balanceados; no as√≠ la prueba de los mismos ya que deben enfrentar datos reales del problema (desbalanceados). . scikit-learn apenas provee algoritmos de muestro, podemos usar la extensi√≥n imbalanced-learn que implementa varios. . pip install imbalanced-learn . imbalanced-learn aporta objetos del tipo muestreador (sampler) que implementan los m√©todos . fit(X, y) y | sample(X, y). | . Algunos algoritmos: . Under-sampling ClusterCentroids | RandomUnderSampler | . | Over-sampling SMOTE | RandomOverSampler | . | . Nota: imbalanced-learn re-implementa la clase Pipeline para que admita muestreadores. . Ejemplo: RandomUnderSampler . from sklearn.datasets import make_classification from imblearn.under_sampling import RandomUnderSampler # generaci√≥n de un conjunto de datos X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=200, random_state=10) # aplicaci√≥n de random under-sampling rus = RandomUnderSampler() X_resampled, y_resampled = rus.fit_sample(X, y) . Ejercicio: Boston price data set . Cantidad de instancias: 506 . Atributos (13) . 1. CRIM per capita crime rate by town 2. ZN proportion of residential land zoned for lots over 25,000 sq.ft. 3. INDUS proportion of non-retail business acres per town 4. CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) 5. NOX nitric oxides concentration (parts per 10 million) 6. RM average number of rooms per dwelling 7. AGE proportion of owner-occupied units built prior to 1940 8. DIS weighted distances to five Boston employment centres 9. RAD index of accessibility to radial highways 10. TAX full-value property-tax rate per 10,000 USD 11. PTRATIO pupil-teacher ratio by town 12. B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town 13. LSTAT % lower status of the population . Objetivos (1) . 14. MEDV Median value of owner-occupied homes in 1000‚Äôs USD . Valores ausentes: No . from sklearn.datasets import load_boston boston = load_boston() X, y = boston.data, boston.target print(&#39;X: datos &#39;, X.shape) print(&#39;y: objetivo&#39;, y.shape) . X: datos (506, 13) y: objetivo (506,) . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) . Sugerencias: . 1.1 Modelo lineal 1.1.1. Ordinary least squares | 1.1.2. Ridge regression | 1.1.3. Lasso | . | . . from sklearn.metrics import r2_score r2_score(y_test, y_pred) # mientras m√°s cerca de 1.0, mejor . 0.89914051165600339 . Automatizaci&#243;n . TPOT es una herramienta de aprendizaje autom√°tico automatizado que optimiza el flujo de trabajo. . pip install tpot . from tpot import TPOTRegressor tpot = TPOTRegressor(generations=3, population_size=20, verbosity=2) tpot.fit(X_train, y_train) y_pred = tpot.predict(X_test) . Warning: xgboost.XGBRegressor is not available and will not be used by TPOT. . Optimization Progress: 50%|‚ñà‚ñà‚ñà‚ñà‚ñà | 40/80 [00:11&lt;00:09, 4.27pipeline/s] . Generation 1 - Current best internal CV score: 14.469198135864161 . Optimization Progress: 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 60/80 [00:28&lt;00:10, 1.84pipeline/s] . Generation 2 - Current best internal CV score: 14.43020757845812 . . Generation 3 - Current best internal CV score: 14.358659442160791 Best pipeline: RandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=DEFAULT, RandomForestRegressor__min_samples_leaf=DEFAULT, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=DEFAULT) . . from sklearn.metrics import r2_score r2_score(y_test, y_pred) # mientras m√°s cerca de 1.0, mejor . 0.87143869370430582 . tpot.export(&#39;tpot_boston_pipeline.py&#39;) .",
            "url": "https://matiasbattocchia.github.io/datitos/Recorrido-por-scikit-learn.html",
            "relUrl": "/Recorrido-por-scikit-learn.html",
            "date": " ‚Ä¢ Aug 4, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Acerca de",
          "content": "Mi nombre es Mat√≠as Battocchia. Estudi√© en Exactas (UBA). Trabajo en Vindow.com. Vivo en Mendoza, Argentina. . Los temas que m√°s sigo son procesamiento del lenguaje natural (NLP) y redes neuronales de grafos (GNN). . Tambi√©n soy organizador de encuentros de ciencia de datos. . Contactame por Telegram. . Cr√©ditos . El sitio est√° hecho con fastpages. | Los emojis vienen de aqu√≠. | Logo y favicon vienen de ac√°. | .",
          "url": "https://matiasbattocchia.github.io/datitos/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://matiasbattocchia.github.io/datitos/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}