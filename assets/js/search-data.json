{
  
    
        "post0": {
            "title": "Ejemplo de cómo usar Ignite",
            "content": "Recientemente han salido varias librerías de alto nivel para PyTorch. Una vez planteados los datasets y los modelos todavía queda bastante por programar, sobre todo los bucles de entrenamiento y validación, código que salvando detalles de implementación es siempre el mismo (boilerplate). Librerías como PyTorch Lighting y PyTorch Ignite prometen ahorrarnos el código repetitivo y concentrarnos en lo particular. . Se instala con . pip install pytorch-ignite . import torch from sklearn.metrics import balanced_accuracy_score from ignite.engine import Engine, Events from ignite.metrics import Accuracy, Loss . if torch.cuda.is_available(): device = torch.device(&#39;cuda&#39;) else: device = torch.device(&#39;cpu&#39;) . Definimos el triángulo modelo-optimizador-criterio. Lo de siempre. . modelo = Modelo().to(device) optimizador = torch.optim.Adam(modelo.parameters(), lr=1e-3, weight_decay=1e-5) criterio = torch.nn.CrossEntropyLoss() . Bucle de entrenamiento. . def entrenar(motor, lote): modelo.train() optimizador.zero_grad() predicciones = modelo(lote.documentos.to(device)) pérdida = criterio(predicciones, lote.etiquetas.to(device)) pérdida.backward() optimizador.step() return pérdida.item() entrenador = Engine(entrenar) . Bucle de evaluación. Sirve para todo lo que no es entrenamiento, a ser validación e inferencia. . def evaluar(motor, lote): modelo.eval() with torch.no_grad(): predicciones = modelo(lote.documentos.to(device)) # validación if lote.etiquetas is not None: return predicciones, lote.etiquetas.to(device) # inferencia return predicciones evaluador = Engine(evaluar) inferidor = Engine(evaluar) . Adjuntamos algunas métricas al evaluador. Notar que entrenar devuelve el valor de la función de pérdida mientras que evaluar devuelve predicciones. . Accuracy().attach(evaluador, &#39;accuracy&#39;) Loss(criterio).attach(evaluador, &#39;loss&#39;) . Evento: al completar una época de entrenamiento. | Acción: registrar métricas del dataset de entrenamiento. Para ello usamos evaluador.run(train_dl). | . @entrenador.on(Events.EPOCH_COMPLETED) def loguear_resultados_entrenamiento(entrenador): evaluador.run(train_dl) # accedemos a este atributo gracias a haber adjuntado métricas previamente métricas = evaluador.state.metrics print(f&quot;[{entrenador.state.epoch:02}] TRAIN Accuracy: {métricas[&#39;accuracy&#39;]:.2f} Loss: {métricas[&#39;loss&#39;]:.2f}&quot;) . Evento: al completar una época de entrenamiento. Acá podría ser al completar X épocas para no validar tan seguido. | Acción: registrar métricas del dataset de validación. Para ello usamos evaluador.run(valid_dl). | . Esta función podría hacer sido más parecida a la de arriba pero no lo es porque queremos calcular una métrica que no viene con Ignite. Tenemos dos opciones, definir una métrica adjuntable como ignite.metrics.Accuracy —que no lo hicimos y quizás hubiese sido lo mejor— o poner la lógica en la función, como vemos aquí. . @entrenador.on(Events.EPOCH_COMPLETED) def loguear_resultados_validación(entrenador): # esta artimaña tendrá sentido más adelante evaluador.predicciones = [] evaluador.etiquetas = [] evaluador.run(valid_dl) # de todas las categorías nos quedamos con la más probable para cada muestra predicciones = torch.cat(evaluador.predicciones).argmax(dim=1).cpu() etiquetas = torch.cat(evaluador.etiquetas).cpu() score = balanced_accuracy_score(etiquetas, predicciones) métricas = evaluador.state.metrics print(f&quot;[{entrenador.state.epoch:02}] VALID Accuracy: {métricas[&#39;accuracy&#39;]:.2f} Loss: {métricas[&#39;loss&#39;]:.2f} Balanced accuracy: {score:.2f}&quot;) . Evento: al procesar un lote de validación. | Acción: almacenar predicciones y etiquetas. | . Esto le da sentido a la artimaña que mencionamos. La misma sirve para instanciar listas vacías al inicio de la validación, a las que se le agregaran los resultados de cada lote. . @evaluador.on(Events.ITERATION_COMPLETED) def colectar_validaciones_lote(evaluador): evaluador.predicciones.append(evaluador.state.output[0]) evaluador.etiquetas.append(evaluador.state.output[1]) . Evento: al completar el entrenamiento (todas las épocas). | Acción: realizar inferencias. Para ello usamos inferidor.run(infer_dl). | . @entrenador.on(Events.COMPLETED) def colectar_inferencias(entrenador): print(&#39;Realizando inferencias...&#39;) # mismo truco de antes inferidor.y_pred = [] inferidor.run(infer_dl) # de todas las categorías nos quedamos con la más probable para cada muestra y_pred = torch.cat(inferidor.y_pred).argmax(dim=1).reshape(-1,1) # quizás sea un buen momento para recuperar las categorías originales #y_pred = vocabulario_etiquetas.índices_a_tókenes(y_pred) # ya que estamos, guardamos los resultados en un CSV pd.DataFrame(y_pred).to_csv(&#39;submit.csv&#39;, header=False) . Evento: al procesar un lote de inferencia. | Acción: almacenar predicciones. | . @inferidor.on(Events.ITERATION_COMPLETED) def colectar_inferencias_lote(inferidor): inferidor.y_pred.append(inferidor.state.output) . Finalmente largamos el entrenamiento con entrenador.run(train_dl). Este es el engranaje principal del mecanismo, que al completar bucles moverá a los otros engranajes (Engines, que no son literalmente enganajes pero puede que sea una buena metáfora). . entrenador.run(train_dl, max_epochs=5) . [01] VALID Accuracy: 0.80 Loss: 1.11 Balanced accuracy: 0.75 [01] TRAIN Accuracy: 0.98 Loss: 0.02 [02] VALID Accuracy: 0.80 Loss: 1.11 Balanced accuracy: 0.75 [02] TRAIN Accuracy: 0.98 Loss: 0.02 [03] VALID Accuracy: 0.80 Loss: 1.11 Balanced accuracy: 0.75 [03] TRAIN Accuracy: 0.98 Loss: 0.02 [04] VALID Accuracy: 0.80 Loss: 1.11 Balanced accuracy: 0.75 [04] TRAIN Accuracy: 0.98 Loss: 0.02 [05] VALID Accuracy: 0.80 Loss: 1.11 Balanced accuracy: 0.75 [05] TRAIN Accuracy: 0.98 Loss: 0.02 Realizando inferencias... . Recursos . 8 Creators and Core Contributors Talk About Their Model Training Libraries From PyTorch Ecosystem | .",
            "url": "https://matiasbattocchia.github.io/datitos/PyTorch-Ignite.html",
            "relUrl": "/PyTorch-Ignite.html",
            "date": " • Aug 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Preprocesamiento de texto para NLP (parte 2)",
            "content": "En la primera parte llegamos a convertir esto . [ &#39;que se requiere para un prestamo personal&#39;, &#39;me piden mi numero de cuenta es mi cbu&#39;, ] . en esto . [ [4160, 4683, 4484, 3703, 5294, 4011, 3825], [3275, 3854, 3319, 3554, 1532, 1462, 2151, 3319, 950], ] . donde dijimos que las partes fundamentales son la tokenización —separar a los documentos en unidades de información— y la numericalización —el asignarle a cada uno de los tókenes un número, más que nada para que la computadora, que gusta mucho de los números, sea feliz—. . También habíamos dicho que un tóken es un atributo pero no dijimos mucho más al respecto. Veamos cómo puede ser esto. La tarea de ejemplo es clasificar documentos. Estamos acustumbrados a tener muestras y etiquetas como X e y en las que la primera es una matriz de muestras (filas) y atributos (columnas), y la segunda suele ser una columna. Cuando el dataset está sin pre-procesar tenemos las muestras (filas) pero no los atributos (columnas), por lo general tenemos una única columna con los documentos en forma de strings, lo que mucha forma de atributos no tiene. . Ahora que hemos pre-procesado el texto estamos a un paso de obtener los atributos. La función de los atributos es describir o caracterizar a las muestras. El modelo lee estos atributos para realizar inferencias. Hay distintas maneras de describir a los documentos, algunas más sofisticadas que otras, una intuitiva es aprovechar que los tókenes están numerados desde 0 hasta L (len(vocabulario)) y otorgarle una columna a cada uno en la matriz de atributos de tamaño N x L (donde N es la cantidad de muestras). . Hecho esto, solo resta contar cuántas veces aparece cada tóken en cada documento y asentarlo en la matriz. . | bien hola si todo - &#39;hola todo bien&#39; | 1 1 0 1 &#39;si bien bien&#39; | 2 0 1 0 . Como comentario, esta forma de describir los documentos ignora enteramente el órden de los tókenes, sabemos que el sentido de una oración puede cambir completamente si cambiamos algunas palabras de lugar. Para el problema en cuestión, no parece ser tan grave ya que para clasificar una pregunta podría bastar con reconocer algunas palabras claves como cambio y clave o requisito y préstamo. . Ver tf-idf. . PyTorch . El típico bucle de entrenamiento de PyTorch tiene esta pinta. . for época in range(N_ÉPOCAS): for lote in datos_entrenamiento: # reseteamos los gradientes optimizador.zero_grad() predicciones = red_neuronal(lote.X) pérdida = criterio(predicciones, lote.y) # calculamos los gradientes pérdida.backward() # aplicamos los gradientes optimizador.step() . Recordemos que a diferencia de otros modelos las redes neuronales revisitan varias veces el dataset, en lo que se llaman épocas, cada época es un recorrido por todas las muestras de entrenamiento. . En una época el dataset se puede mostrar entero, de a una muestra, o como es común hoy en día de a grupos o lotes (batches). La experiencia mostró que es útil variar el orden de las muestras en cada época. . PyTorch provee ciertas facilidades para el manejo de los datos con las clases definidas en torch.utils.data a ser: . Dataset. Organiza los datos. Le pasamos un número o índice de muestra y nos devuelve la muestra usualmente como una tupla (atributos, etiqueta). | Sampler. Salvo que lo queramos de otra manera, se encarga de brindar un orden aleatoreo de los índices del dataset; uno diferente cada vez que le preguntamos. | BatchSampler. Por defecto, se inicializa con un Sampler y el tamaño de lote. Se encarga de armar grupos de índices; diferentes cada vez que le preguntamos. | DataLoader. Valiéndose de los grupos de índices de BatchSampler, obtiene muestras de Dataset. De esta manera para cada época devuelve lotes de muestras al azar. | Por Sampler y BatchSampler no nos detendremos ya el comportamiento por defecto, que es barajar el dataset en cada época y armar lotes del mismo tamaño es todo lo que necesitamos. . Dataset . from torch.utils.data import Dataset class Textset(Dataset): def __init__(self, documentos, etiquetas=None): self.documentos = documentos self.etiquetas = etiquetas or np.full(len(documentos), np.nan) def __len__(self): return len(self.documentos) def __getitem__(self, item): return self.documentos[item], self.etiquetas[item] . Es una clase que necesita implementar __len__ y __getitem__. Podría encargarse de levantar y pre-procesar el dataset, que por comodidad lo hemos cargado con Pandas y pre-procesado por fuera: el constructor (__init__) podría recibir el nombre del archivo, leerlo y aplicarle las funciones pertinentes. No lo hemos hecho internamente porque el vocabulario debe nutrirse del dataset de entrenamiento ya pre-procesado [falta]. . También necesitaremos crear un Textset para el dataset de inferencia, para el cual no contamos con las etiquetas. En el caso de no pasar etiquetas generamos una lista llena de NaNs del mismo largo que la lista de documentos. . train_ds = Textset(train_índices, etiquetas_train_índices) len(train_ds) . 18093 . train_ds[10_000] . ([8, 169, 1, 4652, 0, 17, 65], 40) . Es bastante similar a lo que una lista de tuplas podría lograr, aunque fue una buena oportunidad para juntar los documentos y las etiquetas que luego de cargar el DataFrame y hasta ahora recorrieron caminos separados. Lo realmente importante es el DataLoader, no podemos usar una lista como dataset porque requiere que sea una instancia de Dataset. . DataLoader . DataLoader es un iterable. Los iterables son colecciones de elementos que se pueden recorrer; implementan el método __iter__, del que se espera que devuelva un objeto iterador (iterador = iter(iterable)). A su vez el iterador implementa el método __next__ que se encarga devolver secuencialmente los elementos de la colección hasta que se agota; una vez que esto sucede el iterador debe ser descartado y en todo caso le pedimos al iterable que nos arme un nuevo iterador. Cuando usamos la construcción for ítem in iterable, el intérprete de Python implícitamente obtiene un iterador. . Ver la sección de interables en el tutorial de Python. . lista = iter([&#39;uno&#39;,&#39;dos&#39;]) next(lista) . &#39;uno&#39; . next(lista) . &#39;dos&#39; . next(lista) . StopIteration Traceback (most recent call last) &lt;ipython-input-203-cfa830c9416d&gt; in &lt;module&gt; -&gt; 1 next(lista) StopIteration: . No hay próximo elemento. Cuando se llega al fin del iterador se levanta la excepción StopIteration. . Suficientes detalles por ahora. Todo esto para decir que DataLoader es un iterable que particularmente devuelve un iterador distinto cada vez, a diferencia de una lista en la que los elementos siempre se recorren en el mismo orden. Es decir, se trata de una colección de lotes pero cada iterador agrupa lotes según como dicte BatchSampler, que suele ser aleatorio. . En cada época le pedimos un iterador a DataLoader, por lo que recorremos todo el Dataset agrupado en lotes de manera diferente cada vez. . from torch.utils.data import DataLoader train_dl = DataLoader(train_ds, batch_size=32, shuffle=True) . Le estamos diciendo a DataLoader que queremos lotes de 32 muestras (batch_size) y que el armado de los lotes sea aleatorio (shuffle). . un_lote = next(iter(train_dl)) un_lote . [[tensor([ 12, 5168, 26, 9, 16, 8, 24, 10, 8, 15, 49, 49, 46, 16, 15, 8, 1, 62, 26, 12, 8, 7, 12, 157, 44, 2082, 5, 62, 1, 76, 8, 74]), tensor([ 140, 10, 75, 4, 4, 22, 84, 1519, 48, 75, 27, 357, 105, 40, 48, 1911, 213, 585, 14, 48, 19, 203, 102, 164, 57, 0, 2, 22, 1009, 262, 274, 630]), tensor([ 17, 51, 371, 1058, 64, 4, 71, 27, 17, 9, 1, 2, 59, 57, 2, 6, 0, 928, 62, 6, 1973, 3, 17, 713, 10, 36, 56, 4, 18, 21, 1, 5]), tensor([ 56, 67, 83, 0, 22, 724, 18, 24, 3, 765, 64, 207, 1454, 2, 765, 36, 179, 2, 358, 13, 38, 236, 56, 3, 3, 0, 5, 32, 1, 98, 1009, 6])], tensor([144, 153, 247, 3, 55, 0, 223, 15, 18, 6, 26, 160, 89, 199, 149, 49, 260, 285, 13, 3, 198, 18, 23, 0, 1, 103, 35, 112, 128, 20, 128, 3])] . Está bueno que ya veamos tensores de PyTorch porque vamos a necesitar los datos en forma de tensor para alimentar a la red neuronal. Sin embargo, algo no parece andar bien con el lote que acabamos de obtener. . len(un_lote) . 2 . Tenemos dos elementos adentro del lote, podríamos pensar que el primero agrupa documentos y el segundo, etiquetas. . type(un_lote[1]), len(un_lote[1]) . (torch.Tensor, 32) . Las etiquetas del lote están perfecto, son un tensor de una dimensión con largo 32. . type(un_lote[0]), len(un_lote[0]) . (list, 4) . En cambio la agrupación de documentos no tiene sentido. Es otra lista de tamaño 4 con tensores adentro. ¿Qué está pasando? . Tensores . El problema parece radicar en los tensores. Son estructuras que las podemos imaginar como una columna cuando tienen una dimensión, una tabla cuando son dos, un cubo cuando tres... . Los tensores son similares a los ndarrays de NumPy, con el aditivo que también pueden ser usados en la GPU para acelerar los cómputos. Ver más de tensores en el tutorial de PyTorch. . En el caso de los documentos que a la altura del Dataset son listas de listas de índices, son dos dimensiones, y al llevarlos a una tabla vemos que tendríamos tantas filas como documentos y tantas columnas como índices tenga el documento más largo de la colección pero que no todos los documentos tienen tantos índices como columnas la tabla. . índices = [ [2,2], [4,4,4,4], [7,7,7,7,7,7,7], ] índices . [[2, 2], [4, 4, 4, 4], [7, 7, 7, 7, 7, 7, 7]] . import torch torch.tensor(índices) . ValueError Traceback (most recent call last) &lt;ipython-input-232-121200966211&gt; in &lt;module&gt; 1 import torch 2 -&gt; 3 torch.tensor(índices) ValueError: expected sequence of length 2 at dim 1 (got 4) . Como anticipamos, no le gustó nada. . T&#243;kenes especiales . Lo mencionamos al pasar, a veces se utilizan tókenes especiales como &lt;separador de palabra&gt;, &lt;separador de oración&gt;, &lt;inicio del texto&gt;, &lt;fin del texto&gt;. Hay de todo tipo, según la tarea a realizar. Uno que está presente generalmente en los proyectos es el tóken de relleno &lt;relleno&gt; (en inglés padding). . El tóken de relleno nos va a servir para hacer que todos los documentos tengan el mismo largo y finalmente podamos convertirlos en un tensor. No lo vamos a hacer inmediatamente ya que no nos interesa que tengan el mismo largo en todo el dataset sino en todo el lote. Como los lotes son generados en el DataLoader, este último tendrá que encargarse de rellenar los documentos. . Vamos a modificar Vocab quien se encarga de la lista de tókenes para que incluya a &lt;relleno&gt;. . # versión 4 import numpy as np from itertools import chain from collections import Counter class Vocab(): @property def índice_relleno(self): return self.mapeo.get(self.tóken_relleno) def __init__(self, tóken_desconocido=&#39;&lt;unk&gt;&#39;, tóken_relleno=&#39;&lt;pad&gt;&#39;, frecuencia_mínima=0.0, frecuencia_máxima=1.0, longitud_mínima=1, longitud_máxima=np.inf, stop_words=[], límite_vocabulario=None): self.tóken_desconocido = tóken_desconocido self.tóken_relleno = tóken_relleno self.frecuencia_mínima = frecuencia_mínima self.frecuencia_máxima = frecuencia_máxima self.longitud_mínima = longitud_mínima self.longitud_máxima = longitud_máxima self.stop_words = stop_words self.límite_vocabulario = límite_vocabulario # ningún cambio aquí def reducir_vocabulario(self, lote): contador_absoluto = Counter(chain(*lote)) contador_documentos = Counter() for doc in lote: contador_documentos.update(set(doc)) # frecuencia mínima if isinstance(self.frecuencia_mínima, int): # frecuencia de tóken vocabulario_mín = [tóken for tóken, frecuencia in contador_absoluto.most_common() if frecuencia &gt;= self.frecuencia_mínima] else: # frecuencia de documento vocabulario_mín = [tóken for tóken, frecuencia in contador_documentos.most_common() if frecuencia/len(lote) &gt;= self.frecuencia_mínima] # frecuencia máxima if isinstance(self.frecuencia_máxima, int): # frecuencia de tóken vocabulario_máx = [tóken for tóken, frecuencia in contador_absoluto.most_common() if self.frecuencia_máxima &gt;= frecuencia] else: # frecuencia de documento vocabulario_máx = [tóken for tóken, frecuencia in contador_documentos.most_common() if self.frecuencia_máxima &gt;= frecuencia/len(lote)] # intersección de vocabulario_mín y vocabulario_máx preservando el órden vocabulario = [tóken for tóken in vocabulario_mín if tóken in vocabulario_máx] # longitud vocabulario = [tóken for tóken in vocabulario if self.longitud_máxima &gt;= len(tóken) &gt;= self.longitud_mínima] # stop words vocabulario = [tóken for tóken in vocabulario if tóken not in self.stop_words] # límite vocabulario = vocabulario[:self.límite_vocabulario] return vocabulario def fit(self, lote): vocabulario = self.reducir_vocabulario(lote) if self.tóken_desconocido: vocabulario.append(self.tóken_desconocido) if self.tóken_relleno: vocabulario.insert(0, self.tóken_relleno) self.mapeo = {tóken: índice for índice, tóken in enumerate(vocabulario)} return self # ningún cambio aquí def transform(self, lote): if self.tóken_desconocido: # reemplazar return [[tóken if tóken in self.mapeo else self.tóken_desconocido for tóken in doc] for doc in lote] else: # ignorar return [[tóken for tóken in doc if tóken in self.mapeo] for doc in lote] # ningún cambio aquí def tókenes_a_índices(self, lote): lote = self.transform(lote) return [[self.mapeo[tóken] for tóken in doc] for doc in lote] # ningún cambio aquí def índices_a_tókenes(self, lote): mapeo_inverso = list(self.mapeo.keys()) return [[mapeo_inverso[índice] for índice in doc] for doc in lote] def __len__(self): return len(self.mapeo) . El índice del tóken de relleno suele ser 0 y para continuar con esta tradición en vez de hacerle append al vocabulario le hicimos un prepend para que el tóken encabece el listado. Además usamos el decorador @property para tener un atributo índice_relleno (en vez de un método) que nos devuelva el índice del tóken. . v = Vocab().fit(train_docs) v.índice_relleno . 0 . La funci&#243;n que rellena . def rellenar_documentos(lote, largos, índice_relleno): máximo_largo = max(largos) return [doc + [índice_relleno] * (máximo_largo - largos[i]) for i, doc in enumerate(lote)] . Le tenemos que pasar el lote, el largo o tamaño de cada documento del lote y el índice de relleno. . índices = [ [2,2], [4,4,4,4], [7,7,7,7,7,7,7], ] largos = [2,4,7] rellenos = rellenar_documentos(índices, largos, v.índice_relleno) rellenos . [[2, 2, 0, 0, 0, 0, 0], [4, 4, 4, 4, 0, 0, 0], [7, 7, 7, 7, 7, 7, 7]] . torch.tensor(rellenos) . tensor([[2, 2, 0, 0, 0, 0, 0], [4, 4, 4, 4, 0, 0, 0], [7, 7, 7, 7, 7, 7, 7]]) . ¡Ahora sí funcionó! . Tama&#241;o del documento . Vamos a incluir el tamaño del documento (en cantidad de tókenes/índices) junto a cada ítem del dataset ya que nos va a hacer falta para la función que rellena. . from torch.utils.data import Dataset class Textset(Dataset): def __init__(self, documentos, etiquetas=None): self.documentos = documentos self.etiquetas = etiquetas or np.full(len(documentos), np.nan) def __len__(self): return len(self.documentos) def __getitem__(self, item): return self.documentos[item], len(self.documentos[item]), self.etiquetas[item] . train_ds = Textset(train_índices, etiquetas_train_índices) train_ds[10_000] . ([8, 169, 1, 4652, 0, 17, 65], 7, 40) . Bonus: AtributoDiccionario . ¿Alguna vez quisiste acceder a los elementos de un diccionario como si fuesen atributos de un objeto? Es decir así . d = {&#39;uno&#39;:1, &#39;dos&#39;:2, &#39;tres&#39;:3} d.uno # =&gt; 1 . en vez de así . d[&#39;uno&#39;] # =&gt; 1 . Con esta magia ahora es posible: . # https://stackoverflow.com/questions/4984647/accessing-dict-keys-like-an-attribute class AtriDicc(): def __init__(self, *args, **kwargs): self.__dict__ = dict(*args, **kwargs) def __repr__(self): return repr(self.__dict__) . AtriDicc(uno=1, dos=2, tres=3).uno . 1 . Vamos a pimpiar la clase Textset con esto para que en vez de devolver elementos del dataset como tuplas (documento, largo, etiqueta) en el que debemos acordarnos que el orden de los elementos, devolvemos un AtriDicc en el que accedemos las cosas por su nombre y es más cómodo que un diccionario. . from torch.utils.data import Dataset class Textset(Dataset): def __init__(self, documentos, etiquetas=None): self.documentos = documentos self.etiquetas = etiquetas or np.full(len(documentos), np.nan) def __len__(self): return len(self.documentos) def __getitem__(self, item): return AtriDicc( documento = self.documentos[item], largo = len(self.documentos[item]), etiqueta = self.etiquetas[item], ) . train_ds = Textset(train_índices, etiquetas_train_índices) train_ds[10_000].documento . [8, 169, 1, 4652, 0, 17, 65] . Funci&#243;n collate . Collate significa juntar diferentes piezas de información para ver sus similaridades y diferencias, también puede ser colectar y organizar las hojas de un reporte, un libro. En el contexto de DataLoader quiere decir arreglar el lote. Entonces esta función recibe una lista de elementos del Dataset, en nuestro caso una lista de de AtriDiccs, y debe devolver el lote en una forma útil y en lo posible realizar conversiones a tensores. . DataLoader posee una collate function por defecto que utiliza internamente y que en muchos casos funciona correctamente, pero otros como ahora que tenemos documentos de distinto largo nos toca definir una función propia. . from torch.nn.utils.rnn import pack_padded_sequence def rellenar_lote(lote): &quot;&quot;&quot;Prepara lotes para ingresar a nn.Embedding&quot;&quot;&quot; documentos = [elemento.documento for elemento in lote] largos = [elemento.largo for elemento in lote] etiquetas = [elemento.etiqueta for elemento in lote] rellenos = rellenar_documentos(documentos, largos, v.índice_relleno) # para RNNs descomentar esta línea #rellenos = pack_padded_sequence(rellenos, largos, batch_first=True, enforce_sorted=False) return AtriDicc( documentos = torch.tensor(rellenos), etiquetas = torch.tensor(etiquetas), ) . Cuando instanciamos un DataLoader le pasamos la función que acabamos de definir. . train_dl = DataLoader(train_ds, collate_fn=rellenar_lote, batch_size=3, shuffle=True) . un_lote = next(iter(train_dl)) un_lote.documentos . tensor([[781, 31, 17, 104, 111, 9, 383, 93, 18, 11, 489, 0, 0], [ 20, 4, 11, 7, 272, 78, 29, 96, 5, 396, 16, 86, 16], [ 26, 69, 17, 313, 4, 258, 22, 4, 102, 0, 0, 0, 0]]) . un_lote.etiquetas . tensor([ 80, 316, 16]) . Funciona de maravillas. . Una funci&#243;n alternativa . La función anterior es compatible con el módulo de PyTorch nn.Embedding que suele se la puerta de entrada en los modelos de procesamiento de texto. Todavía no hemos hablado nada de los embeddings. Quizás sea un momento para mencionar a nn.EmbeddingBag, que tiene requerimientos completamente diferentes al primer módulo. . def offsetear_lote(lote): &quot;&quot;&quot;Prepara lotes para ingresar a nn.EmbeddingBag&quot;&quot;&quot; documentos = [torch.tensor(elemento.documento) for elemento in lote] offsets = [0] + [elemento.largo for elemento in lote][:-1] etiquetas = [elemento.etiqueta for elemento in lote] return AtriDicc( documentos = torch.cat(documentos), offsets = torch.tensor(offsets).cumsum(dim=0), etiquetas = torch.tensor(etiquetas), ) . Esta función yuxtapone los documentos por un lado, y por otro (offsets) indica cuándo comienza cada documento en ese continuo. . train_dl = DataLoader(train_ds, collate_fn=offsetear_lote, batch_size=3, shuffle=True) . un_lote = next(iter(train_dl)) un_lote.documentos . tensor([ 35, 14, 8, 544, 46, 6, 2493, 30, 384, 2, 1062, 27, 236, 5, 778, 378, 22, 4, 53, 1, 866, 9, 17, 1564, 109, 68, 186, 16, 6, 1419]) . un_lote.offsets . tensor([ 0, 9, 16]) . Avanzado: Memory pinning . https://pytorch.org/docs/stable/data.html#memory-pinning . Esta técnica consiste en pre-disponibilizar los tensores en el GPU. Llevar un lote del disco o de la memoria a la GPU insume tiempo y puede causar un cuello de botella durante el entrenamiento. . Pasar la opción pin_memory=True al DataLoader pondrá automáticamente a los tensores en la pinned memory. Por defecto funciona con tensores y colecciones de tensores. Cuando los lotes son de un tipo personalizado (por ejemplo AtriDicc), normal cuando se utiliza una collate function propia, es necesario que el tipo defina el método pin_memory. . class AttrDict(): def __init__(self, *args, **kwargs): self.__dict__ = dict(*args, **kwargs) def __repr__(self): return repr(self.__dict__) def pin_memory(self): for atributo, valor in self.__dict__.items(): self.__dict__[atributo] = valor.pin_memory() if hasattr(valor, &#39;pin_memory&#39;) else valor return self . El pre-procesamiento hasta ahora . vocabulario_documentos = Vocab().fit(train_docs) train_índices = vocabulario_documentos.tókenes_a_índices(train_docs) valid_índices = vocabulario_documentos.tókenes_a_índices(valid_docs) infer_índices = vocabulario_documentos.tókenes_a_índices(infer_docs) . train_ds = Textset(train_índices) valid_ds = Textset(valid_índices) infer_ds = Textset(infer_índices) . Solo definimos el modelo, no lo entrenamos. Elegimos la función offsetear_lote ya que el modelo usa nn.EmbeddingBag. . from torch.utils.data import DataLoader train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=offsetear_lote, pin_memory=True) # validación e inferencia no requieren `shuffle` valid_dl = DataLoader(valid_ds, batch_size=32, shuffle=False, collate_fn=offsetear_lote, pin_memory=True) infer_dl = DataLoader(infer_ds, batch_size=32, shuffle=False, collate_fn=offsetear_lote, pin_memory=True) . Hacemos unas definiciones necesarias. No es el punto de lo que queremos mostrar, lo podés pasar por alto. . import torch.nn as nn import torch.nn.functional as F DIM_EMBEDDINGS = 8 class ClasificadorBolsa(nn.Module): def __init__(self, vocab_size, embed_dim, num_class): super().__init__() self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False, mode=&#39;max&#39;) self.fc = nn.Linear(embed_dim, num_class) def forward(self, text, offsets): embedded = self.embedding(text, offsets) return self.fc(embedded) modelo = ClasificadorBolsa( len(vocabulario_documentos), DIM_EMBEDDINGS, len(vocabulario_etiquetas) ).to(device) . Ídem. . if torch.cuda.is_available(): device = torch.device(&#39;cuda&#39;) else: device = torch.device(&#39;cpu&#39;) . Nuevamente. . optimizador = torch.optim.Adam(modelo.parameters(), lr=1e-3, weight_decay=1e-5) criterio = nn.CrossEntropyLoss() . Alto aquí. Así es como se usa un DataLoader. . ÉPOCAS = 10 for época in range(ÉPOCAS): for lote in train_dl: optimizador.zero_grad() predicciones = modelo(lote.documentos.to(device), lote.offsets.to(device)) pérdida = criterio(predicciones, lote.etiquetas.to(device)) pérdida.backward() optimizador.step() . Con esto concluye la segunda parte. Quedaron los embeddings para la tercera. Ahora deberíamos tener más control sobre la carga de datos en PyTorch. Muchos ejemplos de uso y tutoriales dan por sentada esta parte al utilizar datasets de ejemplos, que ya vienen pre-procesados y/o que la carga por defecto de PyTorch maneja sin inconvenientes. .",
            "url": "https://matiasbattocchia.github.io/datitos/Preprocesamiento-de-texto-para-NLP-parte-2.html",
            "relUrl": "/Preprocesamiento-de-texto-para-NLP-parte-2.html",
            "date": " • Jul 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Preprocesamiento de texto para NLP (parte 1)",
            "content": "Vamos a hacer un recorrido por los pasos básicos del pre-procesamiento de texto. Estos pasos son necesarios para transformar texto del lenguaje humano a un formato legible para máquinas para su posterior procesamiento, particularmente motiva esta publicación el procesamiento en PyTorch. . Veremos cómo realizar estos pasos con código propio, para mayor entendimiento de lo que está sucediendo, y con spaCy, una herramienta de nuestro agrado. . En concreto, los pasos son: . Limpieza, la remoción del contenido no deseado. | Normalización, la conversión diferentes formas a una sola. | Tokenización, la separación del texto en tókenes (unidades mínimas, por ejemplo palabras). | Separación en conjuntos de datos: entrenamiento, validación, prueba. | Generación del vocabulario, la lista de tókenes conocidos. | Numericalización, el mapeo de tókenes a números enteros. | Estos pasos son comunes distintas aproximaciones al procesamiento del lenguaje. En la parte 2 mostraremos pasos útiles para abarcarlo usando deep learning. . Loteo, la generación de porciones de muestras de entrenamiento. | Relleno, la conversión del lote en un tensor de PyTorch. | Carga de embeddings, opcionalmente el uso de embeddings precalculados. | Nota: El órden de los primeros tres pasos (limpieza, normalización, tokenización) puede variar según conveniencia. El resto de los pasos mantiene el órden. . Dataset de ejemplo . ¿Qué sería de esta publicación sin algunos ejemplos? Vamos a usar el dataset de la competencia clasificación de preguntas de clientes de Meta:Data. . import pandas as pd df = pd.read_csv(&#39;train.csv&#39;, sep=&#39;|&#39;) . with pd.option_context(&#39;display.max_colwidth&#39;, -1): display(df.sample(10)) . Pregunta Intencion . 9221 hice una compra y no tengo las acciones acreditadas | Cat_153 | . 1298 quiero saber si yo puedo solicitar un préstamo | Cat_248 | . 4488 perdi la credencial universitaria. como solicito una nueva? | Cat_294 | . 28 cambiar moneda tarjeta debitar exterior | Cat_289 | . 19970 quiero adherir al debito de la tarjeta el servicio epec. me pide que ingrese numero cuenta de digitos pero en la factura no aparece un numero | Cat_129 | . 3510 llegar tarjeta recargable solicití¬≠ | Cat_293 | . 14373 buenas tardes | Cat_19 | . 5485 saber de cuanto es el pago mínimo de tarjeta visa | Cat_351 | . 6752 verificar reclamo | Cat_135 | . 10645 que sucede cuando se me vence el plazo fijo? | Cat_180 | . Expresiones regulares . Si las expresiones regulares no te resultan familiares entonces vale la pena estudiarlas brevemente, ya que las usaremos. Podés mirar este tutorial que encontramos en la web. . import re . Limpieza . Muchas técnicas modernas no realizan limpieza alguna. Dependiendo de lo que queramos hacer tal vez convenga deshacernos de algunos elementos. En el dataset de ejemplo los signos de puntuación no parecen tener gran relevancia, quizás tampoco la tengan los números (que aparentemente han sido removidos de antemano). . def limpiar(texto): puntuación = r&#39;[,;.:¡!¿?@#$%&amp;[ ](){}&lt;&gt;~=+ -*/| _^`&quot; &#39;]&#39; # signos de puntuación texto = re.sub(puntuación, &#39; &#39;, texto) # dígitos [0-9] texto = re.sub(&#39; d&#39;, &#39; &#39;, texto) return texto . En esta función substituimos los signos de puntuación . , ; . : ¡ ! ¿ ? @ # $ % &amp; [ ] ( ) { } &lt; &gt; ~ = + - * / | _ ^ ` &quot; &#39; . por espacios (me gusta más; usar string vacío &#39;&#39; para eliminarlos) medieante expresiones regulares (algunos caracteres tuvieron que ser escapados anteponiendo por tener un significado especial para la expresión regular). Hacemos lo mismo con los dígitos. Veamos un ejemplo de funcionamiento. . limpiar(&#39;hoy 13 trabajan?&#39;) . &#39;hoy trabajan &#39; . Otros elementos que podríamos pensar en remover son caracteres invisibles, espacios redundantes. Veremos que esto en particular también puede ser resulto en la tokenización. . Normalizaci&#243;n . Normalizar es la tarea de llevar lo que puede ser expresado de múltiples maneras como fechas, números y abreviaturas a una única forma. Por ejemplo . 13/03/30 -&gt; trece de marzo de dos mil treinta DC -&gt; departamento de computación . Se trata de una práctica clásica de la época de los modelos de lenguaje probabilísticos, que intentaban reducir lo más posible la cantidad de palabras. En cierta forma 1 palabra = 1 atributo (lo que en los &#39;90s conocimos como convertibilidad). Elegir atributos es ingeniería de atributos, la parte central del machine learning, y lo justamente lo que el deep learning busca automatizar. . Sin embargo hay una normalización muy común hoy, el convertir todo el texto a minúsculas. En el caso del español, una normalización común es la remoción de tildes. . def normalizar(texto): # todo a minúsculas texto = texto.lower() # tildes y diacríticas texto = re.sub(&#39;á&#39;, &#39;a&#39;, texto) texto = re.sub(&#39;é&#39;, &#39;e&#39;, texto) texto = re.sub(&#39;í&#39;, &#39;i&#39;, texto) texto = re.sub(&#39;ó&#39;, &#39;o&#39;, texto) texto = re.sub(&#39;ú&#39;, &#39;u&#39;, texto) texto = re.sub(&#39;ü&#39;, &#39;u&#39;, texto) texto = re.sub(&#39;ñ&#39;, &#39;n&#39;, texto) return texto . normalizar(&#39;Me podrán dar información de un préstamo personal&#39;) . &#39;me podran dar informacion de un prestamo personal&#39; . Hay una librería llamada unidecode que realiza transliteración: representa letras o palabras de un alfabeto en otro, útil si tenemos caracteres en ruso (cirílico) o chino (caracteres Han), aún útil para el alfabeto latino cuando queremos pasar de Unicode a ASCII (lo que substituiría las tildes). . #!pip install unidecode from unidecode import unidecode unidecode(&#39;Me podrán dar información de un préstamo personal&#39;) . &#39;Me podran dar informacion de un prestamo personal&#39; . Una normalización que vale la pena intentar con este dataset es la correción ortográfica con un paquete como pyspellchecker. Quizás con artículos de diarios en los que la redacción está más cuidada esto no valga la pena, pero en contextos más informales como este, conversaciones por char, Twitter, las palabras mal escritas en realidad refieren a una sola palabra y no a distintos significados. . Tokenizaci&#243;n . Tokenizar es separar el texto en partes más pequeñas llamadas tókenes. Una unidad muy común es la palabras pero depende de lo que queramos hacer, si es que no hemos eliminado a los signos de puntuación estos también serían tókenes. Las palabras frecuentemente están compuestas por una raíz, prefijo y/o sufijo, por lo que podríamos decidir separarlos también. En inglés es común separar it&#39;s en it y &#39;s, si bien en español esta situación no es común. . A diferencia de la limpieza y la normalización, la tokenización es un paso indispesable en la preparación de texto para su procesamiento. . Para el dataset en cuestión la tokenización es simple, vamos a separar seǵun espacios y demás caracteres invisibles como t (tabulación) y n (salto de línea). De haber signos de puntuación, pro ejemplo si quisiéramos procesar un documento extenso en oraciones, el proceso es más complejo ya que final. tiene un punto en vez de un espacio, y no siempre los puntos demarcan el final de un tóken como en A.M. y P.M.. . Debemos definir si elementos como los signos de puntuación son tókenes o si simplemente delimitan palabras o tókenes, en cuyo caso desaparecerían en el proceso. Mismo con los caracteres invisibles, si estuviésemos haciendo un modelo que programe en Python, la indentación es fundamental y deberiera mantenerse. . def tokenizar(texto): # IMPORTANTE: podría devolver una lista vacía return [tóken for tóken in texto.split()] . split también se encarga de los caracteres invisibles repetidos. . tokenizar(&#39;hola vengo a flotar&#39;) . [&#39;hola&#39;, &#39;vengo&#39;, &#39;a&#39;, &#39;flotar&#39;] . Acá estamos cambiando el tipo de datos, ya que de un string hemos pasado a una lista de strings. . Si la expresión dentro de la función no te resulta familiar, es una construcción llamada list comprehension y es una manera muy efectiva de armar una lista. Es lo mismo que hacer . lista = [] for i in range(10): lista.append(i) lista . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . pero de una manera más expresiva y también más eficiente (está optimizado por el lenguaje) . [i for i in range(10)] . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . Varios modelos de lenguaje utilizan caracteres en vez de palabras como tókenes, esto es útil por varios motivos que listaremos más adelante. Otros utilizan partes de palabras como sílabas (las partes se determinan estadísticamente). Ver https://arxiv.org/pdf/1508.07909.pdf. . Tokenizaci&#243;n utilizando alguna librer&#237;a . #!pip install spacy #!python -m spacy download es_core_news_sm import spacy nlp = spacy.load(&#39;es_core_news_sm&#39;) doc = nlp(&#39;Esto es una frase.&#39;) print([tóken.text for tóken in doc]) . [&#39;Esto&#39;, &#39;es&#39;, &#39;una&#39;, &#39;frase&#39;, &#39;.&#39;] . Otros pre-procesos . Clásicamente se aplicaban alguno de estos para reducir aún más la cantidad de palabras: . Stemming . Stem, de raíz, reduce la inflección de las palabras, mapeando un grupo de palabras a la misma raíz, sin importar si la raíz es una palabras válida en el lenguaje. . caminando, caminar, camino -&gt; camin . Lemmatization . A diferencia del stemming, la lematización reduce las palabras inflexadas a palabras que pertenecen al lenguaje. La raíz pasa a llamarse lema. . Primera parte del pre-procesamiento . def preprocesar(texto): texto = limpiar(texto) texto = normalizar(texto) texto = tokenizar(texto) return texto . Conjuntos de datos . En la competencias normalmente encontramos dos archivos, el de entrenamiento y el de inferencia —que le suelen llamar de prueba y es el que tenemos que predecir para entregar—. Del que suelen llamar train también tenemos que obtener el de validación. . infer_df = pd.read_csv(&#39;test.csv&#39;, sep=&#39;,&#39;) . from sklearn.model_selection import train_test_split train_df, valid_df = train_test_split(df, test_size=.1, random_state=42) . . Ahora estamos en condiciones de pre-procesar todo lo que tenemos: . train_docs = [preprocesar(doc) for doc in train_df[&#39;Pregunta&#39;].values] valid_docs = [preprocesar(doc) for doc in valid_df[&#39;Pregunta&#39;].values] infer_docs = [preprocesar(doc) for doc in infer_df[&#39;Pregunta&#39;].values] . Hemos pasado de una Series de Pandas, array de NumPy o una lista de strings . train_df[&#39;Pregunta&#39;].values[:4] . array([&#39;que se requiere para un préstamo personal?&#39;, &#39;me piden mi número de cuenta es mi cbu?&#39;, &#39;necesitar adherir aysa tarjeta&#39;, &#39;te financian igual un usado o un 0km?&#39;], dtype=object) . a una lista de listas de strings . train_docs[:4] . [[&#39;que&#39;, &#39;se&#39;, &#39;requiere&#39;, &#39;para&#39;, &#39;un&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;me&#39;, &#39;piden&#39;, &#39;mi&#39;, &#39;numero&#39;, &#39;de&#39;, &#39;cuenta&#39;, &#39;es&#39;, &#39;mi&#39;, &#39;cbu&#39;], [&#39;necesitar&#39;, &#39;adherir&#39;, &#39;aysa&#39;, &#39;tarjeta&#39;], [&#39;te&#39;, &#39;financian&#39;, &#39;igual&#39;, &#39;un&#39;, &#39;usado&#39;, &#39;o&#39;, &#39;un&#39;, &#39;km&#39;]] . Un poco de nomenclatura: estamos llamando corpus a la colección de textos. Nos referimos también a los textos como documentos. También estamos usando el término lote (batch) para referirnos a un (sub)conjunto de documentos. . Vocabulario . Este paso es importante. Aquí definimos y limitamos la tókenes que vamos a utilizar. El lenguaje es infinito, para convertirlo en un problema tratable muchas veces los que hacemos es reducirlo. Clave para varias prácticas de reducción es contar las frecuencias de los tókenes, esto es, cuántas veces aparece cada tóken en todo el corpus. Como mencionamos las palabras más frecuentes no aportan mucha información y las más infrecuentes si bien son las que más información tienen no llegarán a ser representativas para nuestro modelo. Descartar palabras poco frecuentes también afecta a errores ortográficos. . Útil para este paso es la clase Counter de la librería estándar de Python. . from collections import Counter c = Counter([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;a&#39;,&#39;b&#39;,&#39;a&#39;]) # obtener los dos elementos más comunes y sus frecuencias c.most_common(2) . [(&#39;a&#39;, 3), (&#39;b&#39;, 2)] . Una función de la librería estándar llamada chain nos dará una mano convirtiendo la lista de listas de tókenes en una lista de tókenes, similar a numpy.flatten, ya que Counter espera una lista con elementos a contar y nuestros tókenes están separados por documentos, hay que juntarlos. . from itertools import chain list(chain([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;], [&#39;c&#39;,&#39;d&#39;])) . [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;, &#39;d&#39;] . chain encadena las listas que le pasamos como argumentos variables. Podemos usar el operador splat * para contentar a la función (convertir la lista principal en una serie de argumentos). . list(chain( *[ [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;], [&#39;c&#39;,&#39;d&#39;] ] )) . [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;, &#39;d&#39;] . En vez de una lista podemos pedir un conjunto (set), en el que los elementos no se repiten. Este bien podría ser el vocabulario. . set(chain( *[ [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;], [&#39;c&#39;,&#39;d&#39;] ] )) . {&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;} . En definitiva, es la lista oficial de tókenes. . # versión 1 class Vocab(): def fit(self, lote): self.vocabulario = set(chain(*lote)) return self def __len__(self): return len(self.vocabulario) . Es importante generar el vocabulario con el dataset de entrenamiento, ya que como mencionamos se trata de la lista de palabras conocidas. Le agregamos un __len__ porque también es útil conocer el tamaño del vocabulario. . v = Vocab().fit(train_docs) len(v) . NameError Traceback (most recent call last) &lt;ipython-input-3-04b70b8e4c3d&gt; in &lt;module&gt; -&gt; 1 v = Vocab().fit(train_docs) 2 len(v) NameError: name &#39;train_docs&#39; is not defined . ¿Qué pasa con las palabras que no están en la lista? Se las conoce como tókenen fuera del vocabulario (out-of-vocabulary, abreviado OOV). Estas requieren acciones especiales, podríamos . ignorarlas | reemplazarlas por un tóken especial | inferirlas (ver más adelante, embeddings) | . # versión 1.1 class Vocab(): def __init__(self, tóken_desconocido=&#39;&lt;unk&gt;&#39;): self.tóken_desconocido = tóken_desconocido def fit(self, lote): self.vocabulario = list(set(chain(*lote))) if self.tóken_desconocido: self.vocabulario.append(self.tóken_desconocido) return self def transform(self, lote): if self.tóken_desconocido: # reemplazar return [[tóken if tóken in self.vocabulario else self.tóken_desconocido for tóken in doc] for doc in lote] else: # ignorar return [[tóken for tóken in doc if tóken in self.vocabulario] for doc in lote] def __len__(self): return len(self.vocabulario) . Vocab().fit(train_docs).transform([ [&#39;poder&#39;, &#39;gestionar&#39;, &#39;clave&#39;, &#39;paso&#39;, &#39;pagina&#39;], [&#39;desde&#39;, &#39;cuando&#39;, &#39;arranco&#39;, &#39;con&#39;, &#39;el&#39;, &#39;programa&#39;, &#39;de&#39;, &#39;millas&#39;], ]) . [[&#39;poder&#39;, &#39;gestionar&#39;, &#39;clave&#39;, &#39;paso&#39;, &#39;pagina&#39;], [&#39;desde&#39;, &#39;cuando&#39;, &#39;&lt;unk&gt;&#39;, &#39;con&#39;, &#39;el&#39;, &#39;programa&#39;, &#39;de&#39;, &#39;millas&#39;]] . Numericalizaci&#243;n . También conocido como indexación. Así como a las unidades mínimas que consideramos las llamamos tókenes, a los números que los representan los llamamos índices. Ya que el vocabulario tiene la lista de tókenes, le vamos a pedir una responsabilidad adicional: que mantenga una asignación entre tókenes y números enteros. Posiblemente ya te ha sucedido pasarle valores no númericos a un estimador y ver cómo falla. . vocabulario = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;&lt;unk&gt;&#39;] {tóken: índice for índice, tóken in enumerate(vocabulario)} . {&#39;a&#39;: 0, &#39;b&#39;: 1, &#39;c&#39;: 2, &#39;d&#39;: 3, &#39;&lt;unk&gt;&#39;: 4} . que es lo mismo que . mapeo = {} for índice, tóken in enumerate(vocabulario): mapeo[tóken] = índice mapeo . {&#39;a&#39;: 0, &#39;b&#39;: 1, &#39;c&#39;: 2, &#39;d&#39;: 3, &#39;&lt;unk&gt;&#39;: 4} . ¿Qué es lo que hace enumerate? Como su nombre lo indica, enumera los elementos de una colección. . list(enumerate(vocabulario)) . [(0, &#39;a&#39;), (1, &#39;b&#39;), (2, &#39;c&#39;), (3, &#39;d&#39;), (4, &#39;&lt;unk&gt;&#39;)] . # versión 2 class Vocab(): def __init__(self, tóken_desconocido=&#39;&lt;unk&gt;&#39;): self.tóken_desconocido = tóken_desconocido def fit(self, lote): vocabulario = list(set(chain(*lote))) if self.tóken_desconocido: vocabulario.append(self.tóken_desconocido) self.mapeo = {tóken: índice for índice, tóken in enumerate(vocabulario)} return self def transform(self, lote): if self.tóken_desconocido: # reemplazar return [[tóken if tóken in self.mapeo else self.tóken_desconocido for tóken in doc] for doc in lote] else: # ignorar return [[tóken for tóken in doc if tóken in self.mapeo] for doc in lote] def __len__(self): return len(self.mapeo) . Comprobemos que la nueva versión de Vocab funciona como la anterior. Además veamos qué sucedo cuando no queremos el tóken para palabras fuera de vocabulario. . Vocab(tóken_desconocido=None).fit(train_docs).transform([ [&#39;poder&#39;, &#39;gestionar&#39;, &#39;clave&#39;, &#39;paso&#39;, &#39;pagina&#39;], [&#39;desde&#39;, &#39;cuando&#39;, &#39;arranco&#39;, &#39;con&#39;, &#39;el&#39;, &#39;programa&#39;, &#39;de&#39;, &#39;millas&#39;], ]) . [[&#39;poder&#39;, &#39;gestionar&#39;, &#39;clave&#39;, &#39;paso&#39;, &#39;pagina&#39;], [&#39;desde&#39;, &#39;cuando&#39;, &#39;con&#39;, &#39;el&#39;, &#39;programa&#39;, &#39;de&#39;, &#39;millas&#39;]] . Ahora vamos a agregar métodos para convertir tókenes a índices y viceversa. . # versión 2.1 class Vocab(): def __init__(self, tóken_desconocido=&#39;&lt;unk&gt;&#39;): self.tóken_desconocido = tóken_desconocido def fit(self, lote): # agregamos `sorted` porque el orden al aplicar `set` no está asegurado vocabulario = list(sorted(set(chain(*lote)))) if self.tóken_desconocido: vocabulario.append(self.tóken_desconocido) self.mapeo = {tóken: índice for índice, tóken in enumerate(vocabulario)} return self def transform(self, lote): if self.tóken_desconocido: # reemplazar return [[tóken if tóken in self.mapeo else self.tóken_desconocido for tóken in doc] for doc in lote] else: # ignorar return [[tóken for tóken in doc if tóken in self.mapeo] for doc in lote] def tókenes_a_índices(self, lote): lote = self.transform(lote) return [[self.mapeo[tóken] for tóken in doc] for doc in lote] def índices_a_tókenes(self, lote): mapeo_inverso = list(self.mapeo.keys()) return [[mapeo_inverso[índice] for índice in doc] for doc in lote] def __len__(self): return len(self.mapeo) . v = Vocab(tóken_desconocido=None).fit(train_docs) v.tókenes_a_índices([ [&#39;que&#39;, &#39;se&#39;, &#39;requiere&#39;, &#39;para&#39;, &#39;un&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;me&#39;, &#39;piden&#39;, &#39;mi&#39;, &#39;numero&#39;, &#39;de&#39;, &#39;cuenta&#39;, &#39;es&#39;, &#39;mi&#39;, &#39;cbu&#39;], ]) . [[4160, 4683, 4484, 3703, 5294, 4011, 3825], [3275, 3854, 3319, 3554, 1532, 1462, 2151, 3319, 950]] . v.índices_a_tókenes([ [4160, 4683, 4484, 3703, 5294, 4011, 3825], [3275, 3854, 3319, 3554, 1532, 1462, 2151, 3319, 950], ]) . [[&#39;que&#39;, &#39;se&#39;, &#39;requiere&#39;, &#39;para&#39;, &#39;un&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;me&#39;, &#39;piden&#39;, &#39;mi&#39;, &#39;numero&#39;, &#39;de&#39;, &#39;cuenta&#39;, &#39;es&#39;, &#39;mi&#39;, &#39;cbu&#39;]] . Casos especiales . ¿Qué sucede con los documentos que al ser tokenizados regresan vacíos? ¿O con documentos compuestos enteramente por palabras fuera del vocabulario? . documentos_problemáticos = [ &#39;??? ???&#39;, &#39;Banks charge high fees for foreign ATM&#39; ] [preprocesar(doc) for doc in documentos_problemáticos] . [[], [&#39;banks&#39;, &#39;charge&#39;, &#39;high&#39;, &#39;fees&#39;, &#39;for&#39;, &#39;foreign&#39;, &#39;atm&#39;]] . v = Vocab().fit(train_docs) v.transform([[], [&#39;banks&#39;, &#39;charge&#39;, &#39;high&#39;, &#39;fees&#39;, &#39;for&#39;, &#39;foreign&#39;]]) . [[], [&#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;]] . v.tókenes_a_índices([[], [&#39;banks&#39;, &#39;charge&#39;, &#39;high&#39;, &#39;fees&#39;, &#39;for&#39;, &#39;foreign&#39;]]) . [[], [5583, 5583, 5583, 5583, 5583, 5583]] . v.índices_a_tókenes([[], [5583, 5583, 5583, 5583, 5583, 5583]]) . [[], [&#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;, &#39;&lt;unk&gt;&#39;]] . La conclusión es que no pasa nada (al menos por ahora). . Bonus: reducci&#243;n del vocabulario . La idea es limitar los tókenes que vamos a utilizar. En cierta forma cada tóken es un atributo (feature) y quisiéramos proveer atributos que sean de utilidad para el estimador. . El lenguaje es infinito, para convertirlo en un problema tratable muchas veces los que hacemos es reducirlo. Clave para varias prácticas de reducción es contar las frecuencias de los tókenes, esto es, cuántas veces aparece cada tóken en todo el corpus. Como mencionamos las palabras más frecuentes no aportan mucha información y las más infrecuentes si bien son las que más información tienen no llegarán a ser representativas para nuestro modelo. Descartar palabras poco frecuentes también afecta a errores ortográficos. . Útil para este paso es la clase Counter de la librería estándar de Python. . from collections import Counter c = Counter([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;a&#39;,&#39;b&#39;,&#39;a&#39;]) # obtener los elementos ordenados de más comunes a menos c.most_common() . [(&#39;a&#39;, 3), (&#39;b&#39;, 2), (&#39;c&#39;, 1)] . Acerca de contar palabras, no te pierdas la ley de Zipf-Models:-Bag-of-Words). . M&#225;s comunes . Una estrategia simple es ordenar a los tókenes según frecuencia y poner un límite duro al vocabulario, de modo de quedarnos con los límite más comunes. . límite = 2 vocabulario = list(c)[:límite] vocabulario . [&#39;a&#39;, &#39;b&#39;] . Por frecuencia de t&#243;ken . Podríamos descartar los que aparecen . más de máximo veces, | menos de mínimo veces. | . máximo = 3 mínimo = 2 vocabulario = [tóken for tóken, frecuencia in c.most_common() if máximo &gt;= frecuencia &gt;= mínimo] vocabulario . [&#39;a&#39;, &#39;b&#39;] . Por frecuencia de documento . O bien, en vez de contar las apariciones absolutas, contar en cuántos documentos aparece cada tóken. Un tóken que aparezca en todos los documentos no colaboraría en una tarea de clasificación, a distinguir documentos pero uno que aparezca en la mitad de los documentos podría ser útil para separarlos en dos grupos. . c = Counter() lote = [ [&#39;hola&#39;, &#39;buen&#39;, &#39;día&#39;], [&#39;hola&#39;, &#39;buenas&#39;, &#39;tardes&#39;], ] for doc in lote: c.update(set(doc)) c.most_common() . [(&#39;hola&#39;, 2), (&#39;buen&#39;, 1), (&#39;día&#39;, 1), (&#39;tardes&#39;, 1), (&#39;buenas&#39;, 1)] . Vamos a normalizar la frecuencias por la cantidad total de documentos ($D$) y de manera similar al punto anterior podríamos descartar los elementos que aparecen en: . más del máximo proporción de los documentos. | menos del mínimo proporción de los documentos. | . D = len(lote) máximo = .9 mínimo = .1 vocabulario = [tóken for tóken, frecuencia in c.most_common() if máximo &gt;= frecuencia/D &gt;= mínimo] vocabulario . [&#39;buen&#39;, &#39;día&#39;, &#39;tardes&#39;, &#39;buenas&#39;] . Stop words . Hay listas armadas de palabras muy comunes (stop words). Podemos elaborarla de alguna manera o usar alguna existente. . #!pip install nltk import nltk nltk.download(&#39;stopwords&#39;) from nltk.corpus import stopwords stopwords.words(&#39;spanish&#39;)[:10] . [nltk_data] Downloading package stopwords to /home/matias/nltk_data... [nltk_data] Package stopwords is already up-to-date! . [&#39;de&#39;, &#39;la&#39;, &#39;que&#39;, &#39;el&#39;, &#39;en&#39;, &#39;y&#39;, &#39;a&#39;, &#39;los&#39;, &#39;del&#39;, &#39;se&#39;] . Un detalle a cuidar es que la tokenización usada para la lista de stop words tiene que haber sido la misma o similar que la usada para los documentos. . def filtrar_stop_words(lote): return [[tóken for tóken in doc if tóken not in stopwords.words(&#39;spanish&#39;)] for doc in lote] filtrar_stop_words([ [&#39;que&#39;, &#39;se&#39;, &#39;requiere&#39;, &#39;para&#39;, &#39;un&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;me&#39;, &#39;piden&#39;, &#39;mi&#39;, &#39;numero&#39;, &#39;de&#39;, &#39;cuenta&#39;, &#39;es&#39;, &#39;mi&#39;, &#39;cbu&#39;], ]) . [[&#39;requiere&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;piden&#39;, &#39;numero&#39;, &#39;cuenta&#39;, &#39;cbu&#39;]] . Por longitud . Esta técnica no requiere contar la frecuencia de los tókenes, simplemente filtramos tókenes muy cortos o muy largos ya que en general son ruidos. . def filtrar_por_longitud(lote, máxima, mínima): return [[tóken for tóken in doc if máxima &gt;= len(tóken) &gt;= mínima] for doc in lote] filtrar_por_longitud([ [&#39;que&#39;, &#39;se&#39;, &#39;requiere&#39;, &#39;para&#39;, &#39;un&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;me&#39;, &#39;piden&#39;, &#39;mi&#39;, &#39;numero&#39;, &#39;de&#39;, &#39;cuenta&#39;, &#39;es&#39;, &#39;mi&#39;, &#39;cbu&#39;], ], máxima=9, mínima=3) . [[&#39;que&#39;, &#39;requiere&#39;, &#39;para&#39;, &#39;prestamo&#39;, &#39;personal&#39;], [&#39;piden&#39;, &#39;numero&#39;, &#39;cuenta&#39;, &#39;cbu&#39;]] . Implementaci&#243;n . Veamos cómo acomodamos lo que hemos visto ahora en la clase Vocab. . # versión 3 import numpy as np from itertools import chain from collections import Counter class Vocab(): def __init__(self, tóken_desconocido=&#39;&lt;unk&gt;&#39;, frecuencia_mínima=0.0, frecuencia_máxima=1.0, longitud_mínima=1, longitud_máxima=np.inf, stop_words=[], límite_vocabulario=None): self.tóken_desconocido = tóken_desconocido self.frecuencia_mínima = frecuencia_mínima self.frecuencia_máxima = frecuencia_máxima self.longitud_mínima = longitud_mínima self.longitud_máxima = longitud_máxima self.stop_words = stop_words self.límite_vocabulario = límite_vocabulario def reducir_vocabulario(self, lote): contador_absoluto = Counter(chain(*lote)) contador_documentos = Counter() for doc in lote: contador_documentos.update(set(doc)) # frecuencia mínima if isinstance(self.frecuencia_mínima, int): # frecuencia de tóken vocabulario_mín = [tóken for tóken, frecuencia in contador_absoluto.most_common() if frecuencia &gt;= self.frecuencia_mínima] else: # frecuencia de documento vocabulario_mín = [tóken for tóken, frecuencia in contador_documentos.most_common() if frecuencia/len(lote) &gt;= self.frecuencia_mínima] # frecuencia máxima if isinstance(self.frecuencia_máxima, int): # frecuencia de tóken vocabulario_máx = [tóken for tóken, frecuencia in contador_absoluto.most_common() if self.frecuencia_máxima &gt;= frecuencia] else: # frecuencia de documento vocabulario_máx = [tóken for tóken, frecuencia in contador_documentos.most_common() if self.frecuencia_máxima &gt;= frecuencia/len(lote)] # intersección de vocabulario_mín y vocabulario_máx preservando el órden vocabulario = [tóken for tóken in vocabulario_mín if tóken in vocabulario_máx] # longitud vocabulario = [tóken for tóken in vocabulario if self.longitud_máxima &gt;= len(tóken) &gt;= self.longitud_mínima] # stop words vocabulario = [tóken for tóken in vocabulario if tóken not in self.stop_words] # límite vocabulario = vocabulario[:self.límite_vocabulario] return vocabulario def fit(self, lote): vocabulario = self.reducir_vocabulario(lote) if self.tóken_desconocido: vocabulario.append(self.tóken_desconocido) self.mapeo = {tóken: índice for índice, tóken in enumerate(vocabulario)} return self def transform(self, lote): if self.tóken_desconocido: # reemplazar return [[tóken if tóken in self.mapeo else self.tóken_desconocido for tóken in doc] for doc in lote] else: # ignorar return [[tóken for tóken in doc if tóken in self.mapeo] for doc in lote] def tókenes_a_índices(self, lote): lote = self.transform(lote) return [[self.mapeo[tóken] for tóken in doc] for doc in lote] def índices_a_tókenes(self, lote): mapeo_inverso = list(self.mapeo.keys()) return [[mapeo_inverso[índice] for índice in doc] for doc in lote] def __len__(self): return len(self.mapeo) . Vocab(longitud_mínima=3).fit(train_docs).transform([ [&#39;poder&#39;, &#39;gestionar&#39;, &#39;clave&#39;, &#39;paso&#39;, &#39;pagina&#39;], [&#39;desde&#39;, &#39;cuando&#39;, &#39;arranco&#39;, &#39;con&#39;, &#39;el&#39;, &#39;programa&#39;, &#39;de&#39;, &#39;millas&#39;], ]) . [[&#39;poder&#39;, &#39;gestionar&#39;, &#39;clave&#39;, &#39;paso&#39;, &#39;pagina&#39;], [&#39;desde&#39;, &#39;cuando&#39;, &#39;&lt;unk&gt;&#39;, &#39;con&#39;, &#39;&lt;unk&gt;&#39;, &#39;programa&#39;, &#39;&lt;unk&gt;&#39;, &#39;millas&#39;]] . El pre-procesamiento hasta ahora . v = Vocab().fit(train_docs) train_índices = v.tókenes_a_índices(train_docs) valid_índices = v.tókenes_a_índices(valid_docs) infer_índices = v.tókenes_a_índices(infer_docs) . Con esto concluye la primera parte. Hay varias librerías que tienen clases que se encargan de efectuar los pasos que hemos visto. Tienen un comportamiento por defecto, que es configurable (los parámetros que hemos visto) y a su vez, personalizable, para reemplazar algunos o todos los pasos por código propio. En general son librerías desarrolladas por angloparlantes, funcionan out-of-the-box bien para el inglés; cuando queremos procesar texto en español vale la pena tener más control sobre estos procesos. . CountVectorizer de scikit-learn. | TextDataBunch de fast.ai. | . Pre-procesando las etiquetas . Las etiquetas del dataset también necesitan ser convertidas a números enteros consecutivos. No lo pensamos para este fin pero Vocab sería útil en este aspecto. El único tema es que Vocab.fit y demás métodos esperan listas de listas de tókenes y a las etiquetas las encontramos en forma de listas de tókenes simplemente. . train_df[&#39;Intencion&#39;].values . array([&#39;Cat_248&#39;, &#39;Cat_42&#39;, &#39;Cat_132&#39;, ..., &#39;Cat_293&#39;, &#39;Cat_138&#39;, &#39;Cat_219&#39;], dtype=object) . Podemos llevar la columna de las etiquetas a una lista de listas con train_df[&#39;Intencion&#39;].values.reshape(-1,1), de manera de poder interfacearlo con Vocab. Algo como train_df[[&#39;Intencion&#39;]].values para que Pandas devuelva un DataFrame en vez de una Series también funcionaría. . train_etiquetas = train_df[[&#39;Intencion&#39;]].values valid_etiquetas = valid_df[[&#39;Intencion&#39;]].values train_etiquetas . array([[&#39;Cat_248&#39;], [&#39;Cat_42&#39;], [&#39;Cat_132&#39;], ..., [&#39;Cat_293&#39;], [&#39;Cat_138&#39;], [&#39;Cat_219&#39;]], dtype=object) . Todo lo que tenga que ver con limitación del vocabulario o agregado de tókenes especiales no nos interesa para este caso de uso. . vocabulario_etiquetas = Vocab(tóken_desconocido=None).fit(train_etiquetas) train_etiquetas = vocabulario_etiquetas.tókenes_a_índices(train_etiquetas) valid_etiquetas = vocabulario_etiquetas.tókenes_a_índices(valid_etiquetas) train_etiquetas[:10] . [[6], [128], [0], [104], [6], [17], [8], [202], [306], [166]] . Ya casi estamos. Solo debemos reconvertir a las etiquetas en una lista de índices (su dimensión original) con un recurso que ya conocemos. . train_etiquetas = list(chain(*train_etiquetas)) valid_etiquetas = list(chain(*valid_etiquetas)) train_etiquetas[:10] . [6, 128, 0, 104, 6, 17, 8, 202, 306, 166] . Fuentes consultadas . http://anie.me/On-Torchtext/ | https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908 | .",
            "url": "https://matiasbattocchia.github.io/datitos/Preprocesamiento-de-texto-para-NLP-parte-1.html",
            "relUrl": "/Preprocesamiento-de-texto-para-NLP-parte-1.html",
            "date": " • Jul 23, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Graph isomorphism networks",
            "content": "Estas notas corresponden a la sección de código de una charla que di en el seminario MachinLenin en 2019. Esta es la presentación que usé y este el repositorio. . Esta es una implementación simple, con fines didácticos y para nada eficiente de la publicación How powerful are graph neural networks?. Para una implementación eficiente ver GINConv de Deep Graph Library (DGL). . La publicación mencionada fue la que elegí para comenzar a aprender sobre redes neuronales de grafos (graph neural networks o GNN). Unas redes interesantes porque no todo es texto, ni todo son imágenes, ni tablas... una vasta cantidad de información se representa en forma de grafo, y estas redes se especializan en esta estructura de datos. . Publicaciones . Hoy en día si tuviese que recomendar lecturas introductorias, serían los reviews de esta lista. . Reviews . Benchmarking Graph Neural Networks | Representation Learning on Graphs: Methods and Applications | http://snap.stanford.edu/proj/embeddings-www/ | . Arquitecturas . GAT | GIN | GCN | GraphSAGE | . import torch import networkx as nx %matplotlib inline . Adjacency matrix . G = nx.binomial_graph(5,0.5) # TODO: numerar los nodos en el gráfico nx.draw(G) . A = torch.tensor( nx.adjacency_matrix(G).todense(), dtype=torch.float32 ) A . tensor([[0., 0., 1., 1., 0.], [0., 0., 0., 1., 1.], [1., 0., 0., 1., 1.], [1., 1., 1., 0., 1.], [0., 1., 1., 1., 0.]]) . X = torch.randint(low=0, high=2, size=(5,2), dtype=torch.float32) X . tensor([[0., 1.], [0., 0.], [1., 0.], [0., 0.], [0., 0.]]) . A @ X . tensor([[1., 0.], [0., 0.], [0., 1.], [1., 1.], [1., 0.]]) . Dataset . import torch.utils.data import importlib gnns = importlib.import_module(&#39;powerful-gnns.util&#39;) class GraphDataset(torch.utils.data.Dataset): &quot;&quot;&quot; Levanta los datasets de Powerful-GNNS. &quot;&quot;&quot; def __init__(self, dataset, degree_as_tag=False): self.data, self.classes = gnns.load_data(dataset, degree_as_tag) self.features = self.data[0].node_features.shape[1] def __len__(self): return len(self.data) def __getitem__(self, idx): graph = self.data[idx] adjacency_matrix = nx.adjacency_matrix( graph.g ).todense() item = {} item[&#39;adjacency_matrix&#39;] = torch.tensor(adjacency_matrix, dtype=torch.float32) item[&#39;node_features&#39;] = graph.node_features item[&#39;label&#39;] = graph.label return item . DS = GraphDataset(&#39;PROTEINS&#39;) . loading data # classes: 2 # maximum node tag: 3 # data: 1113 . DL = torch.utils.data.DataLoader(DS) . Net modules . class GINConv(torch.nn.Module): def __init__(self, hidden_dim): super().__init__() self.linear = torch.nn.Linear(hidden_dim, hidden_dim) def forward(self, A, X): &quot;&quot;&quot; Params A [batch x nodes x nodes]: adjacency matrix X [batch x nodes x features]: node features matrix Returns - X&#39; [batch x nodes x features]: updated node features matrix &quot;&quot;&quot; X = self.linear(X + A @ X) X = torch.nn.functional.relu(X) return X . class GNN(torch.nn.Module): def __init__(self, input_dim, hidden_dim, output_dim, n_layers): super().__init__() self.in_proj = torch.nn.Linear(input_dim, hidden_dim) self.convs = torch.nn.ModuleList() for _ in range(n_layers): self.convs.append(GINConv(hidden_dim)) # In order to perform graph classification, each hidden state # [batch x nodes x hidden_dim] is concatenated, resulting in # [batch x nodes x hiddem_dim*(1+n_layers)], then aggregated # along nodes dimension, without keeping that dimension: # [batch x hiddem_dim*(1+n_layers)]. self.out_proj = torch.nn.Linear(hidden_dim*(1+n_layers), output_dim) def forward(self, A, X): X = self.in_proj(X) hidden_states = [X] for layer in self.convs: X = layer(A, X) hidden_states.append(X) X = torch.cat(hidden_states, dim=2).sum(dim=1) X = self.out_proj(X) return X . Train loop . model = GNN(input_dim=DS.features, hidden_dim=3, output_dim=DS.classes, n_layers=3) criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.01) EPOCHS = 5 for epoch in range(EPOCHS): running_loss = 0.0 for i, batch in enumerate(DL): A = batch[&#39;adjacency_matrix&#39;] X = batch[&#39;node_features&#39;] labels = batch[&#39;label&#39;] optimizer.zero_grad() outputs = model(A, X) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f&#39;{epoch} - loss: {running_loss/(i+1)}&#39;) . 0 - loss: 1.2691171061103663 1 - loss: 0.32851389047806046 2 - loss: 0.944981049655904 3 - loss: 0.2894633889519622 4 - loss: 0.3056941165221455 . TODO . Dropout | Mini-batches (collate_fn for padding) | Batch normalization . | Extended neighborhood X = self.linear(X + A @ X + A**2 @ X) . | Node classification | Link prediction | .",
            "url": "https://matiasbattocchia.github.io/datitos/Graph-isomorphism-network.html",
            "relUrl": "/Graph-isomorphism-network.html",
            "date": " • Oct 24, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Pandas (parte 1)",
            "content": "import pandas as pd . Estructuras de datos . Dos tipos de datos muy usados en Python son las listas (list) y los diccionarios (dict). Ambos son útiles para coleccionar valores de todo tipo. . lista = [&#39;hidrógeno&#39;, &#39;helio&#39;, &#39;litio&#39;] lista . [&#39;hidrógeno&#39;, &#39;helio&#39;, &#39;litio&#39;] . Podemos referenciar un valor coleccionado utilizando el operador índice [ ]. Las listas indexan por posición. En Python se empieza contar desde cero, por lo tanto las posiciones también. . # get lista[0] . &#39;hidrógeno&#39; . # set lista[0] = &#39;?&#39; lista . [&#39;?&#39;, &#39;helio&#39;, &#39;litio&#39;] . Los diccionarios son colecciones de pares etiqueta:valor e indexan por etiqueta. No es posible referenciar a un elemento por posición. . diccionario = { &#39;H&#39; :&#39;hidrógeno&#39;, &#39;He&#39;:&#39;helio&#39;, &#39;Li&#39;:&#39;litio&#39;, } diccionario . {&#39;H&#39;: &#39;hidrógeno&#39;, &#39;He&#39;: &#39;helio&#39;, &#39;Li&#39;: &#39;litio&#39;} . # get diccionario[&#39;H&#39;] . &#39;hidrógeno&#39; . # set diccionario[&#39;Be&#39;] = &#39;berilio&#39; diccionario . {&#39;H&#39;: &#39;hidrógeno&#39;, &#39;He&#39;: &#39;helio&#39;, &#39;Li&#39;: &#39;litio&#39;, &#39;Be&#39;: &#39;berilio&#39;} . Pandas . Una Series es un objeto unidimensional, similar a una columna en una tabla. Se comporta comporta como una lista y también le asigna una etiqueta a cada elemento en la colección, comportándose como un diccionario. Por defecto, cada elemento recibirá una etiqueta que va de 0 a N-1, donde N es la longitud/tamaño de la colección. . # creación de una Series a partir de una lista arbitraria pd.Series([&#39;hidrógeno&#39;, &#39;helio&#39;, &#39;litio&#39;]) . 0 hidrógeno 1 helio 2 litio dtype: object . Nota: dtype es el tipo de los elementos de la colección. En este caso object hace referencia a objetos en general, es decir, cualquier tipo de objeto: entero (int), flotante (float), texto (str), etcétera. . # creación de una Series especificando un índice pd.Series([&#39;hidrógeno&#39;, &#39;helio&#39;, &#39;litio&#39;], index=[&#39;H&#39;, &#39;He&#39;, &#39;Li&#39;]) . H hidrógeno He helio Li litio dtype: object . # creación de una Series a partir de un diccionario (las etiquetas serán los valores del índice) s = pd.Series({ &#39;H&#39; :&#39;hidrógeno&#39;, &#39;He&#39;:&#39;helio&#39;, &#39;Li&#39;:&#39;litio&#39;, }) s . H hidrógeno He helio Li litio dtype: object . # obtener el índice s.index . Index([&#39;H&#39;, &#39;He&#39;, &#39;Li&#39;], dtype=&#39;object&#39;) . El índice es un objeto del tipo Index, aportado por Pandas. . # obtener los elementos s.values . array([&#39;hidrógeno&#39;, &#39;helio&#39;, &#39;litio&#39;], dtype=object) . Los valores se coleccionan en arreglos de NumPy (objetos del tipo array). Los arreglos son similares a las listas, tienen más funcionalidades. . # el índice se utiliza para seleccionar elementos específicos s[&#39;H&#39;] . &#39;hidrógeno&#39; . # los valores se pueden cambiar o agregar mediante asignación como si se tratase de un diccionario s[&#39;Be&#39;] = &#39;berilio&#39; s . H hidrógeno He helio Li litio Be berilio dtype: object . . Un DataFrame es una estructura tabular (bidimensional) compuesta por filas y columnas, similar a una hoja de cálculo, una tabla de una base de datos, o a un data.frame del lenguaje R. . Se puede pensar al DataFrame como una Series de Series: una Series cuyo índice son los nombres de las columnas y cuyos elementos son Series que se comportan como columnas. El índice de las Series-columna son los nombres de las filas. . # creación de un DataFrame a partir de una lista de listas pd.DataFrame([ [1, 1.008], [2, 4.003], [3, 6.941], [4, 9.012] ]) . 0 1 . 0 1 | 1.008 | . 1 2 | 4.003 | . 2 3 | 6.941 | . 3 4 | 9.012 | . # creación de un DataFrame especificando índice y columnas df = pd.DataFrame( [[1, 1.008], [2, 4.003], [3, 6.941], [4, 9.012]], index=[&#39;H&#39;, &#39;He&#39;, &#39;Li&#39;, &#39;Be&#39;], columns=[&#39;número_atómico&#39;, &#39;masa_atómica&#39;] ) df . número_atómico masa_atómica . H 1 | 1.008 | . He 2 | 4.003 | . Li 3 | 6.941 | . Be 4 | 9.012 | . Otras formas de inicializar un DataFrame, usando: . diccionario de listas | diccionario de diccionarios | lista de diccionarios | . # obtener los nombres de las filas df.index . Index([&#39;H&#39;, &#39;He&#39;, &#39;Li&#39;, &#39;Be&#39;], dtype=&#39;object&#39;) . # obtener los nombres de las columnas df.columns . Index([&#39;número_atómico&#39;, &#39;masa_atómica&#39;], dtype=&#39;object&#39;) . # obtener solo los valores df.values . array([[1. , 1.008], [2. , 4.003], [3. , 6.941], [4. , 9.012]]) . Nota: Los nombres de columnas también son un índice (Index). Recapitulando, las Series tiene un índice y los DataFrames tienen dos. Como Pandas piensa la tabla como una colección de columnas, para obtener un valor de una celda primero hay que acceder a la columna y luego a la fila. . # podemos seleccionar una columna, obtendremos una Series df[&#39;masa_atómica&#39;] . H 1.008 He 4.003 Li 6.941 Be 9.012 Name: masa_atómica, dtype: float64 . # podemos cambiar o crear una columna mediante asignación, normalmente de una Series; usamos la del ejemplo anterior df[&#39;nombre&#39;] = s df . número_atómico masa_atómica nombre . H 1 | 1.008 | hidrógeno | . He 2 | 4.003 | helio | . Li 3 | 6.941 | litio | . Be 4 | 9.012 | berilio | . # porque una columna es una Series, podemos usar el índice de la Series para obtener el valor de la celda df[&#39;masa_atómica&#39;][&#39;He&#39;] . 4.003 . Nota: Los índices se corresponden con las dimensiones del objeto, un valor de una celda no tiene índices, por lo que es un objeto cero-dimensional, también llamado escalar. . Carga de datos . Ver más: http://pandas.pydata.org/pandas-docs/stable/io.html . Pandas importa y exporta datos de y hacia gran cantidad de formatos. . CSV . df = pd.read_csv(&#39;datos/properati_básico.csv.gz&#39;) . Para exportar un DataFrame a un archivo CSV podemos usar . df.to_csv(&#39;archivo.csv&#39;) . Excel . Requiere un paquete adicional. . !pip install xlrd . Leer una hoja de cálculo. . pd.read_excel(&#39;datos/archivo.xlsx&#39;) . SQL . El siguiente código muestra cómo consultar una base de datos. . import sqlite3 conexión = sqlite3.connect(&#39;datos/db.sqlite&#39;) consulta = &quot;SELECT * FROM tabla&quot; pd.read_sql(consulta, conexión) . BigQuery . Requiere un paquete adicional. . !pip install pandas-gbq . El resto es parecido a importar desde una base de datos relacional. . consulta = &quot;SELECT * FROM tabla&quot; pd.read_gbq(consulta, project_id=&#39;properati-data-public&#39;) . Inspecci&#243;n . # ver primeras filas; probar tail para las últimas df.head() . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 0 2017-11-01 | departamento | -34.629118 | -58.480835 | 275000.0 | NaN | NaN | 5.0 | FLORESTA | . 1 2017-11-03 | departamento | -34.627120 | -58.475595 | 268000.0 | 134.0 | 122.0 | 4.0 | FLORESTA | . 2 2017-11-03 | departamento | -34.630429 | -58.486193 | 88000.0 | 37.0 | 34.0 | 2.0 | FLORESTA | . 3 2017-11-09 | departamento | -34.628319 | -58.479828 | 134900.0 | 70.0 | 70.0 | 3.0 | FLORESTA | . 4 2017-11-14 | departamento | -34.620887 | -58.491288 | 69000.0 | NaN | 50.0 | 2.0 | FLORESTA | . El método info nos informa de ambos índices (filas y columnas), incluyendo cantidades. Nombres de columnas, tipos de datos, cantidades de valores no nulos, uso de memoria RAM. . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 24258 entries, 0 to 24257 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 fecha 24258 non-null object 1 tipo 24258 non-null object 2 lat 24258 non-null float64 3 lon 24258 non-null float64 4 precio 23141 non-null float64 5 superficie_total 22261 non-null float64 6 superficie_cubierta 22664 non-null float64 7 ambientes 20533 non-null float64 8 barrio 24258 non-null object dtypes: float64(6), object(3) memory usage: 1.7+ MB . El método describe muestra estadísticas básicas acerca de las columnas númericas. Aplica sobre todas las columnas númericas, incluso sobre aquellas en las que no tiene sentido hacer estadísticas (lat y lon por ejemplo). . df.describe() . lat lon precio superficie_total superficie_cubierta ambientes . count 24258.000000 | 24258.000000 | 2.314100e+04 | 22261.000000 | 22664.000000 | 20533.000000 | . mean -34.599238 | -58.436230 | 2.779905e+05 | 133.717578 | 137.000176 | 2.874056 | . std 0.026307 | 0.039340 | 3.611446e+05 | 1633.426319 | 2418.268251 | 1.690704 | . min -34.695717 | -58.529982 | 4.870050e+03 | 0.000000 | 1.000000 | 1.000000 | . 25% -34.618590 | -58.464357 | 1.200000e+05 | 46.000000 | 41.000000 | 2.000000 | . 50% -34.598897 | -58.436198 | 1.800000e+05 | 70.000000 | 62.000000 | 3.000000 | . 75% -34.580350 | -58.403884 | 2.990000e+05 | 120.000000 | 104.000000 | 4.000000 | . max -34.536066 | -58.353946 | 1.350000e+07 | 184000.000000 | 263960.000000 | 38.000000 | . Indexaci&#243;n . Por indexación nos referimos a la selección de un subconjunto de un DataFrame o de una Series. Por ahora hemos visto un uso básico de los corchetes [ ], el operador índice, para acceder a columnas de un DataFrame y a valores de una Series. En el caso de las Series los casos de uso del operador no difieren prácticamente de lo que se puede hacer con un arreglo de NumPy, sin embargo en el caso de los DataFrames se complejiza ya que permite seleccionar por filas (no lo vimos) o por columnas (sí lo vimos). . Los corchetes existen en Pandas por conveniencia para hacer algunas operaciones más simples, lo cual también traerá aparejado limitaciones. Pandas va a determinar si estamos queriendo acceder a la tabla usando el índice que se llama index (nombres filas) o el que se llama columns. . A su vez hay dos maneras de utilizar los índices: . por etiqueta (como en los diccionarios), | por posición (como en las listas). | . Esta modalidad dual no está presente en los diccionarios, que no pueden ser indexados por posición, ni en las listas, que no pueden ser indexadas por etiqueta. Está presente en Pandas y nos permite acceder a los datos de la forma conveniente según el caso. . # un nombre columna, devuelve una Series df[&#39;barrio&#39;].head() . 0 FLORESTA 1 FLORESTA 2 FLORESTA 3 FLORESTA 4 FLORESTA Name: barrio, dtype: object . No es posible obtener una sola fila con [ ], pero sí un subconjunto de filas y también un subconjunto de columnas. Antes de mostrar cómo hacerlo, introducimos tres nuevos conceptos: slice, lista de índices y máscara booleana. . Cortes . Existe en Python un tipo de objeto llamado slice, traducido como corte o rebanada, cuyo fin es ayudar a producir sublistas a partir de listas, &quot;cortando&quot; una lista desde una posición inicial hasta una posición final. En realidad la lista original no es modificada sino que se devuelve una nueva, usando los elementos que ya existen en la original (los objetos no son copiados). . frutas = [&#39;naranja&#39;, &#39;banana&#39;, &#39;ananá&#39;, &#39;durazno&#39;, &#39;uva&#39;] frutas . [&#39;naranja&#39;, &#39;banana&#39;, &#39;ananá&#39;, &#39;durazno&#39;, &#39;uva&#39;] . Sabemos cómo obtener valores individuales. . frutas[2] . &#39;ananá&#39; . Supongamos que nos interesa conseguir una sublista con las tres primeras frutas, serían los elementos en las posiciones 0, 1, 2, por lo que deberíamos cortar desde 0 hasta 3 (no inclusive). . corte = slice(0, 3) corte . slice(0, 3, None) . frutas[corte] . [&#39;naranja&#39;, &#39;banana&#39;, &#39;ananá&#39;] . Podemos obtener el mismo resultado con un atajo. Los atajos reciben el nombre de azúcar sintáctica, son expresiones cómodas para idiomas frecuentes, que existen en paralelo a las formas regulares del lenguaje. . frutas[0:3] . [&#39;naranja&#39;, &#39;banana&#39;, &#39;ananá&#39;] . # si la posición inicial está ausente, se entiende que va desde el principio frutas[:3] . [&#39;naranja&#39;, &#39;banana&#39;, &#39;ananá&#39;] . # si la posición final está ausente, se entiende que va hasta el final frutas[3:] . [&#39;durazno&#39;, &#39;uva&#39;] . # esto no parece útil ahora pero lo será más adelante frutas[:] . [&#39;naranja&#39;, &#39;banana&#39;, &#39;ananá&#39;, &#39;durazno&#39;, &#39;uva&#39;] . # ídem pero sin azúcar corte = slice(None) corte . slice(None, None, None) . frutas[corte] . [&#39;naranja&#39;, &#39;banana&#39;, &#39;ananá&#39;, &#39;durazno&#39;, &#39;uva&#39;] . Listas de &#237;ndices y m&#225;scaras booleanas . Los arreglos de NumPy son como las listas de Python pero con funcionalidades agregadas. Admiten indexado por posición, por corte y añaden nuevas maneras de formar sub-arreglos, mediante . listas o rangos de posiciones, | listas de valores booleanos, llamadas máscaras. | . import numpy as np frutas = np.array([&#39;naranja&#39;, &#39;banana&#39;, &#39;ananá&#39;, &#39;durazno&#39;, &#39;uva&#39;]) frutas . array([&#39;naranja&#39;, &#39;banana&#39;, &#39;ananá&#39;, &#39;durazno&#39;, &#39;uva&#39;], dtype=&#39;&lt;U7&#39;) . Listas de posiciones . Supongamos que queremos obtener los elementos naranja, ananá, uva. A diferencia de las listas que solo permiten enteros o cortes, NumPy nos permite pasar una lista con las posiciones de interés. . # en vez de usar un escalar (int, str...) usamos una lista frutas[[0,2,4]] . array([&#39;naranja&#39;, &#39;ananá&#39;, &#39;uva&#39;], dtype=&#39;&lt;U7&#39;) . En Python existe un tipo de objeto llamado rango (range), que se puede crear —como casi todos los tipos de objetos en el lenguaje— utilizando una función homónima, que representa una colección de números desde un valor inicial hasta un valor final (no inclusive) utilizando un intervalo determinado. A diferencia de slice, los rangos pueden ser convertidos en listas. . Si se trata de un patrón regular, es mejor definir un rango antes que una lista. . # desde cero hasta cinco contando de a dos rango = range(0, 5, 2) rango . range(0, 5, 2) . # pueden ser convertidos en listas list(rango) . [0, 2, 4] . # se obtienen los mismos resultados frutas[rango] . array([&#39;naranja&#39;, &#39;ananá&#39;, &#39;uva&#39;], dtype=&#39;&lt;U7&#39;) . M&#225;scaras . La máscara debe tener el mismo largo que el arreglo y estar compuesta por valores booleanos (bool). En este caso, cinco elementos. . máscara = [True, False, True, False, True] frutas[máscara] . array([&#39;naranja&#39;, &#39;ananá&#39;, &#39;uva&#39;], dtype=&#39;&lt;U7&#39;) . Se filtran aquellos elementos cuya posición se corresponde con las posiciones de los valores False de la máscara, se seleccionan aquellos donde True. La máscara sirve para ocultar algunas partes y mostrar otras. . . Podemos utilizar una lista de etiquetas para obtener un subconjunto de columnas. . # lista de nombres de columnas, devuelve un DataFrame df[[&#39;barrio&#39;, &#39;ambientes&#39;, &#39;precio&#39;]].head() . barrio ambientes precio . 0 FLORESTA | 5.0 | 275000.0 | . 1 FLORESTA | 4.0 | 268000.0 | . 2 FLORESTA | 2.0 | 88000.0 | . 3 FLORESTA | 3.0 | 134900.0 | . 4 FLORESTA | 2.0 | 69000.0 | . Podemos utilizar un corte para obtener un subconjunto de filas por posición. . df[:5] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 0 2017-11-01 | departamento | -34.629118 | -58.480835 | 275000.0 | NaN | NaN | 5.0 | FLORESTA | . 1 2017-11-03 | departamento | -34.627120 | -58.475595 | 268000.0 | 134.0 | 122.0 | 4.0 | FLORESTA | . 2 2017-11-03 | departamento | -34.630429 | -58.486193 | 88000.0 | 37.0 | 34.0 | 2.0 | FLORESTA | . 3 2017-11-09 | departamento | -34.628319 | -58.479828 | 134900.0 | 70.0 | 70.0 | 3.0 | FLORESTA | . 4 2017-11-14 | departamento | -34.620887 | -58.491288 | 69000.0 | NaN | 50.0 | 2.0 | FLORESTA | . Nota: En muchos casos los nombres de filas van a coincidir con las posiciones de filas, ya que si no especificamos un índice, el índice por defecto que asigna Pandas a las filas va de 0 a N-1, siendo N la cantidad de filas. Estas etiquetas coinciden con la posición. . También podemos utilizar una máscara. Hay formas muy prácticas de generar máscaras, la que se presenta a continuación solo tiene fines demostrativos y jamás volverá a ser utilizada. . # la función len nos dice la cantidad de filas del DataFrame cantidad_filas = len(df) # usando la función np.null armamos un arreglo tan largo como la tabla, lleno de un valor especificado (False) máscara = np.full(cantidad_filas, False) # retocamos la máscara para recuperar solo las primeras dos filas máscara[0] = True máscara[1] = True df[máscara] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 0 2017-11-01 | departamento | -34.629118 | -58.480835 | 275000.0 | NaN | NaN | 5.0 | FLORESTA | . 1 2017-11-03 | departamento | -34.627120 | -58.475595 | 268000.0 | 134.0 | 122.0 | 4.0 | FLORESTA | . Resumimos las capacidades de [ ]: . Una etiqueta de columna, devuelve una columna. | Una lista de etiquetas de columnas, devuelve una subtabla. | Un corte de posiciones de filas, devuelve una subtabla. | Una máscara de posiciones de filas, devuelve una subtabla. | . Se nota el comportamiento de filas por sus posiciones y columnas por sus etiquetas. . Alternativamente se puede acceder a una columna como si fuera un atributo del DataFrame. . # funciona como df[&#39;barrio&#39;]; es como df$barrio en R df.barrio.head() . 0 FLORESTA 1 FLORESTA 2 FLORESTA 3 FLORESTA 4 FLORESTA Name: barrio, dtype: object . Filas y columnas . ¿Qué pasa si queremos filtrar utilizando ambos índices simultáneamente? En principio podríamos hacer . df[columnas][filas] . sin embargo esta manera efectúa dos selecciones, una seguida de la otra, no es simultánea, lo que traerá más adelante implicancias para la asignación. Para esta situación Pandas provee las siguientes formas de indexar. . loc[filas, columnas] para acceder por etiquetas, | iloc[filas, columnas] para acceder por posición. | . Es un poco confuso al principio, la i de iloc hace pensar en la fila (o en la columna) número i, o sea en la posición i. . La limitación de estas formas es que no es posible mezclar etiquetas con posiciones; por ejemplo posición de filas y etiquetas de columnas, una combinación bastante deseable. . # esta línea de código desordena las filas para que sus etiquetas ya no coincidan con # el orden que las filas tienen en la tabla (no hace falta entender esta línea en este momento); # el método sample sirve para obtener una muestra al azar de filas df = df.sample(frac=1, random_state=42) df.head() . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 19411 2018-02-23 | ph | -34.640644 | -58.500677 | 128000.0 | 65.0 | 60.0 | NaN | VILLA LURO | . 17819 2018-02-28 | departamento | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | BARRACAS | . 14611 2018-02-07 | departamento | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | COLEGIALES | . 22362 2018-02-20 | casa | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | VILLA DEVOTO | . 16962 2018-04-11 | departamento | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | VILLA CRESPO | . Por etiquetas . # valor de una celda, fila y columna; devuelve un escalar (valor suelto) df.loc[100, &#39;fecha&#39;] . &#39;2018-02-21&#39; . La selección de todas las columnas en R se logra con df[filas,] pero en Python esta sintaxis no es válida, opciones son df.loc[filas] o df.loc[filas, :]. . # un fila puntual, todas las columnas; devuelve una Series df.loc[100] . fecha 2018-02-21 tipo departamento lat -34.629 lon -58.4797 precio 92000 superficie_total 48 superficie_cubierta 48 ambientes 3 barrio FLORESTA Name: 100, dtype: object . # varias filas (lista de etiquetas), una columna; devuelve una Series df.loc[[1,10,100], &#39;fecha&#39;] . 1 2017-11-03 10 2017-11-29 100 2018-02-21 Name: fecha, dtype: object . # varias filas, varias columnas; devuelve un DataFrame df.loc[[1,10,100], [&#39;superficie_cubierta&#39;, &#39;superficie_total&#39;]] . superficie_cubierta superficie_total . 1 122.0 | 134.0 | . 10 341.0 | 401.0 | . 100 48.0 | 48.0 | . La selección de todas las filas en R se logra con df[,columnas] pero en Python esta sintáxis no es válida, se requiere de un corte total slice(None) o más común, su azúcar sintáctica df.loc[:, columnas]. . # todas las filas, algunas columnas; devuelve un DataFrame df.loc[:, [&#39;superficie_cubierta&#39;, &#39;superficie_total&#39;]].head() . superficie_cubierta superficie_total . 19411 60.0 | 65.0 | . 17819 32.0 | 34.0 | . 14611 48.0 | 58.0 | . 22362 112.0 | 112.0 | . 16962 40.0 | 47.0 | . Si bien las máscaras trabajan por posición y la indexación de loc es por etiquetas, funcionan de todas formas — es confuso pero será uno de los idiomas más útiles e incluso hasta cobrará sentido más adelante. . # utilizamos la máscara del ejemplo anterior df.loc[máscara, [&#39;superficie_cubierta&#39;, &#39;superficie_total&#39;]] . superficie_cubierta superficie_total . 19411 60.0 | 65.0 | . 17819 32.0 | 34.0 | . IMPORTANTE: Pandas hace su propia interpretación de los cortes en la indexación por etiquetas. . Ambos extremos son inclusivos. | Van de un valor del índice a otro según su orden en el índice. | # casteamos Index a lista para que sea más legible list(df.columns) . [&#39;fecha&#39;, &#39;tipo&#39;, &#39;lat&#39;, &#39;lon&#39;, &#39;precio&#39;, &#39;superficie_total&#39;, &#39;superficie_cubierta&#39;, &#39;ambientes&#39;, &#39;barrio&#39;] . Por ejemplo, slice(&#39;tipo&#39;, &#39;precio&#39;) tiene el mismo efecto que [&#39;tipo&#39;, &#39;lat&#39;, &#39;lon&#39;, &#39;precio&#39;]. . df.loc[100, &#39;tipo&#39;:&#39;precio&#39;] . tipo departamento lat -34.629 lon -58.4797 precio 92000 Name: 100, dtype: object . Veamos el caso de las etiquetas de las filas. . # imprimir solo las primeras diez etiquetas list(df.index)[:10] . [19411, 17819, 14611, 22362, 16962, 19442, 15399, 12244, 6674, 4715] . Un corte desde la tercera etiqueta (14611) hasta la quinta (16962), slice(14611, 16962), es como pasar la lista [14611, 22362, 16962]. Si el orden del índice fuese otro, la selección generada por el mismo corte, sería otra. . df.loc[14611:16962] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 14611 2018-02-07 | departamento | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | COLEGIALES | . 22362 2018-02-20 | casa | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | VILLA DEVOTO | . 16962 2018-04-11 | departamento | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | VILLA CRESPO | . Por posici&#243;n . # primeras filas; cuando se indexa por posición no se le presta atención a las etiquetas df.iloc[:5] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 19411 2018-02-23 | ph | -34.640644 | -58.500677 | 128000.0 | 65.0 | 60.0 | NaN | VILLA LURO | . 17819 2018-02-28 | departamento | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | BARRACAS | . 14611 2018-02-07 | departamento | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | COLEGIALES | . 22362 2018-02-20 | casa | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | VILLA DEVOTO | . 16962 2018-04-11 | departamento | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | VILLA CRESPO | . IMPORTANTE: con iloc las columnas se seleccionan por su posición, no por sus etiquetas. . # casteamos Index a lista para que sea más legible list(df.columns) . [&#39;fecha&#39;, &#39;tipo&#39;, &#39;lat&#39;, &#39;lon&#39;, &#39;precio&#39;, &#39;superficie_total&#39;, &#39;superficie_cubierta&#39;, &#39;ambientes&#39;, &#39;barrio&#39;] . # fecha está en la posición 0 en el índice de las columnas, tipo está en la posición 1 df.iloc[:, [0,1]].head() . fecha tipo . 19411 2018-02-23 | ph | . 17819 2018-02-28 | departamento | . 14611 2018-02-07 | departamento | . 22362 2018-02-20 | casa | . 16962 2018-04-11 | departamento | . IMPORTANTE: iloc no acepta máscaras. . Resumen . Como con listas, es posible indexar por posición. | Como con diccionarios, es posible indexar por etiqueta. | Como con arreglos de NumPy, es posible indexar por máscaras booleanas. | Cualquiera de estos indexadores pueden ser escalares (int, str, ...), listas, o cortes. | Funcionan en filas y columnas. | Funcionan en índices jerárquicos (próximo tema). | . Asignaci&#243;n . Las distintas formas de indexar también sirven para guardar valores. . Supongamos que nos informan que las propiedades con etiquetas 333 y 666 están mal anotadas como departamentos, que en realidad son casas. . df.loc[[333, 666]] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 333 2017-12-06 | departamento | -34.552416 | -58.456552 | 120000.0 | 37.0 | 33.0 | 1.0 | NUÑEZ | . 666 2018-03-25 | departamento | -34.556262 | -58.464462 | 377944.0 | 110.0 | 108.0 | 4.0 | NUÑEZ | . # asignamos el valor casa a las filas deseadas en la columna correspondiente df.loc[[333, 666], &#39;tipo&#39;] = &#39;casa&#39; df.loc[[333, 666]] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 333 2017-12-06 | casa | -34.552416 | -58.456552 | 120000.0 | 37.0 | 33.0 | 1.0 | NUÑEZ | . 666 2018-03-25 | casa | -34.556262 | -58.464462 | 377944.0 | 110.0 | 108.0 | 4.0 | NUÑEZ | . Adicionalmente nos reportan que el precio de la propiedad con etiqueta 666 ha sido actualizado. . df.loc[666, [&#39;fecha&#39;, &#39;precio&#39;]] = [&#39;2018-10-28&#39;, 510000] # pasamos una lista de etiquetas para que devuelva un DataFrame, # esto es válido más allá de que sea una lista de un solo elemento df.loc[[666]] . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes barrio . 666 2018-10-28 | casa | -34.556262 | -58.464462 | 510000.0 | 110.0 | 108.0 | 4.0 | NUÑEZ | . &#205;ndices jer&#225;rquicos . Hasta ahora vimos cómo trabajar con índices creados por defecto (cuando las etiquetas van de 0 a N-1) y con índices impuestos al cargar un archivo (las etiquetas de las columnas), sin embargo el índice puede ser modificado según necesidad. Hay varias formas de definir un nuevo índice, una muy común es elegir a una columna para que se convierta en el nuevo índice. . df.set_index(&#39;barrio&#39;, inplace=True) df.head() . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes . barrio . VILLA LURO 2018-02-23 | ph | -34.640644 | -58.500677 | 128000.0 | 65.0 | 60.0 | NaN | . BARRACAS 2018-02-28 | departamento | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | . COLEGIALES 2018-02-07 | departamento | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | . VILLA DEVOTO 2018-02-20 | casa | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | . VILLA CRESPO 2018-04-11 | departamento | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | . Nota: Muchos métodos que transforman DataFrames nos devuelven un nuevo DataFrame, mientras que el original permanece intacto. El argumento inplace=True se usa para que el método transforme el DataFrame original; en este caso el método no devuelve nada (None). . Al convertir a la columna barrio en índice, esta columna dejó de existir. Esto quiere decir que no podemos invocar a la columna como antes, haciendo df[&#39;barrio&#39;] o df.barrio, en todo caso podemos pedir df.index. Es importante recordar que aunque las etiquetas de las filas parezcan una columna, no lo son, así como las etiquetas de las columnas aunque parecen ser una fila, tampoco lo son. . De aquí en más, podemos hacer indexación con las nuevas etiquetas. . # traer todas las propiedades en Palermo df.loc[&#39;PALERMO&#39;].head() . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes . barrio . PALERMO 2018-03-30 | casa | -34.578329 | -58.397749 | 3100000.0 | 450.0 | 350.0 | 3.0 | . PALERMO 2018-04-10 | departamento | -34.586787 | -58.416735 | 145000.0 | NaN | NaN | 3.0 | . PALERMO 2018-04-18 | departamento | -34.582436 | -58.414845 | 850000.0 | 270.0 | 240.0 | 6.0 | . PALERMO 2018-01-26 | departamento | -34.578773 | -58.415132 | 330000.0 | 74.0 | 64.0 | 3.0 | . PALERMO 2018-04-06 | departamento | -34.569005 | -58.436241 | 300000.0 | 110.0 | 98.0 | NaN | . # traer todas las propiedades en Palermo y Chacarita resultado = df.loc[[&#39;PALERMO&#39;,&#39;CHACARITA&#39;], :] resultado.head(2) . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes . barrio . PALERMO 2018-03-30 | casa | -34.578329 | -58.397749 | 3100000.0 | 450.0 | 350.0 | 3.0 | . PALERMO 2018-04-10 | departamento | -34.586787 | -58.416735 | 145000.0 | NaN | NaN | 3.0 | . # al principio de la tabla están las de Palermo, hacia el final de la tabla están las de Chacarita resultado.tail(2) . fecha tipo lat lon precio superficie_total superficie_cubierta ambientes . barrio . CHACARITA 2017-12-09 | ph | -34.582115 | -58.455020 | 329000.0 | 265.0 | 215.0 | 5.0 | . CHACARITA 2018-02-11 | departamento | -34.590656 | -58.448731 | 92000.0 | 33.0 | 30.0 | 1.0 | . Cuando necesitemos recuperar al índice como columna, podemos traerla de vuelta. . df.reset_index(inplace=True) df.head() . barrio fecha tipo lat lon precio superficie_total superficie_cubierta ambientes . 0 VILLA LURO | 2018-02-23 | ph | -34.640644 | -58.500677 | 128000.0 | 65.0 | 60.0 | NaN | . 1 BARRACAS | 2018-02-28 | departamento | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | . 2 COLEGIALES | 2018-02-07 | departamento | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | . 3 VILLA DEVOTO | 2018-02-20 | casa | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | . 4 VILLA CRESPO | 2018-04-11 | departamento | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | . . Pandas tiene la capacidad de convertir a más de una columna en índice, lo que recibe el nombre de multi-índice, en la que cada ex-columna aporta un nivel de indexación, estableciéndose también una jerarquía de indexación. Los valores existentes en las columnas se convierten en etiquetas. . df.set_index([&#39;barrio&#39;,&#39;tipo&#39;], inplace=True) df.head() . fecha lat lon precio superficie_total superficie_cubierta ambientes . barrio tipo . VILLA LURO ph 2018-02-23 | -34.640644 | -58.500677 | 128000.0 | 65.0 | 60.0 | NaN | . BARRACAS departamento 2018-02-28 | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | . COLEGIALES departamento 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | . VILLA DEVOTO casa 2018-02-20 | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | . VILLA CRESPO departamento 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | . En este caso barrio es el nivel 0 de indexación, tiene precedencia por sobre tipo, que viene a ser el nivel 1. Vamos a poder referenciar a los niveles por nombre y por posición. . df.index.names . FrozenList([&#39;barrio&#39;, &#39;tipo&#39;]) . Seleccionar filas utilizando solo el nivel superior es como si trabajásemos con un índice simple, lo que veníamos haciendo. . # notar que la tabla devuelta carece del nivel superior df.loc[&#39;PALERMO&#39;].head() . fecha lat lon precio superficie_total superficie_cubierta ambientes . tipo . casa 2018-03-30 | -34.578329 | -58.397749 | 3100000.0 | 450.0 | 350.0 | 3.0 | . departamento 2018-04-10 | -34.586787 | -58.416735 | 145000.0 | NaN | NaN | 3.0 | . departamento 2018-04-18 | -34.582436 | -58.414845 | 850000.0 | 270.0 | 240.0 | 6.0 | . departamento 2018-01-26 | -34.578773 | -58.415132 | 330000.0 | 74.0 | 64.0 | 3.0 | . departamento 2018-04-06 | -34.569005 | -58.436241 | 300000.0 | 110.0 | 98.0 | NaN | . Antes de continuar es necesario introducir a las tuplas (tuple), otro tipo de objeto nativo de Python. Son muy parecidas a las listas, se diferencian en algo que no nos importa mucho ahora: las listas son objetos mutables y las tuplas son inmutables. La forma literal de crearlas es utilizando paréntesis en vez de corchetes. . tupla = (1, 2, 3) tupla . (1, 2, 3) . Para hacer una selección utilizando más niveles del multi-índice se requiere escribir las etiquetas deseadas dentro de tuplas respetando la jerarquía de niveles, así (nivel 0, nivel 1, ...). . # seleccionamos departamentos en Palermo filas = (&#39;PALERMO&#39;, &#39;departamento&#39;) df.loc[filas].head() . /home/matias/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:4: PerformanceWarning: indexing past lexsort depth may impact performance. after removing the cwd from sys.path. . fecha lat lon precio superficie_total superficie_cubierta ambientes . barrio tipo . PALERMO departamento 2018-04-10 | -34.586787 | -58.416735 | 145000.0 | NaN | NaN | 3.0 | . departamento 2018-04-18 | -34.582436 | -58.414845 | 850000.0 | 270.0 | 240.0 | 6.0 | . departamento 2018-01-26 | -34.578773 | -58.415132 | 330000.0 | 74.0 | 64.0 | 3.0 | . departamento 2018-04-06 | -34.569005 | -58.436241 | 300000.0 | 110.0 | 98.0 | NaN | . departamento 2018-04-06 | -34.582669 | -58.414019 | 360000.0 | 110.0 | 110.0 | 4.0 | . # seleccionamos propiedades en San Telmo y Chacarita que son departamentos filas = ([&#39;SAN TELMO&#39;,&#39;CHACARITA&#39;], &#39;departamento&#39;) # al usar multi-índice es IMPORTANTE especificar también las columnas, aunque no queramos filtrarlas df.loc[filas, :].head() . fecha lat lon precio superficie_total superficie_cubierta ambientes . barrio tipo . CHACARITA departamento 2018-02-07 | -34.585998 | -58.454436 | 190000.0 | 73.0 | 70.0 | 4.0 | . departamento 2018-04-08 | -34.583265 | -58.452488 | 118000.0 | 33.0 | 30.0 | 1.0 | . departamento 2018-04-18 | -34.582635 | -58.451826 | 325000.0 | 86.0 | NaN | 3.0 | . SAN TELMO departamento 2018-03-27 | -34.619606 | -58.374275 | 365000.0 | 140.0 | 140.0 | 3.0 | . CHACARITA departamento 2018-03-02 | -34.587261 | -58.452466 | 158000.0 | 62.0 | 60.0 | 2.0 | . Para seleccionar las propiedades en todos los barrios que son departamentos tenemos que pedir todas las etiquetas del nivel 0 (todos los barrios) y la etiqueta del nivel 1 (departamento). . En principio podríamos pensar lo siguiente, ya que : representaría todas las etiquetas de los barrios. . filas = (:, &#39;departamento&#39;) . Sin embargo esta sintaxis no es correcta: el azúcar sintáctica de : solo sirve cuando está encerrado entre corchetes [:] y en el caso anterior lo está en paréntesis. . Recordemos que : es una forma abreviada de tipear slice(None) y que estas dos formas significan lo mismo. . filas = (slice(None), &#39;departamento&#39;) df.loc[filas, :].head() . fecha lat lon precio superficie_total superficie_cubierta ambientes . barrio tipo . BARRACAS departamento 2018-02-28 | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | . COLEGIALES departamento 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | . VILLA CRESPO departamento 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | . VILLA LURO departamento 2018-03-18 | -34.638511 | -58.504406 | 73000.0 | 29.0 | 26.0 | 1.0 | . VILLA URQUIZA departamento 2018-03-27 | -34.577253 | -58.497460 | 150392.0 | 53.0 | 46.0 | 2.0 | . Pandas provee una manera de recuperar el azúcar sintáctica. . filas = pd.IndexSlice[:, &#39;departamento&#39;] df.loc[filas, :].head() . fecha lat lon precio superficie_total superficie_cubierta ambientes . barrio tipo . BARRACAS departamento 2018-02-28 | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | . COLEGIALES departamento 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | . VILLA CRESPO departamento 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | . VILLA LURO departamento 2018-03-18 | -34.638511 | -58.504406 | 73000.0 | 29.0 | 26.0 | 1.0 | . VILLA URQUIZA departamento 2018-03-27 | -34.577253 | -58.497460 | 150392.0 | 53.0 | 46.0 | 2.0 | . La gente suele escribir menos recurriendo a esta artimaña. . # darle un alias a pd.IndexSlice idx = pd.IndexSlice idx[:, &#39;departamento&#39;] . (slice(None, None, None), &#39;departamento&#39;) . Otra opción que tenemos a mano para seleccionar todos los departamentos, más simple aunque solo sirve para selección y no para asignación, es el método cross-section (xs) que selecciona datos según valor de un índice particular. . df.xs(&#39;departamento&#39;, level=&#39;tipo&#39;, drop_level=False).head() . fecha lat lon precio superficie_total superficie_cubierta ambientes . barrio tipo . BARRACAS departamento 2018-02-28 | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | . COLEGIALES departamento 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | . VILLA CRESPO departamento 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | . VILLA LURO departamento 2018-03-18 | -34.638511 | -58.504406 | 73000.0 | 29.0 | 26.0 | 1.0 | . VILLA URQUIZA departamento 2018-03-27 | -34.577253 | -58.497460 | 150392.0 | 53.0 | 46.0 | 2.0 | . No lo hemos mostrado pero el índice de las columnas también puede ser un multi-índice. . # nos despedimos de los multi-índices por ahora... df.reset_index(inplace=True) . Operaciones vectorizadas . Vector y arreglo son dos términos intercambiables. El título se refiere a operaciones sobre arreglos de NumPy que se realizan elemento a elemento (element-wise) que también están Pandas por estar construido sobre NumPy. Además estas operaciones están preparadas para tratar con valores ausentes. Ninguna de estas dos funcionalidades está presente en Python puro. . a = np.array([2, 2, 2]) b = np.array([1, 2, np.nan]) a * b . array([ 2., 4., nan]) . Nota: Los valores ausentes NA no existen nativamente en Python, como sí existen en R. NumPy lo mitiga usando NaN (not a number) para representarlos. NaN es un valor del tipo float. Como los arreglos de NumPy pueden tener un solo dtype, un arreglo de enteros es promovido a un arreglo de flotantes y un arreglo de booleanos es llevado a uno de objetos (object) en presencia de valores ausentes, ya que por ejemplo, no podríamos tener un arreglo con enteros y NaN (float) simultáneamente. . Los operadores (+, -, ...) terminan invocando a funciones; existen dentro de la sintáxis del lenguaje por practicidad. Cuando alguno de los operandos es un arreglo de NumPy, se invocan funciones de NumPy correspondientes (np.add, np.substract, ...) en vez de funciones de Python puro. También las podemos llamar directamente. . np.multiply(a, b) . array([ 2., 4., nan]) . Como característica, son funciones universales (ufunc), ya que hacen su trabajo elemento a elemento independientemente de las dimensiones de los argumentos (escalar, columna, tabla). . Pandas a su vez implementa las funciones de NumPy como métodos (aunque en la mayor parte del tiempo usamos operadores). . # convertimos de USD a ARS df.precio.multiply(35).head() . 0 4480000.0 1 3010000.0 2 6795250.0 3 8575000.0 4 5001500.0 Name: precio, dtype: float64 . Las operaciones vectorizadas en cualquiera de sus sabores (operadores, funciones, métodos) son ideales para trabajar sobre las columnas existentes y crear nuevas. . df[&#39;precio_m2&#39;] = df.precio / df.superficie_total df.head() . barrio tipo fecha lat lon precio superficie_total superficie_cubierta ambientes precio_m2 . 0 VILLA LURO | ph | 2018-02-23 | -34.640644 | -58.500677 | 128000.0 | 65.0 | 60.0 | NaN | 1969.230769 | . 1 BARRACAS | departamento | 2018-02-28 | -34.643719 | -58.372644 | 86000.0 | 34.0 | 32.0 | 1.0 | 2529.411765 | . 2 COLEGIALES | departamento | 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | 3347.413793 | . 3 VILLA DEVOTO | casa | 2018-02-20 | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | 2187.500000 | . 4 VILLA CRESPO | departamento | 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | 3040.425532 | . Operadores y funciones comunes . Existen operadores como + que requieren dos operandos, llamados operadores binarios y operadores como - (negación) que trabajan con un solo operando, los operadores unarios. . Comparación . == np.equal != np.not_equal &lt; np.less &lt;= np.less_equal &gt; np.greater &gt;= np.greater_equal . Matemáticas . + np.add - np.subtract - np.negate (unario) * np.multiply / np.divide ** np.pow np.sqrt np.exp np.log . Booleanas . &amp; np.bitwise_and | np.bitwise_or ~ np.bitwise_not (unario) . Flotantes . np.round np.isna np.notna . IMPORTANTE: and y or realizan una sola evaluación booleana en el objeto entero, mientras que &amp; y | realizan múltiples evaluaciones sobre el contenido (elementos de la colección) de los objetos. Cuando trabajamos con arreglos, esto último es lo que normalmente queremos. . a = np.array([True, True, False]) b = np.array([True, False, False]) # bitwise and a &amp; b . array([ True, False, False]) . # bitwise or a | b . array([ True, True, False]) . Operaciones con texto . Los strings (str) de Python poseen una gran variedad de métodos muy útiles. Pandas implementa estos métodos y atributos (junto con otros más) como operaciones vectorizadas que se acceden desde el atributo Series.str y DataFrame.str. . frutas = pd.Series([&#39;manzana&#39;, &#39;banana&#39;]) frutas.str.upper() . 0 MANZANA 1 BANANA dtype: object . Operaciones con objetos temporales . Python viene con objetos del tipo fecha (date), hora (time), fecha y hora (datetime) e intervalos temporales (timedelta). Los métodos y atributos de estos objetos aparecen vectorizados bajo el atributo Series.dt y DataFrame.dt. . fechas = pd.Series([&#39;1990-01-01&#39;, &#39;2018-10-31&#39;]) fechas . 0 1990-01-01 1 2018-10-31 dtype: object . # convertimos strings a datetime fechas = pd.to_datetime(fechas) fechas . 0 1990-01-01 1 2018-10-31 dtype: datetime64[ns] . # atributo year de datetime fechas.dt.year . 0 1990 1 2018 dtype: int64 . Nota: NumPy no implementa operaciones con texto y tiempo pero Pandas sí. . Selecci&#243;n usando columnas . En vez de usar el índice de las filas podemos seleccionar filas usando las columnas. Para ello generamos máscaras booleanas usando columnas (que de por sí tienen un largo igual al de la tabla) y operaciones que devuelvan valores booleanos. Las operaciones de comparación son bastante útiles en este contexto ya que nos permiten seleccionar filas según valores en las columnas. . # seleccionar las propiedades con precio menor a $80.000 df[df.precio &lt; 80000].head() . barrio tipo fecha lat lon precio superficie_total superficie_cubierta ambientes precio_m2 . 5 VILLA LURO | departamento | 2018-03-18 | -34.638511 | -58.504406 | 73000.0 | 29.0 | 26.0 | 1.0 | 2517.241379 | . 7 BELGRANO | local | 2018-01-25 | -34.560463 | -58.458433 | 32000.0 | 30.0 | 30.0 | 1.0 | 1066.666667 | . 16 VILLA CRESPO | local | 2018-04-21 | -34.597679 | -58.443019 | 11111.0 | 200.0 | 200.0 | NaN | 55.555000 | . 26 LINIERS | local | 2018-04-19 | -34.638658 | -58.528026 | 19900.0 | 16.0 | 16.0 | NaN | 1243.750000 | . 31 BALVANERA | departamento | 2018-03-25 | -34.612309 | -58.412878 | 54000.0 | 25.0 | 25.0 | 1.0 | 2160.000000 | . IMPORTANTE: Por una cuestión de precedencia de operadores, de querer combinar máscaras usando operaciones bitwise, si las máscaras son creadas usando operadores de comparación, deben ser encerradas entre paréntesis. Esto es porque Python resuelve antes los operadores como &amp; y | que los operadores como &lt; e ==; usamos los paréntesis para indicarle a Python el orden de evaluación pretendido: primero las comparaciones, luego las operaciones bitwise. Como en matemáticas, lo que está dentro de paréntesis debe resolverse primero (tiene precedencia) sobre lo que está afuera. . df[(df.precio &lt; 80000) &amp; (df.ambientes == 3)].head() . barrio tipo fecha lat lon precio superficie_total superficie_cubierta ambientes precio_m2 . 1404 PARQUE PATRICIOS | departamento | 2018-04-28 | -34.629577 | -58.398071 | 78000.0 | 57.0 | 50.0 | 3.0 | 1368.421053 | . 2277 FLORES | departamento | 2018-04-20 | -34.652157 | -58.459236 | 70000.0 | 60.0 | NaN | 3.0 | 1166.666667 | . 2378 VILLA LUGANO | departamento | 2018-03-09 | -34.686170 | -58.466922 | 77000.0 | 70.0 | 62.0 | 3.0 | 1100.000000 | . 3054 BALVANERA | departamento | 2017-12-06 | -34.612801 | -58.406688 | 78000.0 | 44.0 | 44.0 | 3.0 | 1772.727273 | . 3055 VILLA SOLDATI | departamento | 2018-01-08 | -34.668741 | -58.443146 | 52000.0 | 55.0 | 55.0 | 3.0 | 945.454545 | . # en vez de operadores de comparación podemos usar sus métodos equivalentes y ahorrarnos los problemas de precedencia df[df.precio.lt(80000) &amp; df.ambientes.eq(3)].head() . barrio tipo fecha lat lon precio superficie_total superficie_cubierta ambientes precio_m2 . 1404 PARQUE PATRICIOS | departamento | 2018-04-28 | -34.629577 | -58.398071 | 78000.0 | 57.0 | 50.0 | 3.0 | 1368.421053 | . 2277 FLORES | departamento | 2018-04-20 | -34.652157 | -58.459236 | 70000.0 | 60.0 | NaN | 3.0 | 1166.666667 | . 2378 VILLA LUGANO | departamento | 2018-03-09 | -34.686170 | -58.466922 | 77000.0 | 70.0 | 62.0 | 3.0 | 1100.000000 | . 3054 BALVANERA | departamento | 2017-12-06 | -34.612801 | -58.406688 | 78000.0 | 44.0 | 44.0 | 3.0 | 1772.727273 | . 3055 VILLA SOLDATI | departamento | 2018-01-08 | -34.668741 | -58.443146 | 52000.0 | 55.0 | 55.0 | 3.0 | 945.454545 | . Alineamiento autom&#225;tico . En casos en los que intervengan más de una tabla o columna, como cuando queremos añadir una columna nueva a una tabla, podríamos pensar en cuidar aspectos como que la columna tenga el mismo largo que la tabla y que el orden de los elementos en la nueva columna se condiga con el orden de las filas de la tabla. Esto normalmente es así al trabajar con arreglos de NumPy. . Afortunadamente nada de esto debe preocuparnos al usar Pandas. Los índices cumplen una función de gran utilidad en las operaciones y en las asignaciones, donde les elementos son alianeados según sus índices para que todo salga como lo esperamos. . Operaciones . algunas_frutas = pd.Series([&#39;🥝&#39;, &#39;🥥&#39;, &#39;🍇&#39;, &#39;🍌&#39;, &#39;🍎&#39;]) algunas_frutas.index = [&#39;kiwi&#39;, &#39;coco&#39;, &#39;uva&#39;, &#39;banana&#39;, &#39;manzana&#39;] algunas_frutas . kiwi 🥝 coco 🥥 uva 🍇 banana 🍌 manzana 🍎 dtype: object . otras_frutas = pd.Series([&#39;🍇&#39;, &#39;🍐&#39;, &#39;🍎&#39;]) otras_frutas.index = [&#39;uva&#39;, &#39;pera&#39;, &#39;manzana&#39;] otras_frutas . uva 🍇 pera 🍐 manzana 🍎 dtype: object . # operación suma entre dos Series que no tienen el mismo largo ni coinciden en todas las etiquetas algunas_frutas + otras_frutas . banana NaN coco NaN kiwi NaN manzana 🍎🍎 pera NaN uva 🍇🍇 dtype: object . El índice de la Series resultante tiene todas las etiquetas de ambos operandos, con los valores resultantes de la operación. Notar que los elementos no estaban alineados. . Nota: Cuando alguno de los operandos es NaN el resultado de la operación es NaN. . Asignaci&#243;n . # convertimos a una Series en un DataFrame de una columna y nombramos a esa columna dg = algunas_frutas.to_frame(name=&#39;algunas&#39;) dg . algunas . kiwi 🥝 | . coco 🥥 | . uva 🍇 | . banana 🍌 | . manzana 🍎 | . dg[&#39;otras&#39;] = otras_frutas dg . algunas otras . kiwi 🥝 | NaN | . coco 🥥 | NaN | . uva 🍇 | 🍇 | . banana 🍌 | NaN | . manzana 🍎 | 🍎 | . Se respeta el índice de la tabla, es decir no se le agregan las etiquetas que otras_frutas tiene de más. Los valores se asignan alineados y los valores con etiquetas no presentes en otras_frutas quedan ausentes en la nueva columna. . M&#225;scaras . La alineación automática funciona aún en el indexado, siempre y cuando la máscara sea una Series y su índice coincida con el del objeto sobre el cual se indexa, lo que suele ser el caso cuando trabajamos con las columnas del DataFrame. Lo que veremos a continuación no funcionaría con una máscara que sea un arreglo de NumPy ya que carece de índice. . # seleccionamos propiedades de dos ambientes máscara = df.ambientes.eq(2) máscara.head() . 0 False 1 False 2 True 3 True 4 True Name: ambientes, dtype: bool . # dejamos un testimonio del resultado df[máscara].head() . barrio tipo fecha lat lon precio superficie_total superficie_cubierta ambientes precio_m2 . 2 COLEGIALES | departamento | 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | 3347.413793 | . 3 VILLA DEVOTO | casa | 2018-02-20 | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | 2187.500000 | . 4 VILLA CRESPO | departamento | 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | 3040.425532 | . 6 VILLA URQUIZA | departamento | 2018-03-27 | -34.577253 | -58.497460 | 150392.0 | 53.0 | 46.0 | 2.0 | 2837.584906 | . 15 NUÑEZ | departamento | 2017-11-21 | -34.540443 | -58.473773 | 213500.0 | 48.0 | 44.0 | 2.0 | 4447.916667 | . # reordenamos la Series de atrás para adelante, lo que se conoce como revertir (reverse) # (no hace falta entender esta línea en este momento) al_revés = máscara[::-1] # el orden de esta máscara es diferente al de la anterior al_revés.head() . 24257 True 24256 False 24255 False 24254 False 24253 False Name: ambientes, dtype: bool . # corroboramos que el resultado es el mismo df[al_revés].head() . /home/matias/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index. . barrio tipo fecha lat lon precio superficie_total superficie_cubierta ambientes precio_m2 . 2 COLEGIALES | departamento | 2018-02-07 | -34.573595 | -58.442591 | 194150.0 | 58.0 | 48.0 | 2.0 | 3347.413793 | . 3 VILLA DEVOTO | casa | 2018-02-20 | -34.611046 | -58.503044 | 245000.0 | 112.0 | 112.0 | 2.0 | 2187.500000 | . 4 VILLA CRESPO | departamento | 2018-04-11 | -34.605809 | -58.453481 | 142900.0 | 47.0 | 40.0 | 2.0 | 3040.425532 | . 6 VILLA URQUIZA | departamento | 2018-03-27 | -34.577253 | -58.497460 | 150392.0 | 53.0 | 46.0 | 2.0 | 2837.584906 | . 15 NUÑEZ | departamento | 2017-11-21 | -34.540443 | -58.473773 | 213500.0 | 48.0 | 44.0 | 2.0 | 4447.916667 | . Nota: Que el indexado haga uso de las etiquetas de la máscara es la razón por la cuál las máscaras funcionan en el indexado por etiquetas (loc) y no en el indexado por posición (iloc). . Ejemplo integrador . El siguiente ejemplo asigna a aquellos avisos que carecen de superficie cubierta, su superficie total. . df.loc[df.superficie_cubierta.isnull(), &#39;superficie_cubierta&#39;] = df.superficie_total . loc para indexar por filas y columnas; necesitamos indexar por etiquetas para hacer referencia a la columna sobre la cual hacer la asignación por su nombre. | Creamos una máscara usando la columna superficie_cubierta y la operación vectorizada isnull, que servirá para seleccionar aquellas filas en las que superficie_cubierta es NaN. | Asignamos los valores presentes en la columna superficie_total y no nos preocupamos por hacer una selección (como la del punto anterior) sobre estos valores ya que la alineación automática se encargará de alinear las filas de ambos lados de la asignación por su etiqueta. | Visualizaci&#243;n b&#225;sica . Ver más: http://pandas.pydata.org/pandas-docs/stable/visualization.html . # activar gráficos en Jupyter; hace falta correrlo una vez %matplotlib inline . Tanto Series como DataFrames tienen un atributo llamado plot que habilita los siguientes métodos para distintos gráficos típicos. . line y area para líneas y líneas sólidas. | bar y barh para barras verticales y horizontales. | pie para tortas. | scatter para puntos. | hist y hexbin para histogramas e histogramas 2D. | density o kde para densidad de probabilidad. | box para boxplot. | . IMPORTANTE: Cuando graficamos en base a un DataFrame, cada columna se interpreta como una variable. . df.precio.plot.hist(bins=1000, xlim=(0,500000)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f825e369fd0&gt; . Actividad de investigaci&#243;n . Consultar la ayuda o la web para saber más de los métodos que se encuentran a continuación. Por ejemplo: . help(pd.DataFrame.sort_values) . Help on function sort_values in module pandas.core.frame: sort_values(self, by, axis=0, ascending=True, inplace=False, kind=&#39;quicksort&#39;, na_position=&#39;last&#39;, ignore_index=False) Sort by the values along either axis. Parameters - by : str or list of str Name or list of names to sort by. - if `axis` is 0 or `&#39;index&#39;` then `by` may contain index levels and/or column labels. - if `axis` is 1 or `&#39;columns&#39;` then `by` may contain column levels and/or index labels. .. versionchanged:: 0.23.0 Allow specifying index or column level names. axis : {0 or &#39;index&#39;, 1 or &#39;columns&#39;}, default 0 Axis to be sorted. ascending : bool or list of bool, default True Sort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by. inplace : bool, default False If True, perform operation in-place. kind : {&#39;quicksort&#39;, &#39;mergesort&#39;, &#39;heapsort&#39;}, default &#39;quicksort&#39; Choice of sorting algorithm. See also ndarray.np.sort for more information. `mergesort` is the only stable algorithm. For DataFrames, this option is only applied when sorting on a single column or label. na_position : {&#39;first&#39;, &#39;last&#39;}, default &#39;last&#39; Puts NaNs at the beginning if `first`; `last` puts NaNs at the end. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, …, n - 1. .. versionadded:: 1.0.0 Returns - sorted_obj : DataFrame or None DataFrame with sorted values if inplace=False, None otherwise. Examples -- &gt;&gt;&gt; df = pd.DataFrame({ ... &#39;col1&#39;: [&#39;A&#39;, &#39;A&#39;, &#39;B&#39;, np.nan, &#39;D&#39;, &#39;C&#39;], ... &#39;col2&#39;: [2, 1, 9, 8, 7, 4], ... &#39;col3&#39;: [0, 1, 9, 4, 2, 3], ... }) &gt;&gt;&gt; df col1 col2 col3 0 A 2 0 1 A 1 1 2 B 9 9 3 NaN 8 4 4 D 7 2 5 C 4 3 Sort by col1 &gt;&gt;&gt; df.sort_values(by=[&#39;col1&#39;]) col1 col2 col3 0 A 2 0 1 A 1 1 2 B 9 9 5 C 4 3 4 D 7 2 3 NaN 8 4 Sort by multiple columns &gt;&gt;&gt; df.sort_values(by=[&#39;col1&#39;, &#39;col2&#39;]) col1 col2 col3 1 A 1 1 0 A 2 0 2 B 9 9 5 C 4 3 4 D 7 2 3 NaN 8 4 Sort Descending &gt;&gt;&gt; df.sort_values(by=&#39;col1&#39;, ascending=False) col1 col2 col3 4 D 7 2 5 C 4 3 2 B 9 9 0 A 2 0 1 A 1 1 3 NaN 8 4 Putting NAs first &gt;&gt;&gt; df.sort_values(by=&#39;col1&#39;, ascending=False, na_position=&#39;first&#39;) col1 col2 col3 3 NaN 8 4 4 D 7 2 5 C 4 3 2 B 9 9 0 A 2 0 1 A 1 1 . Ordenar valores . sort_values | sort_index | . Tirar filas/columnas . drop | drop_duplicates | . Renombrar filas/columnas . rename | . Datos ausentes . dropna | fillna | . Recursos adicionales . Documentaci&#243;n . API | . Art&#237;culos . Intro to Pandas | Modern Pandas | . Machetes . DataQuest cheat sheet | DataCamp cheat sheet | Pandas cheat sheet | . Libros . Python for data science | Python for data analysis | .",
            "url": "https://matiasbattocchia.github.io/datitos/Pandas-parte-1.html",
            "relUrl": "/Pandas-parte-1.html",
            "date": " • Nov 11, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Acerca de",
          "content": "Mi nombre es Matías Battocchia. Estudié en Exactas (UBA). Trabajo en Mutt Data. Vivo en Mendoza, Argentina. . Organizador de encuentros de ciencia de datos. . Créditos . El sitio está hecho con fastpages. | Los emojis vienen de aquí. | Logo y favicon vienen de acá. | .",
          "url": "https://matiasbattocchia.github.io/datitos/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://matiasbattocchia.github.io/datitos/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}