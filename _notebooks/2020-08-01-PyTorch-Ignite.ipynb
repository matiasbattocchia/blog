{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de cómo usar Ignite\n",
    "\n",
    "> \"/ᐠ｡‸｡ᐟ\\\\ Una librería de alto nivel que facilita el entrenamiento y la evaluación de redes neuronales en PyTorch\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Matías Battocchia\n",
    "- categories: [pytorch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recientemente han salido varias librerías de alto nivel para PyTorch. Una vez planteados los datasets y los modelos todavía queda bastante por programar, sobre todo los bucles de entrenamiento y validación, código que salvando detalles de implementación es siempre el mismo (*boilerplate*). Librerías como [PyTorch Lighting](https://pytorch-lightning.readthedocs.io) y [PyTorch Ignite](https://pytorch.org/ignite) prometen ahorrarnos el código repetitivo y concentrarnos en lo particular.\n",
    "\n",
    "Se instala con\n",
    "\n",
    "```bash\n",
    "pip install pytorch-ignite\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Accuracy, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el triángulo modelo-optimizador-criterio. Lo de siempre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Modelo().to(device)\n",
    "\n",
    "optimizador = torch.optim.Adam(modelo.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "criterio = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bucle de entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar(motor, lote):\n",
    "    modelo.train()\n",
    "\n",
    "    optimizador.zero_grad()\n",
    "    \n",
    "    predicciones = modelo(lote.documentos.to(device))\n",
    "    pérdida = criterio(predicciones, lote.etiquetas.to(device))\n",
    "    \n",
    "    pérdida.backward()\n",
    "    optimizador.step()\n",
    "    \n",
    "    return pérdida.item()\n",
    "\n",
    "\n",
    "entrenador = Engine(entrenar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bucle de evaluación**. Sirve para todo lo que no es entrenamiento, a ser validación e inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar(motor, lote):\n",
    "    modelo.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicciones = modelo(lote.documentos.to(device))\n",
    "    \n",
    "    # validación\n",
    "    if lote.etiquetas is not None:\n",
    "        return predicciones, lote.etiquetas.to(device)\n",
    "    \n",
    "    # inferencia\n",
    "    return predicciones\n",
    "\n",
    "\n",
    "evaluador = Engine(evaluar)\n",
    "inferidor = Engine(evaluar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjuntamos algunas métricas al evaluador. Notar que `entrenar` devuelve el valor de la función de pérdida mientras que `evaluar` devuelve predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy().attach(evaluador, 'accuracy')\n",
    "Loss(criterio).attach(evaluador, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Evento**: al completar una época de entrenamiento.\n",
    "* **Acción**: registrar métricas del dataset de **entrenamiento**. Para ello usamos `evaluador.run(train_dl)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@entrenador.on(Events.EPOCH_COMPLETED)\n",
    "def loguear_resultados_entrenamiento(entrenador):\n",
    "    evaluador.run(train_dl)\n",
    "    \n",
    "    # accedemos a este atributo gracias a haber adjuntado métricas previamente\n",
    "    métricas = evaluador.state.metrics\n",
    "    \n",
    "    print(f\"[{entrenador.state.epoch:02}] TRAIN  Accuracy: {métricas['accuracy']:.2f}  Loss: {métricas['loss']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Evento**: al completar una época de entrenamiento. *Acá podría ser al completar X épocas para no validar tan seguido*.\n",
    "* **Acción**: registrar métricas del dataset de **validación**. Para ello usamos `evaluador.run(valid_dl)`.\n",
    "\n",
    "Esta función podría hacer sido más parecida a la de arriba pero no lo es porque queremos calcular una métrica que no viene con *Ignite*. Tenemos dos opciones, definir una métrica adjuntable como `ignite.metrics.Accuracy` —que no lo hicimos y quizás hubiese sido lo mejor— o poner la lógica en la función, como vemos aquí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "@entrenador.on(Events.EPOCH_COMPLETED)\n",
    "def loguear_resultados_validación(entrenador):\n",
    "    # esta artimaña tendrá sentido más adelante\n",
    "    evaluador.predicciones = []\n",
    "    evaluador.etiquetas = []\n",
    "    \n",
    "    evaluador.run(valid_dl)\n",
    "    \n",
    "    # de todas las categorías nos quedamos con la más probable para cada muestra\n",
    "    predicciones = torch.cat(evaluador.predicciones).argmax(dim=1).cpu()\n",
    "    etiquetas    = torch.cat(evaluador.etiquetas).cpu()\n",
    "    \n",
    "    score = balanced_accuracy_score(etiquetas, predicciones)\n",
    "    \n",
    "    métricas = evaluador.state.metrics\n",
    "    \n",
    "    print(f\"[{entrenador.state.epoch:02}] VALID  Accuracy: {métricas['accuracy']:.2f}  Loss: {métricas['loss']:.2f}  Balanced accuracy: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Evento**: al procesar un lote de validación.\n",
    "* **Acción**: almacenar predicciones y etiquetas.\n",
    "\n",
    "Esto le da sentido a la artimaña que mencionamos. La misma sirve para instanciar listas vacías al inicio de la validación, a las que se le agregaran los resultados de cada lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "@evaluador.on(Events.ITERATION_COMPLETED)\n",
    "def colectar_validaciones_lote(evaluador):\n",
    "    evaluador.predicciones.append(evaluador.state.output[0])\n",
    "    evaluador.etiquetas.append(evaluador.state.output[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Evento**: al completar el entrenamiento (todas las épocas).\n",
    "* **Acción**: realizar inferencias. Para ello usamos `inferidor.run(infer_dl)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@entrenador.on(Events.COMPLETED)\n",
    "def colectar_inferencias(entrenador):\n",
    "    print('Realizando inferencias...')\n",
    "    \n",
    "    # mismo truco de antes\n",
    "    inferidor.y_pred = []\n",
    "    \n",
    "    inferidor.run(infer_dl)\n",
    "    \n",
    "    # de todas las categorías nos quedamos con la más probable para cada muestra\n",
    "    y_pred = torch.cat(inferidor.y_pred).argmax(dim=1).reshape(-1,1)\n",
    "    \n",
    "    # quizás sea un buen momento para recuperar las categorías originales\n",
    "    #y_pred = vocabulario_etiquetas.índices_a_tókenes(y_pred)\n",
    "    \n",
    "    # ya que estamos, guardamos los resultados en un CSV\n",
    "    pd.DataFrame(y_pred).to_csv('submit.csv', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Evento**: al procesar un lote de inferencia.\n",
    "* **Acción**: almacenar predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inferidor.on(Events.ITERATION_COMPLETED)\n",
    "def colectar_inferencias_lote(inferidor):\n",
    "    inferidor.y_pred.append(inferidor.state.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente largamos el entrenamiento con `entrenador.run(train_dl)`. Este es el engranaje principal del mecanismo, que al completar bucles moverá a los otros engranajes (`Engine`s, que no son literalmente enganajes pero puede que sea una buena metáfora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] VALID  Accuracy: 0.80  Loss: 1.11  Balanced accuracy: 0.75\n",
      "[01] TRAIN  Accuracy: 0.98  Loss: 0.02\n",
      "[02] VALID  Accuracy: 0.80  Loss: 1.11  Balanced accuracy: 0.75\n",
      "[02] TRAIN  Accuracy: 0.98  Loss: 0.02\n",
      "[03] VALID  Accuracy: 0.80  Loss: 1.11  Balanced accuracy: 0.75\n",
      "[03] TRAIN  Accuracy: 0.98  Loss: 0.02\n",
      "[04] VALID  Accuracy: 0.80  Loss: 1.11  Balanced accuracy: 0.75\n",
      "[04] TRAIN  Accuracy: 0.98  Loss: 0.02\n",
      "[05] VALID  Accuracy: 0.80  Loss: 1.11  Balanced accuracy: 0.75\n",
      "[05] TRAIN  Accuracy: 0.98  Loss: 0.02\n",
      "Realizando inferencias...\n"
     ]
    }
   ],
   "source": [
    "entrenador.run(train_dl, max_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos\n",
    "\n",
    "* [8 Creators and Core Contributors Talk About Their Model Training Libraries From PyTorch Ecosystem](https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
