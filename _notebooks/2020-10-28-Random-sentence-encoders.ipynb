{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random encoders for sentence classification\n",
    "> \"üê± Codificadores aleatorios para embeddings de oraciones\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Mat√≠as Battocchia\n",
    "- categories: [nlp,charla,paper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Charla basada en este paper:\n",
    "\n",
    "https://arxiv.org/pdf/1901.10444.pdf\n",
    "\n",
    "> A complex pattern-classification problem, cast in a high-dimensional space nonlinearly, is more likely to be linearly separable than in a low-dimensional space, provided that the space is not densely populated.\n",
    "\n",
    "‚Äî‚ÄâCover, T. M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embeddings\n",
    "\n",
    "Palabra $\\rightarrow$ t√≥ken.\n",
    "\n",
    "Embedding: representaci√≥n de densa y de baja dimensionalidad de un t√≥ken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Aproximaciones no supervisadas basadas en la [hip√≥tesis distribucional](https://en.wikipedia.org/wiki/Distributional_semantics): palabras que ocurren en el mismo contexto tienden a tener significados similares.\n",
    "\n",
    "![](images/word_embedding1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Word embeddings pre-entrenados:\n",
    "* word2vec\n",
    "* GloVe\n",
    "* fastText\n",
    "* ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentence embeddings\n",
    "\n",
    "Oraci√≥n $\\rightarrow$ documento.\n",
    "\n",
    "T√©cnica sencilla y aceptable: **max** o **mean** de los t√≥kenes del documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La intenci√≥n es usar un clasificador sobre los embeddings de documentos (*downstream task*).\n",
    "\n",
    "![](images/sentence_embedding2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "O simplemente una medida de similaridad.\n",
    "\n",
    "![](images/sentence_embedding1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tareas y datasets\n",
    "\n",
    "https://arxiv.org/pdf/1705.02364.pdf\n",
    "\n",
    "#### Clasificaci√≥n\n",
    "\n",
    "* sentiment analysis (MR, SST),\n",
    "* product reviews (CR),\n",
    "* subjectivity (SUBJ),\n",
    "* opinion polarity (MPQA),\n",
    "* question-type (TREC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/senteval1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Inferencia y similaridad sem√°ntica\n",
    "\n",
    "* entailment (SNLI, SICK-E),\n",
    "* semantic relatedness (SICK-R, STS),\n",
    "* paraphrasing (MRPC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/senteval2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoders entrenados\n",
    "\n",
    "$h = f_Œ∏(e_1, \\ldots, e_n)$\n",
    "\n",
    "* Interesa obtener una representaci√≥n $h$ de una **oraci√≥n**,\n",
    "* usando alguna funci√≥n $f$ parametrizada por $Œ∏$,\n",
    "* en funci√≥n de embeddings pre-entrenados $e$ donde $e_i$ es la representaci√≥n de la i-√©sima **palabra** en una oraci√≥n de largo $n$.\n",
    "\n",
    "T√≠picamente los codificadores aprenden $Œ∏$, par√°metros que luego se mantien fijos en las tareas de transferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### InferSent\n",
    "\n",
    "https://arxiv.org/abs/1705.02364\n",
    "\n",
    "Supervisado usando el corpus Stanford Natural Language Inference (SNLI). Requiere una gran cantidad de anotaciones.\n",
    "\n",
    "![](images/infersent1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/infersent2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Skip-Thought\n",
    "\n",
    "https://arxiv.org/abs/1506.06726\n",
    "\n",
    "No supervisado. En vez de predecir las palabras que envuelven a una palabra (*skip-gram*), predice las oraciones alrededor de una oraci√≥n dada. Entrenarlo lleva un tiempo muy largo.\n",
    "\n",
    "![](images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/skip-thought.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random encoders\n",
    "\n",
    "Diferentes maneras de parametrizar $f$ para representar el significado de oraciones sin ning√∫n entrenamiento de $Œ∏$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bag of random embedding projections\n",
    "\n",
    "$X = (e_1, \\ldots, e_n)$\n",
    "\n",
    "* $X ‚àà \\mathbb{R}^{n√óD}$.\n",
    "* $n$ es el tama√±o del documento, $D$ es la dimensi√≥n de los *word embeddings*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$h = f_{\\text{pool}}(X W)$\n",
    "\n",
    "* $W ‚àà \\mathbb{R}^{D√ód}$ se inicializa al azar usando una distribuci√≥n uniforme entre $[\\frac{‚àí1}{\\sqrt{d}}, \\frac{1}{\\sqrt{d}}]$.\n",
    "* $D$ es la dimensi√≥n de los *word embeddings*, $d$ es la dimensi√≥n de la proyecci√≥n.\n",
    "* $f_{\\text{pool}} = \\text{max}$ (*max pooling*) o $f_{\\text{pool}} = \\text{mean}$ (*mean pooling*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Random LSTMs\n",
    "\n",
    "$h =  f_{\\text{pool}}(\\text{BiLSTM}(e_1, \\ldots, e_n))$\n",
    "\n",
    "* Los pesos se inicializan al azar usando una distribuci√≥n uniforme entre $[\\frac{‚àí1}{\\sqrt{d}}, \\frac{1}{\\sqrt{d}}]$.\n",
    "* $d$ es la dimensi√≥n del estado oculto de la LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Echo state networks (ESNs)\n",
    "\n",
    "![](images/esn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/esn2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$(\\hat y_1, \\ldots, \\hat y_n) = \\text{ESN}(e_1, \\ldots, e_n)$\n",
    "\n",
    "Descripci√≥n formal de una ESN:\n",
    "\n",
    "$\\tilde h_i = f_{\\text{act}} (W^i e_i + W^h h_{i‚àí1} + b^i)$\n",
    "\n",
    "$h_i = (1‚àíŒ±) h_{i‚àí1} + Œ± \\tilde h_i$\n",
    "\n",
    "* $W^i$, $W^h$, $b^i$ son inicializados al azar y no se actualizan durante el entrenamiento.\n",
    "* $Œ± ‚àà (0,1]$ es el grado de mezcla entre el estado previo y el actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\hat y_i = W^o [e_i;h_i] + b^o$\n",
    "\n",
    "* $W^o$, $b^o$ son los √∫nicos parametros que se entrenan.\n",
    "* $\\hat y_i$ es la predicci√≥n para $y_i$.\n",
    "* **NO SE USA**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$h = f_{\\text{pool}}(\\text{BiESN}(e_1, \\ldots, e_n))$\n",
    "\n",
    "* Se utiliza una ESN bidireccional, los estados del reservorio de ambas direcciones se concatenan $h_i = [\\overrightarrow{h_i};\\overleftarrow{h_i}]$.\n",
    "* Mediante *pooling* de estos estados se obtiene la representaci√≥n de la oraci√≥n $h$.\n",
    "\n",
    "La *echo state property* clama que el estado del reservorio debe ser √∫nicamente determinada por la historia de entrada y que los efectos de un estado dado deben disminuir en favor de estados m√°s recientes. En la pr√°ctica esta propiedad se satisface asegurando que el valor absoluto del autovalor m√°s grande de $W^h$ sea menor que 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resultados\n",
    "\n",
    "![](images/random.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parte 2: C√≥digo\n",
    "\n",
    "https://github.com/dair-ai/emotion_dataset\n",
    "\n",
    "    sadness üò¢\n",
    "    joy üòÉ\n",
    "    love ü•∞\n",
    "    anger üò°\n",
    "    fear üò±\n",
    "    surprise üòØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         141067\n",
       "sadness     121187\n",
       "anger        57317\n",
       "fear         47712\n",
       "love         34554\n",
       "surprise     14972\n",
       "Name: emotions, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('max_colwidth', 400)\n",
    "\n",
    "df = pd.read_pickle('datasets/emotions.pkl')\n",
    "\n",
    "df.emotions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SADNESS\n",
      "* i feel shitty about my looks which makes me feel shitty as a person\n",
      "* i started feeling a few things here and there under me feet or when something messy\n",
      "* i put it aside feeling a little defeated\n",
      "* i feel devastated when i fail\n",
      "* i shut it down reminding myself that i have no time for this feeling burdened by the compulsion to do something about all my thoughts\n",
      "\n",
      "\n",
      "JOY\n",
      "* i feel like all of the colors put together look cool even if they arent realistic\n",
      "* i feel almost too trusting\n",
      "* i feel hopeful somehow and like i am climbing back up from this pit\n",
      "* i feel super happy when i see other people going off for a holiday\n",
      "* i start out feeling very confident positive about my choices and way more together than the stammering person i just painted myself into a second ago i often get this kind of doomsday response\n",
      "\n",
      "\n",
      "LOVE\n",
      "* i walk by animal stores or see people walking their pets i feel a sense of longing for my own animals\n",
      "* i were cool but sometimes i had this gut feeling that she wasn t fond of me\n",
      "* i expect to beaten down to give until i feel as if i can give no more to love without being loved always to continually pray to feel pain for my children and because of my children\n",
      "* i feel like i am expending a lot of effort in supporting them with very little return emotional support\n",
      "* i thought it would be a good time to check in on weasel nation to see how they were feeling about their donut loving coach and their floundering football team\n",
      "\n",
      "\n",
      "ANGER\n",
      "* when some seniors tried to scold and insult some juniors on account of what the juniors were supposed to have said at secondary school\n",
      "* i feel so angry with the person that i have lost and i feel like it is going to consume me at times\n",
      "* i feel a little greedy about these books i got in the mail today\n",
      "* i can really feel those people who insulted the other races\n",
      "* i feels like the type of people who would not bother with such petty crimes but that is what i said about grell beforehand\n",
      "\n",
      "\n",
      "FEAR\n",
      "* i could just embrace feeling weird instead of clinging to what i think is normal\n",
      "* im starting to feel really nervous about all the work that has to be done in the new house john says why\n",
      "* i got so used to the pain that it actually feels weird to be up and functioning instead of being in the usual fetal position\n",
      "* i began to feel very shy and unable to concentrate on my words\n",
      "* im beginning to feel unsure about my current relationship after catching up with my friend jen who was at socc and heard all about her experiences abroad has made wonder what i am doing\n",
      "\n",
      "\n",
      "SURPRISE\n",
      "* i feel fuckin dazed\n",
      "* i feel dazed when im with him\n",
      "* i feel like falling in love with her is part of being amazed at how she makes our family so much better she tells the advocate\n",
      "* i really feel amazed and ashamed at the same time when people say that such a move wont end things the way they are and wont mark a new beginning\n",
      "* i have always had an issue with my weight and stomach fat so this feels weird\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for emotion in df.emotions.unique():\n",
    "    sample = df.query(f'emotions == @emotion').sample(5)\n",
    "    \n",
    "    print(emotion.upper())\n",
    "    \n",
    "    for _, text in sample.text.items():\n",
    "        print('* ' + text)\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Revisando las muestras nos damos cuentas de que es un dataset bastante pol√©mico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tokenizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'was', 'feeling', 'a', 'little', 'low', 'few', 'days', 'back']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [doc.split() for doc in df.text]\n",
    "\n",
    "docs[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Indexaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# versi√≥n 6\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Vocab():\n",
    "    @property\n",
    "    def √≠ndice_relleno(self):\n",
    "        return self.mapeo.get(self.t√≥ken_relleno)\n",
    "    \n",
    "    def __init__(self, t√≥ken_desconocido='<unk>', t√≥ken_relleno='<pad>', frecuencia_m√≠nima=0.0, frecuencia_m√°xima=1.0,\n",
    "                 longitud_m√≠nima=1, longitud_m√°xima=np.inf, stop_words=[], l√≠mite_vocabulario=None):\n",
    "        \n",
    "        self.t√≥ken_desconocido = t√≥ken_desconocido\n",
    "        self.t√≥ken_relleno = t√≥ken_relleno\n",
    "        self.frecuencia_m√≠nima = frecuencia_m√≠nima\n",
    "        self.frecuencia_m√°xima = frecuencia_m√°xima\n",
    "        self.longitud_m√≠nima = longitud_m√≠nima\n",
    "        self.longitud_m√°xima = longitud_m√°xima\n",
    "        self.stop_words = stop_words\n",
    "        self.l√≠mite_vocabulario = l√≠mite_vocabulario\n",
    "    \n",
    "    def reducir_vocabulario(self, lote):\n",
    "        contador_absoluto = Counter(chain(*lote))\n",
    "        \n",
    "        contador_documentos = Counter()\n",
    "        \n",
    "        for doc in lote:\n",
    "            contador_documentos.update(set(doc))\n",
    "        \n",
    "        # frecuencia m√≠nima\n",
    "        if isinstance(self.frecuencia_m√≠nima, int): # frecuencia de t√≥ken\n",
    "            vocabulario_m√≠n = [t√≥ken for t√≥ken, frecuencia in contador_absoluto.most_common() if frecuencia >= self.frecuencia_m√≠nima]\n",
    "        else: # frecuencia de documento\n",
    "            vocabulario_m√≠n = [t√≥ken for t√≥ken, frecuencia in contador_documentos.most_common() if frecuencia/len(lote) >= self.frecuencia_m√≠nima]\n",
    "        \n",
    "        # frecuencia m√°xima\n",
    "        if isinstance(self.frecuencia_m√°xima, int): # frecuencia de t√≥ken\n",
    "            vocabulario_m√°x = [t√≥ken for t√≥ken, frecuencia in contador_absoluto.most_common() if self.frecuencia_m√°xima >= frecuencia]\n",
    "        else: # frecuencia de documento\n",
    "            vocabulario_m√°x = [t√≥ken for t√≥ken, frecuencia in contador_documentos.most_common() if self.frecuencia_m√°xima >= frecuencia/len(lote)]\n",
    "\n",
    "        # intersecci√≥n de vocabulario_m√≠n y vocabulario_m√°x preservando el √≥rden\n",
    "        if len(vocabulario_m√≠n) == len(vocabulario_m√°x):\n",
    "            vocabulario = vocabulario_m√≠n\n",
    "        else:\n",
    "            vocabulario = [t√≥ken for t√≥ken in tqdm(vocabulario_m√≠n, 'Procesando documentos') if t√≥ken in vocabulario_m√°x]\n",
    "\n",
    "        # longitud\n",
    "        vocabulario = [t√≥ken for t√≥ken in vocabulario if self.longitud_m√°xima >= len(t√≥ken) >= self.longitud_m√≠nima]\n",
    "        \n",
    "        # stop words\n",
    "        vocabulario = [t√≥ken for t√≥ken in vocabulario if t√≥ken not in self.stop_words]\n",
    "        \n",
    "        # l√≠mite\n",
    "        vocabulario = vocabulario[:self.l√≠mite_vocabulario]\n",
    "        \n",
    "        return vocabulario\n",
    "        \n",
    "    def fit(self, lote):\n",
    "        vocabulario = []\n",
    "        \n",
    "        if self.t√≥ken_relleno:\n",
    "            vocabulario.append(self.t√≥ken_relleno)\n",
    "        \n",
    "        if self.t√≥ken_desconocido:\n",
    "            vocabulario.append(self.t√≥ken_desconocido)\n",
    "        \n",
    "        vocabulario += self.reducir_vocabulario(lote)\n",
    "        \n",
    "        self.mapeo = {t√≥ken: √≠ndice for √≠ndice, t√≥ken in enumerate(vocabulario)}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, lote):\n",
    "        if self.t√≥ken_desconocido: # reemplazar\n",
    "            return [[t√≥ken if t√≥ken in self.mapeo else self.t√≥ken_desconocido for t√≥ken in doc] for doc in lote]\n",
    "        else: # ignorar\n",
    "            return [[t√≥ken for t√≥ken in doc if t√≥ken in self.mapeo] for doc in lote]\n",
    "    \n",
    "    def t√≥kenes_a_√≠ndices(self, lote):\n",
    "        lote = self.transform(lote)\n",
    "        \n",
    "        return [[self.mapeo[t√≥ken] for t√≥ken in doc] for doc in lote]\n",
    "    \n",
    "    def √≠ndices_a_t√≥kenes(self, lote):\n",
    "        mapeo_inverso = list(self.mapeo.keys())\n",
    "        \n",
    "        return [[mapeo_inverso[√≠ndice] for √≠ndice in doc] for doc in lote]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mapeo)\n",
    "    \n",
    "    @property\n",
    "    def vocabulario(self):\n",
    "        return list(v.mapeo.keys())\n",
    "\n",
    "    def obtener_embeddings(self, fastText):\n",
    "\n",
    "        embeddings = [\n",
    "            fastText[t√≥ken] for t√≥ken in self.vocabulario\n",
    "            if t√≥ken not in (self.t√≥ken_desconocido, self.t√≥ken_relleno)\n",
    "        ]\n",
    "        \n",
    "        embeddings = torch.stack( list( map(torch.tensor, embeddings) ) )\n",
    "\n",
    "        if self.t√≥ken_desconocido:\n",
    "            unk = embeddings.mean(dim=0, keepdim=True)\n",
    "            embeddings = torch.cat([unk, embeddings])\n",
    "\n",
    "        if self.t√≥ken_relleno:\n",
    "            pad = torch.zeros(1, fastText.get_dimension())\n",
    "            embeddings = torch.cat([pad, embeddings])\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75302"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = Vocab(t√≥ken_desconocido=None, t√≥ken_relleno=None)\n",
    "\n",
    "v.fit(docs)\n",
    "\n",
    "len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 23, 5, 6, 53, 409, 187, 162, 98]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.t√≥kenes_a_√≠ndices([['i', 'was', 'feeling', 'a', 'little', 'low', 'few', 'days', 'back']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representaciones pre-entrenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75302, 300])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = v.obtener_embeddings(ft)\n",
    "\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "idxs = v.t√≥kenes_a_√≠ndices(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 300])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i was feeling little low few days back\n",
    "\n",
    "x = e[ idxs[3] ]\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representaciones de oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = 300\n",
    "d = 512\n",
    "\n",
    "w = torch.empty(D, d)\n",
    "\n",
    "w = torch.nn.init.uniform_(w, -1/np.sqrt(d), 1/np.sqrt(d))\n",
    "\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xw = torch.mm(x, w)\n",
    "\n",
    "xw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xw.max(dim=0).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 416809/416809 [01:59<00:00, 3493.89it/s]\n"
     ]
    }
   ],
   "source": [
    "s = torch.stack( [ torch.mm(e[doc], w).max(dim=0).values for doc in tqdm(idxs) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representaciones de emociones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "emo = [\n",
    "    ['sadness'],\n",
    "    ['joy'],\n",
    "    ['love'],\n",
    "    ['anger'],\n",
    "    ['fear'],\n",
    "    ['surprise'],\n",
    "]\n",
    "\n",
    "emo_idxs = v.t√≥kenes_a_√≠ndices(emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "emo_sents = [ torch.mm(e[doc], w).max(dim=0).values for doc in emo_idxs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distancia entre oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8635.8857])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.nn.PairwiseDistance(p=.5)\n",
    "\n",
    "d(emo_sents[0].reshape(1,-1), emo_sents[1].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|‚ñà‚ñã        | 1/6 [01:00<04:58, 59.71s/it]\u001b[A\n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [01:03<02:51, 42.80s/it]\u001b[A\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [01:06<01:32, 30.87s/it]\u001b[A\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [01:09<00:45, 22.77s/it]\u001b[A\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [01:12<00:16, 16.75s/it]\u001b[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [01:14<00:00, 12.48s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([416809, 6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = torch.stack( [d(s, sent) for sent in tqdm(emo_sents)], dim=1 )\n",
    "\n",
    "dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = dist.min(dim=1).indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "labels = {\n",
    "    'sadness':0,\n",
    "    'joy':1,\n",
    "    'love':2,\n",
    "    'anger':3,\n",
    "    'fear':4,\n",
    "    'surprise':5,\n",
    "}\n",
    "\n",
    "df['y_true'] = df.emotions.map(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.43      0.00      0.01    121187\n",
      "         joy       0.51      0.00      0.01    141067\n",
      "        love       0.17      0.03      0.05     34554\n",
      "       anger       0.14      0.23      0.18     57317\n",
      "        fear       0.11      0.19      0.14     47712\n",
      "    surprise       0.04      0.56      0.07     14972\n",
      "\n",
      "    accuracy                           0.08    416809\n",
      "   macro avg       0.23      0.17      0.07    416809\n",
      "weighted avg       0.35      0.08      0.05    416809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(df.y_true, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Muy tristes estos resultados üò¢. Quiz√°s random sentence encoders funcione m√°s para entrenar clasificadores m√°s que para medidas de similaridad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
