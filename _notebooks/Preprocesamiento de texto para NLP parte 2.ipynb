{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de texto para NLP (parte 2)\n",
    "\n",
    "En la [primera parte]() llegamos a convertir esto\n",
    "\n",
    "```python\n",
    "[\n",
    "    'que se requiere para un prestamo personal',\n",
    "    'me piden mi numero de cuenta es mi cbu',\n",
    "]\n",
    "```\n",
    " \n",
    "en esto\n",
    "\n",
    "```python\n",
    "[\n",
    "    [4160, 4683, 4484, 3703, 5294, 4011, 3825],\n",
    "    [3275, 3854, 3319, 3554, 1532, 1462, 2151, 3319, 950],\n",
    "]\n",
    "```\n",
    "\n",
    "donde dijimos que las partes fundamentales son la tokenización —separar a los documentos en unidades de información— y la numericalización —el asignarle a cada uno de los tókenes un número, más que nada para que la computadora, que gusta mucho de los números, sea feliz—.\n",
    "\n",
    "También habíamos dicho que **un tóken es un atributo** pero no dijimos mucho más al respecto. Veamos cómo puede ser esto. La tarea de ejemplo es clasificar documentos. Estamos acustumbrados a tener muestras y etiquetas como `X` e `y` en las que la primera es una matriz de muestras (filas) y atributos (columnas), y la segunda suele ser una columna. Cuando el dataset está sin pre-procesar tenemos las muestras (filas) pero no los atributos (columnas), por lo general tenemos una única columna con los documentos en forma de strings, lo que mucha forma de atributos no tiene. \n",
    "\n",
    "Ahora que hemos pre-procesado el texto estamos a un paso de obtener los atributos. La función de los atributos es describir o caracterizar a las muestras. El modelo lee estos atributos para realizar inferencias. Hay distintas maneras de describir a los documentos, algunas más sofisticadas que otras, una intuitiva es aprovechar que los tókenes están numerados desde 0 hasta L (`len(vocabulario)`) y otorgarle una columna a cada uno en la matriz de atributos de tamaño N x L (donde N es la cantidad de muestras).\n",
    "\n",
    "Hecho esto, solo resta contar cuántas veces aparece cada tóken en cada documento y asentarlo en la matriz.\n",
    "\n",
    "```\n",
    "                  |  bien  hola  si    todo\n",
    "-------------------------------------------\n",
    "'hola todo bien'  |  1     1     0     1\n",
    "'si bien bien'    |  2     0     1     0\n",
    "```\n",
    "\n",
    "Como comentario, esta forma de describir los documentos ignora enteramente el órden de los tókenes, sabemos que el sentido de una oración puede cambir completamente si cambiamos algunas palabras de lugar. Para el problema en cuestión, no parece ser tan grave ya que para clasificar una pregunta podría bastar con reconocer algunas palabras claves como *cambio* y *clave* o *requisito* y *préstamo*.\n",
    "\n",
    "Ver [tf-idf](https://es.wikipedia.org/wiki/Tf-idf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "El típico bucle de entrenamiento de PyTorch tiene esta pinta.\n",
    "\n",
    "```python\n",
    "for época in range(N_ÉPOCAS):\n",
    "    for lote in datos_entrenamiento:\n",
    "        # reseteamos los gradientes\n",
    "        optimizador.zero_grad()\n",
    "        \n",
    "        predicciones = red_neuronal(lote.X)\n",
    "        pérdida = criterio(predicciones, lote.y)\n",
    "        \n",
    "        # calculamos los gradientes\n",
    "        pérdida.backward()\n",
    "    \n",
    "        # aplicamos los gradientes\n",
    "        optimizador.step()\n",
    "```\n",
    "\n",
    "Recordemos que a diferencia de otros modelos las redes neuronales revisitan varias veces el dataset, en lo que se llaman épocas, cada época es un recorrido por todas las muestras de entrenamiento.\n",
    "\n",
    "En una época el dataset se puede mostrar entero, de a una muestra, o como es común hoy en día de a grupos o lotes (*batches*). La experiencia mostró que es útil variar el orden de las muestras en cada época."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provee ciertas facilidades para el manejo de los datos con las clases definidas en [torch.utils.data](https://pytorch.org/docs/stable/data.html) a ser:\n",
    "1. `Dataset`. Organiza los datos. Le pasamos un número o índice de muestra y nos devuelve la muestra usualmente como una tupla `(atributos, etiqueta)`.\n",
    "1. `Sampler`. Salvo que lo queramos de otra manera, se encarga de brindar un orden aleatoreo de los índices del dataset; uno diferente cada vez que le preguntamos.\n",
    "1. `BatchSampler`. Por defecto, se inicializa con un `Sampler` y el tamaño de lote. Se encarga de armar grupos de índices; diferentes cada vez que le preguntamos.\n",
    "1. `DataLoader`. Valiéndose de los grupos de índices de `BatchSampler`, obtiene muestras de `Dataset`. De esta manera para cada época devuelve lotes de muestras al azar.  \n",
    "\n",
    "Por `Sampler` y `BatchSampler` no nos detendremos ya el comportamiento por defecto, que es barajar el dataset en cada época y armar lotes del mismo tamaño es todo lo que necesitamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Textset(Dataset):\n",
    "    def __init__(self, documentos, etiquetas=None):\n",
    "            \n",
    "        self.documentos = documentos\n",
    "        self.etiquetas  = etiquetas or np.full(len(documentos), np.nan)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documentos)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.documentos[item], self.etiquetas[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una clase que necesita implementar `__len__` y `__getitem__`. Podría encargarse de levantar y pre-procesar el dataset, que por comodidad lo hemos cargado con Pandas y pre-procesado por fuera: el constructor (`__init__`) podría recibir el nombre del archivo, leerlo y aplicarle las funciones pertinentes. No lo hemos hecho internamente porque el vocabulario debe nutrirse del dataset de entrenamiento ya pre-procesado [falta].\n",
    "\n",
    "También necesitaremos crear un `Textset` para el dataset de inferencia, para el cual no contamos con las etiquetas. En el caso de no pasar etiquetas generamos una lista llena de NaNs del mismo largo que la lista de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18093"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = Textset(train_índices, etiquetas_train_índices)\n",
    "\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 169, 1, 4652, 0, 17, 65], 40)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[10_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es bastante similar a lo que una lista de tuplas podría lograr, aunque fue una buena oportunidad para juntar los documentos y las etiquetas que luego de cargar el DataFrame y hasta ahora recorrieron caminos separados. Lo realmente importante es el `DataLoader`, no podemos usar una lista como dataset porque requiere que sea una instancia de `Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "`DataLoader` es un *iterable*. Los iterables son colecciones de elementos que se pueden recorrer; implementan el método `__iter__`, del que se espera que devuelva un objeto *iterador* (`iterador = iter(iterable)`). A su vez el iterador implementa el método `__next__` que se encarga devolver secuencialmente los elementos de la colección hasta que se agota; una vez que esto sucede el iterador debe ser descartado y en todo caso le pedimos al iterable que nos arme un nuevo iterador. Cuando usamos la construcción `for ítem in iterable`, el intérprete de Python implícitamente obtiene un iterador.\n",
    "\n",
    "Ver la [sección de interables](https://docs.python.org/3/tutorial/classes.html#iterators) en el tutorial de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uno'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = iter(['uno','dos'])\n",
    "\n",
    "next(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dos'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-cfa830c9416d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay próximo elemento. Cuando se llega al fin del iterador se levanta la excepción `StopIteration`.\n",
    "\n",
    "Suficientes detalles por ahora. Todo esto para decir que `DataLoader` es un iterable que particularmente devuelve un iterador distinto cada vez, a diferencia de una lista en la que los elementos siempre se recorren en el mismo orden. Es decir, se trata de una colección de lotes pero cada iterador agrupa lotes según como dicte `BatchSampler`, que suele ser aleatorio.\n",
    "\n",
    "En cada **época** le pedimos un iterador a `DataLoader`, por lo que recorremos todo el `Dataset` agrupado en lotes de manera diferente cada vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le estamos diciendo a `DataLoader` que queremos lotes de 32 muestras (`batch_size`) y que el armado de los lotes sea aleatorio (`shuffle`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([  12, 5168,   26,    9,   16,    8,   24,   10,    8,   15,   49,   49,\n",
       "            46,   16,   15,    8,    1,   62,   26,   12,    8,    7,   12,  157,\n",
       "            44, 2082,    5,   62,    1,   76,    8,   74]),\n",
       "  tensor([ 140,   10,   75,    4,    4,   22,   84, 1519,   48,   75,   27,  357,\n",
       "           105,   40,   48, 1911,  213,  585,   14,   48,   19,  203,  102,  164,\n",
       "            57,    0,    2,   22, 1009,  262,  274,  630]),\n",
       "  tensor([  17,   51,  371, 1058,   64,    4,   71,   27,   17,    9,    1,    2,\n",
       "            59,   57,    2,    6,    0,  928,   62,    6, 1973,    3,   17,  713,\n",
       "            10,   36,   56,    4,   18,   21,    1,    5]),\n",
       "  tensor([  56,   67,   83,    0,   22,  724,   18,   24,    3,  765,   64,  207,\n",
       "          1454,    2,  765,   36,  179,    2,  358,   13,   38,  236,   56,    3,\n",
       "             3,    0,    5,   32,    1,   98, 1009,    6])],\n",
       " tensor([144, 153, 247,   3,  55,   0, 223,  15,  18,   6,  26, 160,  89, 199,\n",
       "         149,  49, 260, 285,  13,   3, 198,  18,  23,   0,   1, 103,  35, 112,\n",
       "         128,  20, 128,   3])]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_lote = next(iter(train_dl))\n",
    "un_lote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está bueno que ya veamos tensores de PyTorch porque vamos a necesitar los datos en forma de tensor para alimentar a la red neuronal. Sin embargo, algo no parece andar bien con el lote que acabamos de obtener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(un_lote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos dos elementos adentro del lote, podríamos pensar que el primero agrupa documentos y el segundo, etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, 32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(un_lote[1]), len(un_lote[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las etiquetas del lote están perfecto, son un tensor de una dimensión con largo 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 4)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(un_lote[0]), len(un_lote[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cambio la agrupación de documentos no tiene sentido. Es otra lista de tamaño 4 con tensores adentro. ¿Qué está pasando?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensores\n",
    "\n",
    "El problema parece radicar en los tensores. Son estructuras que las podemos imaginar como una columna cuando tienen una dimensión, una tabla cuando son dos, un cubo cuando tres...\n",
    "\n",
    "Los tensores son similares a los `ndarrays` de NumPy, con el aditivo que también pueden ser usados en la GPU para acelerar los cómputos. Ver más de tensores en el [tutorial de PyTorch](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html).\n",
    "\n",
    "En el caso de los documentos que a la altura del `Dataset` son listas de listas de índices, son dos dimensiones, y al llevarlos a una tabla vemos que tendríamos tantas filas como documentos y tantas columnas como índices tenga el documento más largo de la colección pero que **no todos los documentos tienen tantos índices como columnas la tabla**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2], [4, 4, 4, 4], [7, 7, 7, 7, 7, 7, 7]]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "índices = [\n",
    "    [2,2],\n",
    "    [4,4,4,4],\n",
    "    [7,7,7,7,7,7,7],\n",
    "]\n",
    "\n",
    "índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-121200966211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0míndices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 4)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor(índices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como anticipamos, no le gustó nada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tókenes especiales\n",
    "\n",
    "Lo mencionamos al pasar, a veces se utilizan tókenes especiales como `<separador de palabra>`, `<separador de oración>`, `<inicio del texto>`, `<fin del texto>`. Hay de todo tipo, según la tarea a realizar. Uno que está presente generalmente en los proyectos es el tóken de relleno `<relleno>` (en inglés *padding*).\n",
    "\n",
    "El tóken de relleno nos va a servir para hacer que todos los documentos tengan el mismo largo y finalmente podamos convertirlos en un tensor. No lo vamos a hacer inmediatamente ya que no nos interesa que tengan el mismo largo en todo el dataset sino en todo el lote. Como los lotes son generados en el DataLoader, este último tendrá que encargarse de rellenar los documentos.\n",
    "\n",
    "Vamos a modificar `Vocab` quien se encarga de la lista de tókenes para que incluya a `<relleno>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# versión 4\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "class Vocab():\n",
    "    @property\n",
    "    def índice_relleno(self):\n",
    "        return self.mapeo.get(self.tóken_relleno)\n",
    "    \n",
    "    def __init__(self, tóken_desconocido='<unk>', tóken_relleno='<pad>', frecuencia_mínima=0.0, frecuencia_máxima=1.0,\n",
    "                 longitud_mínima=1, longitud_máxima=np.inf, stop_words=[], límite_vocabulario=None):\n",
    "        \n",
    "        self.tóken_desconocido = tóken_desconocido\n",
    "        self.tóken_relleno = tóken_relleno\n",
    "        self.frecuencia_mínima = frecuencia_mínima\n",
    "        self.frecuencia_máxima = frecuencia_máxima\n",
    "        self.longitud_mínima = longitud_mínima\n",
    "        self.longitud_máxima = longitud_máxima\n",
    "        self.stop_words = stop_words\n",
    "        self.límite_vocabulario = límite_vocabulario\n",
    "    \n",
    "    # ningún cambio aquí\n",
    "    def reducir_vocabulario(self, lote):\n",
    "        contador_absoluto = Counter(chain(*lote))\n",
    "        \n",
    "        contador_documentos = Counter()\n",
    "        \n",
    "        for doc in lote:\n",
    "            contador_documentos.update(set(doc))\n",
    "        \n",
    "        # frecuencia mínima\n",
    "        if isinstance(self.frecuencia_mínima, int): # frecuencia de tóken\n",
    "            vocabulario_mín = [tóken for tóken, frecuencia in contador_absoluto.most_common() if frecuencia >= self.frecuencia_mínima]\n",
    "        else: # frecuencia de documento\n",
    "            vocabulario_mín = [tóken for tóken, frecuencia in contador_documentos.most_common() if frecuencia/len(lote) >= self.frecuencia_mínima]\n",
    "        \n",
    "        # frecuencia máxima\n",
    "        if isinstance(self.frecuencia_máxima, int): # frecuencia de tóken\n",
    "            vocabulario_máx = [tóken for tóken, frecuencia in contador_absoluto.most_common() if self.frecuencia_máxima >= frecuencia]\n",
    "        else: # frecuencia de documento\n",
    "            vocabulario_máx = [tóken for tóken, frecuencia in contador_documentos.most_common() if self.frecuencia_máxima >= frecuencia/len(lote)]\n",
    "\n",
    "        # intersección de vocabulario_mín y vocabulario_máx preservando el órden\n",
    "        vocabulario = [tóken for tóken in vocabulario_mín if tóken in vocabulario_máx]\n",
    "\n",
    "        # longitud\n",
    "        vocabulario = [tóken for tóken in vocabulario if self.longitud_máxima >= len(tóken) >= self.longitud_mínima]\n",
    "        \n",
    "        # stop words\n",
    "        vocabulario = [tóken for tóken in vocabulario if tóken not in self.stop_words]\n",
    "        \n",
    "        # límite\n",
    "        vocabulario = vocabulario[:self.límite_vocabulario]\n",
    "        \n",
    "        return vocabulario\n",
    "        \n",
    "    def fit(self, lote):\n",
    "        vocabulario = self.reducir_vocabulario(lote)\n",
    "        \n",
    "        if self.tóken_desconocido:\n",
    "            vocabulario.append(self.tóken_desconocido)\n",
    "        \n",
    "        if self.tóken_relleno:\n",
    "            vocabulario.insert(0, self.tóken_relleno)\n",
    "        \n",
    "        self.mapeo = {tóken: índice for índice, tóken in enumerate(vocabulario)}\n",
    "\n",
    "        return self\n",
    "    \n",
    "    # ningún cambio aquí\n",
    "    def transform(self, lote):\n",
    "        if self.tóken_desconocido: # reemplazar\n",
    "            return [[tóken if tóken in self.mapeo else self.tóken_desconocido for tóken in doc] for doc in lote]\n",
    "        else: # ignorar\n",
    "            return [[tóken for tóken in doc if tóken in self.mapeo] for doc in lote]\n",
    "    \n",
    "    # ningún cambio aquí\n",
    "    def tókenes_a_índices(self, lote):\n",
    "        lote = self.transform(lote)\n",
    "        \n",
    "        return [[self.mapeo[tóken] for tóken in doc] for doc in lote]\n",
    "    \n",
    "    # ningún cambio aquí\n",
    "    def índices_a_tókenes(self, lote):\n",
    "        mapeo_inverso = list(self.mapeo.keys())\n",
    "        \n",
    "        return [[mapeo_inverso[índice] for índice in doc] for doc in lote]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mapeo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El índice del tóken de relleno suele ser 0 y para continuar con esta tradición en vez de hacerle `append` al vocabulario le hicimos un *prepend* para que el tóken encabece el listado. Además usamos el decorador `@property` para tener un atributo `índice_relleno` (en vez de un método) que nos devuelva el índice del tóken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = Vocab().fit(train_docs)\n",
    "\n",
    "v.índice_relleno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La función que rellena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rellenar_documentos(lote, largos, índice_relleno):\n",
    "    máximo_largo = max(largos)\n",
    "    \n",
    "    return [doc + [índice_relleno] * (máximo_largo - largos[i]) for i, doc in enumerate(lote)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tenemos que pasar el lote, el largo o tamaño de cada documento del lote y el índice de relleno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2, 0, 0, 0, 0, 0], [4, 4, 4, 4, 0, 0, 0], [7, 7, 7, 7, 7, 7, 7]]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "índices = [\n",
    "    [2,2],\n",
    "    [4,4,4,4],\n",
    "    [7,7,7,7,7,7,7],\n",
    "]\n",
    "\n",
    "largos = [2,4,7]\n",
    "\n",
    "rellenos = rellenar_documentos(índices, largos, v.índice_relleno)\n",
    "\n",
    "rellenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 0, 0, 0, 0, 0],\n",
       "        [4, 4, 4, 4, 0, 0, 0],\n",
       "        [7, 7, 7, 7, 7, 7, 7]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(rellenos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Ahora sí funcionó!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tamaño del documento\n",
    "\n",
    "Vamos a incluir el tamaño del documento (en cantidad de tókenes/índices) junto a cada ítem del dataset ya que nos va a hacer falta para la función que rellena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Textset(Dataset):\n",
    "    def __init__(self, documentos, etiquetas=None):\n",
    "            \n",
    "        self.documentos = documentos\n",
    "        self.etiquetas  = etiquetas or np.full(len(documentos), np.nan)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documentos)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.documentos[item], len(self.documentos[item]), self.etiquetas[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 169, 1, 4652, 0, 17, 65], 7, 40)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = Textset(train_índices, etiquetas_train_índices)\n",
    "\n",
    "train_ds[10_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: **Atri**buto**Dicc**ionario\n",
    "\n",
    "¿Alguna vez quisiste acceder a los elementos de un diccionario como si fuesen atributos de un objeto? Es decir así\n",
    "\n",
    "```python\n",
    "d = {'uno':1, 'dos':2, 'tres':3}\n",
    "\n",
    "d.uno # => 1\n",
    "```\n",
    "\n",
    "en vez de así\n",
    "\n",
    "```python\n",
    "d['uno'] # => 1\n",
    "```\n",
    "\n",
    "Con esta magia ahora es posible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/4984647/accessing-dict-keys-like-an-attribute\n",
    "\n",
    "class AtriDicc():\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.__dict__ = dict(*args, **kwargs)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AtriDicc(uno=1, dos=2, tres=3).uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a *pimpiar* la clase `Textset` con esto para que en vez de devolver elementos del dataset como tuplas `(documento, largo, etiqueta)` en el que debemos acordarnos que el orden de los elementos, devolvemos un `AtriDicc` en el que accedemos las cosas por su nombre y es más cómodo que un diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Textset(Dataset):\n",
    "    def __init__(self, documentos, etiquetas=None):\n",
    "            \n",
    "        self.documentos = documentos\n",
    "        self.etiquetas  = etiquetas or np.full(len(documentos), np.nan)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documentos)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return AtriDicc(\n",
    "            documento = self.documentos[item],\n",
    "            largo = len(self.documentos[item]),\n",
    "            etiqueta =  self.etiquetas[item],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 169, 1, 4652, 0, 17, 65]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = Textset(train_índices, etiquetas_train_índices)\n",
    "\n",
    "train_ds[10_000].documento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función *collate*\n",
    "\n",
    "*Collate* significa juntar diferentes piezas de información para ver sus similaridades y diferencias, también puede ser colectar y organizar las hojas de un reporte, un libro. En el contexto de `DataLoader` quiere decir arreglar el lote. Entonces esta función recibe una lista de elementos del `Dataset`, en nuestro caso una lista de de `AtriDicc`s, y debe devolver el lote en una forma útil y en lo posible realizar conversiones a tensores.\n",
    "\n",
    "`DataLoader` posee una *collate function* por defecto que utiliza internamente y que en muchos casos funciona correctamente, pero otros como ahora que tenemos documentos de distinto largo nos toca definir una función propia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "def rellenar_lote(lote):\n",
    "    \"\"\"Prepara lotes para ingresar a nn.Embedding\"\"\"\n",
    "    documentos = [elemento.documento for elemento in lote]\n",
    "    largos     = [elemento.largo     for elemento in lote]\n",
    "    etiquetas  = [elemento.etiqueta  for elemento in lote]\n",
    "\n",
    "    rellenos = rellenar_documentos(documentos, largos, v.índice_relleno)  \n",
    "    # para RNNs descomentar esta línea\n",
    "    #rellenos = pack_padded_sequence(rellenos, largos, batch_first=True, enforce_sorted=False)\n",
    "    \n",
    "    return AtriDicc(\n",
    "        documentos = torch.tensor(rellenos),\n",
    "        etiquetas  = torch.tensor(etiquetas),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando instanciamos un `DataLoader` le pasamos la función que acabamos de definir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, collate_fn=rellenar_lote, batch_size=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[781,  31,  17, 104, 111,   9, 383,  93,  18,  11, 489,   0,   0],\n",
       "        [ 20,   4,  11,   7, 272,  78,  29,  96,   5, 396,  16,  86,  16],\n",
       "        [ 26,  69,  17, 313,   4, 258,  22,   4, 102,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_lote = next(iter(train_dl))\n",
    "un_lote.documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 80, 316,  16])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_lote.etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciona de maravillas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una función alternativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función anterior es compatible con el módulo de PyTorch `nn.Embedding` que suele se la puerta de entrada en los modelos de procesamiento de texto. Todavía no hemos hablado nada de los *embeddings*. Quizás sea un momento para mencionar a `nn.EmbeddingBag`, que tiene requerimientos completamente diferentes al primer módulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetear_lote(lote):\n",
    "    \"\"\"Prepara lotes para ingresar a nn.EmbeddingBag\"\"\"\n",
    "    documentos = [torch.tensor(elemento.documento) for elemento in lote]\n",
    "    offsets    = [0] + [elemento.largo for elemento in lote][:-1] \n",
    "    etiquetas  = [elemento.etiqueta for elemento in lote]\n",
    "\n",
    "    return AtriDicc(\n",
    "        documentos = torch.cat(documentos),\n",
    "        offsets    = torch.tensor(offsets).cumsum(dim=0),\n",
    "        etiquetas  = torch.tensor(etiquetas),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función yuxtapone los documentos por un lado, y por otro (`offsets`) indica cuándo comienza cada documento en ese continuo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, collate_fn=offsetear_lote, batch_size=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  35,   14,    8,  544,   46,    6, 2493,   30,  384,    2, 1062,   27,\n",
       "         236,    5,  778,  378,   22,    4,   53,    1,  866,    9,   17, 1564,\n",
       "         109,   68,  186,   16,    6, 1419])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_lote = next(iter(train_dl))\n",
    "un_lote.documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  9, 16])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_lote.offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avanzado: Memory pinning\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html#memory-pinning\n",
    "\n",
    "Cuando los lotes son de un tipo personalizado, normal cuando se utiliza una *collate function* propia, es necesario que el tipo defina el método `pin_memory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict():\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.__dict__ = dict(*args, **kwargs)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self.__dict__)\n",
    "\n",
    "    def pin_memory(self):\n",
    "        for atributo, valor in self.__dict__.items():\n",
    "            self.__dict__[atributo] = valor.pin_memory() if hasattr(valor, 'pin_memory') else valor\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El pre-procesamiento hasta ahora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario_documentos = Vocab().fit(train_docs)\n",
    "\n",
    "train_índices = vocabulario_documentos.tókenes_a_índices(train_docs)\n",
    "valid_índices = vocabulario_documentos.tókenes_a_índices(valid_docs)\n",
    "infer_índices = vocabulario_documentos.tókenes_a_índices(infer_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Textset(train_índices)\n",
    "valid_ds = Textset(valid_índices)\n",
    "infer_ds = Textset(infer_índices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo definimos el modelo, no lo entrenamos. Elegimos la función `offsetear_lote` ya que el modelo usa `nn.EmbeddingBag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True,  collate_fn=offsetear_lote, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=32, shuffle=False, collate_fn=offsetear_lote, pin_memory=True)\n",
    "infer_dl = DataLoader(infer_ds, batch_size=64, shuffle=False, collate_fn=offsetear_lote, pin_memory=True)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ClasificadorBolsa(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False, mode='max')\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "modelo = ClasificadorBolsa(len(vocabulario_documentos), 8, len(vocabulario_etiquetas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto concluye la segunda parte. Quedaron los *embeddings* para la tercera. Ahora deberíamos tener más control sobre la carga de datos en PyTorch. Muchos ejemplos de uso y tutoriales dan por sentada esta parte al utilizar datasets de ejemplos, que ya vienen pre-procesados y/o que la carga por defecto de PyTorch maneja sin inconvenientes."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
